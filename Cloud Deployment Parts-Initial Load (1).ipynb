{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b83d2a6-e014-4c9e-9de7-c0f9af78ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee54ec95-3aee-4dcb-aa7c-04bbef0c8b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time=time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7409dd5-cf76-4224-a72b-47eff40bacf7",
   "metadata": {},
   "source": [
    "# Latest_SFTP_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5466a8b-7370-402b-bc30-6c3d28a621cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class Latest_SFTP_file:\n",
    "    def __init__(self):\n",
    "        self.bucket_name = \"miag-m360-test-bucket\"\n",
    "\n",
    "    def get_latest_file(self):\n",
    "        \"\"\"\n",
    "        Retrieves the name of the latest file uploaded to a specified GCS bucket.\n",
    "\n",
    "        :param bucket_name: Name of the GCS bucket\n",
    "        :return: The name of the latest file or None if the bucket is empty\n",
    "        \"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(self.bucket_name)\n",
    "        blobs = list(bucket.list_blobs())\n",
    "        \n",
    "        downloaded_files = [blob for blob in blobs if blob.name.startswith(\"Downloaded Files/\")]\n",
    "\n",
    "        if not downloaded_files:\n",
    "            print(\"No files in the 'Downloaded Files' folder.\")\n",
    "            return None\n",
    "\n",
    "        # Sort blobs by their updated timestamps (most recent first)\n",
    "        latest_blob = max(downloaded_files, key=lambda blob: blob.updated)\n",
    "\n",
    "        print(f\"The latest file in 'Downloaded Files' is: {latest_blob.name}\")\n",
    "        return latest_blob.name\n",
    "\n",
    "\n",
    "#     def lowest_document_date(self, sftp_df):\n",
    "#         lowest_date = pd.to_datetime(sftp_df['Document date'], format='%d.%m.%Y').min()\n",
    "#         return lowest_date\n",
    "\n",
    "#     def convert_to_yyyymmdd(self, date_str):\n",
    "#         \"\"\"\n",
    "#         Converts a date string from 'yyyy-mm-dd' to 'yyyymmdd' format.\n",
    "\n",
    "#         :param date_str: Date string in 'yyyy-mm-dd' format\n",
    "#         :return: Date string in 'yyyymmdd' format\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")  # Parse the input date\n",
    "#             return date_obj.strftime(\"%Y%m%d\")  # Format it to 'yyyymmdd'\n",
    "#         except ValueError:\n",
    "#             raise ValueError(\"Invalid date format, expected 'yyyy-mm-dd'\")\n",
    "\n",
    "#     def convert_to_yyyy_mm_dd(self, date_str):\n",
    "#         \"\"\"\n",
    "#         Converts a date string from 'yyyymmdd' to 'yyyy-mm-dd' format.\n",
    "\n",
    "#         :param date_str: Date string in 'yyyymmdd' format\n",
    "#         :return: Date string in 'yyyy-mm-dd' format\n",
    "#         \"\"\"\n",
    "#         try:\n",
    "#             date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")  # Parse the input date\n",
    "#             return date_obj.strftime(\"%Y-%m-%d\")  # Format it to 'yyyy-mm-dd'\n",
    "#         except ValueError:\n",
    "#             raise ValueError(\"Invalid date format, expected 'yyyy-mm-dd'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96da596-9e9a-424d-a7cf-389f714ac8ed",
   "metadata": {},
   "source": [
    "# Storage_Bucket_Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b0e243-fa96-4282-8386-a57f19d3f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import io\n",
    "\n",
    "\n",
    "class Storage_Bucket_Operations:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bucket_name = \"miag-m360-test-bucket\"\n",
    "        self.download_files_path = \"Downloaded Files\"\n",
    "\n",
    "    def readFromBucket(self, sftp_file):\n",
    "        client = storage.Client(project='cf-hada-bsc-mcctk-mia-kg')\n",
    "        bucket = client.get_bucket(self.bucket_name)\n",
    "        blob = bucket.blob(f\"{sftp_file}\")\n",
    "        csv_data = blob.download_as_text()\n",
    "        sftp_df = pd.read_csv(io.StringIO(csv_data), index_col=False,\n",
    "                              dtype={\"Store\": str, \"Supplier number (MIAG)\": str, \"Remittance advice number\": str,\n",
    "                                     \"Supplier number (Sales Line)\": str, \"Document number\": str,\n",
    "                                     \"Invoice number\": str, })\n",
    "        return sftp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3cebfa-646d-4981-b09a-416a178daa78",
   "metadata": {},
   "source": [
    "# DB_Instance_Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f43da14-8514-46a8-a37e-6515db597dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandasql as ps\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "class DB_Instance_Operations:\n",
    "\n",
    "    def __init__(self):\n",
    "    # GCS bucket details\n",
    "        bucket_name = \"miag-m360-test-bucket\"\n",
    "        cert_files = {\n",
    "            \"sslrootcert\": \"hada-bsc-miag-m360-psql-pp-server-ca.pem\",\n",
    "            \"sslcert\": \"hada-bsc-miag-m360-psql-pp-client-cert.pem\",\n",
    "            \"sslkey\": \"hada-bsc-miag-m360-psql-pp-client-key.pem\"\n",
    "        }\n",
    "    \n",
    "        # Download certificate files into temporary files\n",
    "        self.cert_temp_paths = self.get_certificates_from_gcs(bucket_name, cert_files)\n",
    "    \n",
    "        # Create the database URL using temporary file paths\n",
    "        self.db_url = (\n",
    "            r\"postgresql+psycopg2://postgres:9rk$Y}gib9kZEucj@10.32.111.54:5432/MIAG-M360_UAT\"\n",
    "            f\"?sslmode=require\"\n",
    "            f\"&sslrootcert={self.cert_temp_paths['sslrootcert']}\"\n",
    "            f\"&sslcert={self.cert_temp_paths['sslcert']}\"\n",
    "            f\"&sslkey={self.cert_temp_paths['sslkey']}\"\n",
    "        )\n",
    "    \n",
    "        # Create the database engine\n",
    "        self.engine = create_engine(self.db_url)\n",
    "\n",
    "\n",
    "    def get_certificates_from_gcs(self, bucket_name, cert_files):\n",
    "        \"\"\"Fetch certificate files from GCS and store them in temporary files.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        temp_paths = {}\n",
    "    \n",
    "        for key, gcs_path in cert_files.items():\n",
    "            # Create a temporary file\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "            temp_file.close()\n",
    "            temp_paths[key] = temp_file.name\n",
    "            print(f\"{key} downloaded and stored temporarily at {temp_file.name}\")\n",
    "    \n",
    "        return temp_paths\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        for path in self.cert_temp_paths.values():\n",
    "            if os.path.exists(path):\n",
    "                os.remove(path)\n",
    "                print(f\"Deleted temporary file: {path}\")\n",
    "\n",
    "\n",
    "    def readSDPTable(self):\n",
    "        # query = \"Delete from sdp_pool;\"\n",
    "        # with self.engine.connect() as connection:\n",
    "        #     connection.execute(text(query))\n",
    "        #     connection.commit()\n",
    "        # print(\"Rows deleted\")\n",
    "        query = \"select * from sdp_pool\"\n",
    "        sdp_df = pd.read_sql_query(query, self.engine)\n",
    "        return sdp_df\n",
    "\n",
    "    def updateSDP(self, sdp, sftp_df):\n",
    "        sq1 = \"SELECT DISTINCT `Supplier number (Sales Line)`, `Supplier number (MIAG)`, `Supplier name`, `Contract area` FROM sftp_df\"\n",
    "        miag2 = ps.sqldf(sq1, locals())\n",
    "        if sdp.shape[0] == 0:\n",
    "            sdp = miag2.copy()\n",
    "        else:\n",
    "            sdpq = \"Select distinct * from sdp\"\n",
    "            sdp_dist_df = ps.sqldf(sdpq)\n",
    "            new_supp_in_sftp_query = \"SELECT * from sftp_df where `Supplier number (Sales Line)` not in (SELECT `Supplier_Number_Sales` FROM sdp)\"\n",
    "            new_supp_df = ps.sqldf(new_supp_in_sftp_query)\n",
    "            push_to_sdp_query = \"Select `Supplier number (Sales Line)`, `Supplier number (MIAG)`, `Supplier name`, `Contract area` from new_supp_df union Select `Supplier_Number_Sales`, `Supplier_Number_MIAG`, `Supplier_Name`, `Contract_Area` from sdp\"\n",
    "            sdp = ps.sqldf(push_to_sdp_query)\n",
    "        return sdp\n",
    "\n",
    "    def writeSDPTable(self, sdp_df):\n",
    "        column_mapping = {\n",
    "            'Supplier number (Sales Line)': 'Supplier_Number_Sales',\n",
    "            'Supplier number (MIAG)': 'Supplier_Number_MIAG',\n",
    "            'Supplier name': 'Supplier_Name',\n",
    "            'Contract area': 'Contract_Area'\n",
    "        }\n",
    "        sdp_df.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.begin() as connection:\n",
    "            delete_query = text(\"Delete from sdp_pool\")\n",
    "            connection.execute(delete_query)\n",
    "            sdp_df.to_sql('sdp_pool', connection, if_exists='append', index=False)\n",
    "        print(\"Written back to SDP Table of DB Instance...\")\n",
    "\n",
    "    def getSupplierNumberForMMSIC(self):\n",
    "        new_supplier_list_for_mmsic = []\n",
    "        supplier_list_for_mmsic = self.readSDPTable()['Supplier_Number_Sales'].to_list()\n",
    "        for i in range(len(supplier_list_for_mmsic)):\n",
    "            new_supplier_list_for_mmsic.append(str(0) + supplier_list_for_mmsic[i][1:])\n",
    "        return new_supplier_list_for_mmsic\n",
    "\n",
    "    def getSupplierNumberForSISIC(self):\n",
    "        new_supplier_list_for_sisic = []\n",
    "        supplier_list_for_sisic = self.readSDPTable()['Supplier_Number_Sales'].to_list()\n",
    "        for i in range(len(supplier_list_for_sisic)):\n",
    "            new_supplier_list_for_sisic.append(str(1) + supplier_list_for_sisic[i][1:])\n",
    "        return new_supplier_list_for_sisic\n",
    "\n",
    "    def getSupplierNumberForFI(self):\n",
    "        new_supplier_list_for_fi = []\n",
    "        supplier_list_for_fi = self.readSDPTable()['Supplier_Number_Sales'].to_list()\n",
    "        for i in range(len(supplier_list_for_fi)):\n",
    "            new_supplier_list_for_fi.append(supplier_list_for_fi[i][5:])\n",
    "        return new_supplier_list_for_fi\n",
    "\n",
    "    def writeICTable(self, extracted_ic_df):\n",
    "        column_mapping = {\n",
    "            'LIFNR': 'lifnr',\n",
    "            'BELNR': 'belnr',\n",
    "            'RENR': 'renr',\n",
    "            'REDAT': 'redat',\n",
    "            'LFSNR': 'lfsnr',\n",
    "            'GEBRF': 'gebrf',\n",
    "            'GSMWB': 'gsmwb',\n",
    "            'GSMWF': 'gsmwf',\n",
    "            'WAERS': 'waers',\n",
    "            'WENUM': 'wenum',\n",
    "            'RGDAT': 'rgdat',\n",
    "            'ABGST': 'abgst',\n",
    "            'AUFNR': 'aufnr',\n",
    "            'VORGN': 'vorgn',\n",
    "            'GJAHR': 'gjahr',\n",
    "            'WEDAT': 'wedat',\n",
    "            'DEBNOTNO': 'debnotno',\n",
    "        }\n",
    "\n",
    "        extracted_ic_df.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from intermediate_ic\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "            extracted_ic_df.to_sql('intermediate_ic', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Intermediate IC Table of DB Instance...\")\n",
    "\n",
    "    def writeFITable(self, extracted_fi_df):\n",
    "        column_mapping = {\n",
    "            'MANDT': 'mandt',\n",
    "            'Document_type': 'document_type',\n",
    "            'document_type_desc': 'document_type_desc',\n",
    "            'GJAHR': 'gjahr',\n",
    "            'BUKRS': 'bukrs',\n",
    "            'GSBER': 'gsber',\n",
    "            'PRCTR': 'prctr',\n",
    "            'store_or_dc': 'store_or_dc',\n",
    "            'KOSTL': 'kostl',\n",
    "            'month_in_fin_year': 'month_in_fin_year',\n",
    "            'BELNR': 'belnr',\n",
    "            'XBLNR': 'xblnr',\n",
    "            'AUGBL': 'augbl',\n",
    "            'AUGDT': 'augdt',\n",
    "            'ZFBDT': 'zfbdt',\n",
    "            'ZBD1T': 'zbd1t',\n",
    "            'ZBD2T': 'zbd2t',\n",
    "            'NETDT': 'netdt',\n",
    "            'BUZEI': 'buzei',\n",
    "            'altkt': 'altkt',\n",
    "            'hkont': 'hkont',\n",
    "            'suppl_no': 'suppl_no',\n",
    "            'BLDAT': 'bldat',\n",
    "            'BUDAT': 'budat',\n",
    "            'CPUDT': 'cpudt',\n",
    "            'partition_date': 'partition_date',\n",
    "            'dana_ingestion_date': 'dana_ingestion_date',\n",
    "            'shkzg': 'shkzg',\n",
    "            'Amount_in_local_currency': 'amount_in_local_currency',\n",
    "            'Amount_in_document_currency': 'amount_in_document_currency',\n",
    "            'Tax_in_local_currency': 'tax_in_local_currency',\n",
    "            'Tax_in_document_currency': 'tax_in_document_currency',\n",
    "            'WAERS': 'waers',\n",
    "            'Batch_Input_session_name': 'batch_input_session_name',\n",
    "            'sgtxt': 'sgtxt',\n",
    "        }\n",
    "\n",
    "        extracted_fi_df.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from intermediate_fi\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "        extracted_fi_df.to_sql('intermediate_fi', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Intermediate FI Table of DB Instance...\")\n",
    "\n",
    "    def readICTable(self):\n",
    "        query = \"select * from intermediate_ic\"\n",
    "        df_ic = pd.read_sql_query(query, self.engine)\n",
    "        return df_ic\n",
    "\n",
    "    def readFITable(self):\n",
    "        query = \"select * from intermediate_fi\"\n",
    "        df_fi = pd.read_sql_query(query, self.engine)\n",
    "        return df_fi\n",
    "    \n",
    "    def writeMergedTable(self, loadfile_df_copy):\n",
    "        loadfile_df_copy['COMPANY_CODE'] = loadfile_df_copy['COMPANY_CODE'].astype(str)\n",
    "        loadfile_df_copy['SUPPLIER_NO'] = loadfile_df_copy['SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['MIAG_SUPPLIER_NO'] = loadfile_df_copy['MIAG_SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['ORDER_NO'] = loadfile_df_copy['ORDER_NO'].astype(str)\n",
    "        loadfile_df_copy['DOC_TYPE'] = loadfile_df_copy['DOC_TYPE'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_NO'] = loadfile_df_copy['INVOICE_NO'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_DATE'] = loadfile_df_copy['INVOICE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DELIVERY_NOTE_NO'] = loadfile_df_copy['DELIVERY_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['TOTAL_AMT_DC'] = pd.to_numeric(loadfile_df_copy['TOTAL_AMT_DC'])\n",
    "        loadfile_df_copy['TOTAL_VAT_DC'] = loadfile_df_copy['TOTAL_VAT_DC'].astype(str)\n",
    "        loadfile_df_copy['CURRENCY'] = loadfile_df_copy['CURRENCY'].astype(str)\n",
    "        loadfile_df_copy['PRE_FINANCE_DATE'] = pd.to_datetime(loadfile_df_copy['PRE_FINANCE_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['GOODS_RECEIPT_NO'] = loadfile_df_copy['GOODS_RECEIPT_NO'].astype(str)\n",
    "        loadfile_df_copy['GOODS_RECEIPT_DATE'] = pd.to_datetime(loadfile_df_copy['GOODS_RECEIPT_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_ENTRY_DATE'] = pd.to_datetime(loadfile_df_copy['INVOICE_ENTRY_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_STATUS'] = loadfile_df_copy['INVOICE_STATUS'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_STATUS_INTERNAL'] = loadfile_df_copy['INVOICE_STATUS_INTERNAL'].astype(str)\n",
    "        loadfile_df_copy['NET_DUE_DATE'] = loadfile_df_copy['NET_DUE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DEBIT_NOTE_NO'] = loadfile_df_copy['DEBIT_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['REMITTANCE_ADVICE_NO'] = loadfile_df_copy['REMITTANCE_ADVICE_NO'].astype(str)\n",
    "        loadfile_df_copy['CLEARING_DATE'] = pd.to_datetime(loadfile_df_copy['CLEARING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['DOCUMENT_NO'] = loadfile_df_copy['DOCUMENT_NO'].astype(str)\n",
    "        loadfile_df_copy['STORE_NO'] = loadfile_df_copy['STORE_NO'].astype(str)\n",
    "        loadfile_df_copy['MATCHING_DATE'] = pd.to_datetime(loadfile_df_copy['MATCHING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['MATCH_STATUS'] = loadfile_df_copy['MATCH_STATUS'].astype(str)\n",
    "        loadfile_df_copy['SYNC_DATE'] = pd.to_datetime(loadfile_df_copy['SYNC_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['SYNC_STATUS'] = loadfile_df_copy['SYNC_STATUS'].astype(str)\n",
    "        loadfile_df_copy['ARKTX'] = loadfile_df_copy['ARKTX'].astype(str)\n",
    "        \n",
    "        \n",
    "        column_mapping = {\n",
    "            'COMPANY_CODE': 'company_code',\n",
    "            'SUPPLIER_NO': 'supplier_no',\n",
    "            'MIAG_SUPPLIER_NO': 'miag_supplier',\n",
    "            'ORDER_NO': 'order_no',\n",
    "            'DOC_TYPE': 'doc_type',\n",
    "            'INVOICE_NO': 'invoice_no',\n",
    "            'INVOICE_DATE': 'invoice_date',\n",
    "            'DELIVERY_NOTE_NO': 'delivery_note_no',\n",
    "            'TOTAL_AMT_DC': 'total_amt_dc',\n",
    "            'TOTAL_VAT_DC': 'total_vat_dc',\n",
    "            'CURRENCY': 'currency',\n",
    "            'PRE_FINANCE_DATE': 'pre_finance_date',\n",
    "            'GOODS_RECEIPT_NO': 'goods_receipt_no',\n",
    "            'INVOICE_ENTRY_DATE': 'invoice_entry_date',\n",
    "            'INVOICE_STATUS': 'invoice_status',\n",
    "            'INVOICE_STATUS_INTERNAL': 'invoice_status_internal',\n",
    "            'NET_DUE_DATE': 'net_due_date',\n",
    "            'DEBIT_NOTE_NO': 'debit_note_no',\n",
    "            'REMITTANCE_ADVICE_NO': 'remittance_advice_no',\n",
    "            'DOCUMENT_NO': 'document_no',\n",
    "            'STORE_NO': 'store_no',\n",
    "            'ARKTX': 'arktx',\n",
    "            'CLEARING_DATE': 'clearing_date',\n",
    "            'GOODS_RECEIPT_DATE': 'goods_receipt_date',\n",
    "            'MATCHING_DATE': 'matching_date',\n",
    "            'MATCH_STATUS': 'match_status',\n",
    "            'SYNC_DATE': 'sync_date',\n",
    "            'SYNC_STATUS': 'sync_status'\n",
    "        }\n",
    "        date_columns = ['INVOICE_DATE', 'PRE_FINANCE_DATE', 'INVOICE_ENTRY_DATE', 'NET_DUE_DATE', 'CLEARING_DATE',\n",
    "                        'GOODS_RECEIPT_DATE', 'MATCHING_DATE', 'SYNC_DATE']\n",
    "        for column in date_columns:\n",
    "            loadfile_df_copy[column] = pd.to_datetime(loadfile_df_copy[column], format='%d.%m.%Y').dt.strftime('%m-%d-%Y')\n",
    "        loadfile_df_copy.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from tbl_merged_data\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "        loadfile_df_copy.to_sql('tbl_merged_data', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Final 360 Table of DB Instance...\")\n",
    "        \n",
    "        \n",
    "    def writeStagedTable(self, loadfile_df_copy):\n",
    "        loadfile_df_copy['COMPANY_CODE'] = loadfile_df_copy['COMPANY_CODE'].astype(str)\n",
    "        loadfile_df_copy['SUPPLIER_NO'] = loadfile_df_copy['SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['MIAG_SUPPLIER_NO'] = loadfile_df_copy['MIAG_SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['ORDER_NO'] = loadfile_df_copy['ORDER_NO'].astype(str)\n",
    "        loadfile_df_copy['DOC_TYPE'] = loadfile_df_copy['DOC_TYPE'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_NO'] = loadfile_df_copy['INVOICE_NO'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_DATE'] = loadfile_df_copy['INVOICE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DELIVERY_NOTE_NO'] = loadfile_df_copy['DELIVERY_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['TOTAL_AMT_DC'] = pd.to_numeric(loadfile_df_copy['TOTAL_AMT_DC'])\n",
    "        loadfile_df_copy['TOTAL_VAT_DC'] = loadfile_df_copy['TOTAL_VAT_DC'].astype(str)\n",
    "        loadfile_df_copy['CURRENCY'] = loadfile_df_copy['CURRENCY'].astype(str)\n",
    "        loadfile_df_copy['PRE_FINANCE_DATE'] = pd.to_datetime(loadfile_df_copy['PRE_FINANCE_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['GOODS_RECEIPT_NO'] = loadfile_df_copy['GOODS_RECEIPT_NO'].astype(str)\n",
    "        loadfile_df_copy['GOODS_RECEIPT_DATE'] = pd.to_datetime(loadfile_df_copy['GOODS_RECEIPT_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_ENTRY_DATE'] = pd.to_datetime(loadfile_df_copy['INVOICE_ENTRY_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_STATUS'] = loadfile_df_copy['INVOICE_STATUS'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_STATUS_INTERNAL'] = loadfile_df_copy['INVOICE_STATUS_INTERNAL'].astype(str)\n",
    "        loadfile_df_copy['NET_DUE_DATE'] = loadfile_df_copy['NET_DUE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DEBIT_NOTE_NO'] = loadfile_df_copy['DEBIT_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['REMITTANCE_ADVICE_NO'] = loadfile_df_copy['REMITTANCE_ADVICE_NO'].astype(str)\n",
    "        loadfile_df_copy['CLEARING_DATE'] = pd.to_datetime(loadfile_df_copy['CLEARING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['DOCUMENT_NO'] = loadfile_df_copy['DOCUMENT_NO'].astype(str)\n",
    "        loadfile_df_copy['STORE_NO'] = loadfile_df_copy['STORE_NO'].astype(str)\n",
    "        loadfile_df_copy['MATCHING_DATE'] = pd.to_datetime(loadfile_df_copy['MATCHING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['MATCH_STATUS'] = loadfile_df_copy['MATCH_STATUS'].astype(str)\n",
    "        loadfile_df_copy['SYNC_DATE'] = pd.to_datetime(loadfile_df_copy['SYNC_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['SYNC_STATUS'] = loadfile_df_copy['SYNC_STATUS'].astype(str)\n",
    "        loadfile_df_copy['ARKTX'] = loadfile_df_copy['ARKTX'].astype(str)\n",
    "        loadfile_df_copy['Description'] = loadfile_df_copy['Description'].astype(str)\n",
    "        loadfile_df_copy['Bus_year'] = loadfile_df_copy['Bus_year'].astype(str)\n",
    "        loadfile_df_copy['AUGBL'] = loadfile_df_copy['AUGBL'].astype(str)\n",
    "        loadfile_df_copy['BLDAT'] = pd.to_datetime(loadfile_df_copy['BLDAT'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['AUGDT'] = loadfile_df_copy['AUGDT'].astype(str)\n",
    "        \n",
    "        \n",
    "        column_mapping = {\n",
    "            'COMPANY_CODE': 'company_code',\n",
    "            'SUPPLIER_NO': 'supplier_no',\n",
    "            'MIAG_SUPPLIER_NO': 'miag_supplier',\n",
    "            'ORDER_NO': 'order_no',\n",
    "            'DOC_TYPE': 'doc_type',\n",
    "            'INVOICE_NO': 'invoice_no',\n",
    "            'INVOICE_DATE': 'invoice_date',\n",
    "            'DELIVERY_NOTE_NO': 'delivery_note_no',\n",
    "            'TOTAL_AMT_DC': 'total_amt_dc',\n",
    "            'TOTAL_VAT_DC': 'total_vat_dc',\n",
    "            'CURRENCY': 'currency',\n",
    "            'PRE_FINANCE_DATE': 'pre_finance_date',\n",
    "            'GOODS_RECEIPT_NO': 'goods_receipt_no',\n",
    "            'INVOICE_ENTRY_DATE': 'invoice_entry_date',\n",
    "            'INVOICE_STATUS': 'invoice_status',\n",
    "            'INVOICE_STATUS_INTERNAL': 'invoice_status_internal',\n",
    "            'NET_DUE_DATE': 'net_due_date',\n",
    "            'DEBIT_NOTE_NO': 'debit_note_no',\n",
    "            'REMITTANCE_ADVICE_NO': 'remittance_advice_no',\n",
    "            'DOCUMENT_NO': 'document_no',\n",
    "            'STORE_NO': 'store_no',\n",
    "            'ARKTX': 'arktx',\n",
    "            'CLEARING_DATE': 'clearing_date',\n",
    "            'GOODS_RECEIPT_DATE': 'goods_receipt_date',\n",
    "            'MATCHING_DATE': 'matching_date',\n",
    "            'MATCH_STATUS': 'match_status',\n",
    "            'SYNC_DATE': 'sync_date',\n",
    "            'SYNC_STATUS': 'sync_status',\n",
    "            'Description': 'description',\n",
    "            'Bus_year': 'bus_year',\n",
    "            'AUGBL': 'augbl',\n",
    "            'BLDAT': 'bldat',\n",
    "            'AUGDT': 'augdt'\n",
    "        }\n",
    "        date_columns = ['INVOICE_DATE', 'PRE_FINANCE_DATE', 'INVOICE_ENTRY_DATE', 'NET_DUE_DATE', 'CLEARING_DATE',\n",
    "                        'GOODS_RECEIPT_DATE', 'MATCHING_DATE', 'SYNC_DATE', 'BLDAT', 'AUGDT']\n",
    "        for column in date_columns:\n",
    "            loadfile_df_copy[column] = pd.to_datetime(loadfile_df_copy[column], format='%d.%m.%Y').dt.strftime('%m-%d-%Y')\n",
    "        loadfile_df_copy.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from tbl_staged\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "        loadfile_df_copy.to_sql('tbl_staged', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Staged Table of DB Instance...\")\n",
    "        \n",
    "        \n",
    "    def writeProcessedTable(self, loadfile_df_copy):\n",
    "        loadfile_df_copy['COMPANY_CODE'] = loadfile_df_copy['COMPANY_CODE'].astype(str)\n",
    "        loadfile_df_copy['SUPPLIER_NO'] = loadfile_df_copy['SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['MIAG_SUPPLIER_NO'] = loadfile_df_copy['MIAG_SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['ORDER_NO'] = loadfile_df_copy['ORDER_NO'].astype(str)\n",
    "        loadfile_df_copy['DOC_TYPE'] = loadfile_df_copy['DOC_TYPE'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_NO'] = loadfile_df_copy['INVOICE_NO'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_DATE'] = loadfile_df_copy['INVOICE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DELIVERY_NOTE_NO'] = loadfile_df_copy['DELIVERY_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['TOTAL_AMT_DC'] = pd.to_numeric(loadfile_df_copy['TOTAL_AMT_DC'])\n",
    "        loadfile_df_copy['TOTAL_VAT_DC'] = loadfile_df_copy['TOTAL_VAT_DC'].astype(str)\n",
    "        loadfile_df_copy['CURRENCY'] = loadfile_df_copy['CURRENCY'].astype(str)\n",
    "        loadfile_df_copy['PRE_FINANCE_DATE'] = pd.to_datetime(loadfile_df_copy['PRE_FINANCE_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['GOODS_RECEIPT_NO'] = loadfile_df_copy['GOODS_RECEIPT_NO'].astype(str)\n",
    "        loadfile_df_copy['GOODS_RECEIPT_DATE'] = pd.to_datetime(loadfile_df_copy['GOODS_RECEIPT_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_ENTRY_DATE'] = pd.to_datetime(loadfile_df_copy['INVOICE_ENTRY_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_STATUS'] = loadfile_df_copy['INVOICE_STATUS'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_STATUS_INTERNAL'] = loadfile_df_copy['INVOICE_STATUS_INTERNAL'].astype(str)\n",
    "        loadfile_df_copy['NET_DUE_DATE'] = loadfile_df_copy['NET_DUE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DEBIT_NOTE_NO'] = loadfile_df_copy['DEBIT_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['REMITTANCE_ADVICE_NO'] = loadfile_df_copy['REMITTANCE_ADVICE_NO'].astype(str)\n",
    "        loadfile_df_copy['CLEARING_DATE'] = pd.to_datetime(loadfile_df_copy['CLEARING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['DOCUMENT_NO'] = loadfile_df_copy['DOCUMENT_NO'].astype(str)\n",
    "        loadfile_df_copy['STORE_NO'] = loadfile_df_copy['STORE_NO'].astype(str)\n",
    "        loadfile_df_copy['MATCHING_DATE'] = pd.to_datetime(loadfile_df_copy['MATCHING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['MATCH_STATUS'] = loadfile_df_copy['MATCH_STATUS'].astype(str)\n",
    "        loadfile_df_copy['SYNC_DATE'] = pd.to_datetime(loadfile_df_copy['SYNC_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['SYNC_STATUS'] = loadfile_df_copy['SYNC_STATUS'].astype(str)\n",
    "        loadfile_df_copy['ARKTX'] = loadfile_df_copy['ARKTX'].astype(str)\n",
    "        loadfile_df_copy['Description'] = loadfile_df_copy['Description'].astype(str)\n",
    "        loadfile_df_copy['Bus_year'] = loadfile_df_copy['Bus_year'].astype(str)\n",
    "        loadfile_df_copy['AUGBL'] = loadfile_df_copy['AUGBL'].astype(str)\n",
    "        loadfile_df_copy['BLDAT'] = pd.to_datetime(loadfile_df_copy['BLDAT'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['AUGDT'] = loadfile_df_copy['AUGDT'].astype(str)\n",
    "        \n",
    "        \n",
    "        column_mapping = {\n",
    "            'COMPANY_CODE': 'company_code',\n",
    "            'SUPPLIER_NO': 'supplier_no',\n",
    "            'MIAG_SUPPLIER_NO': 'miag_supplier',\n",
    "            'ORDER_NO': 'order_no',\n",
    "            'DOC_TYPE': 'doc_type',\n",
    "            'INVOICE_NO': 'invoice_no',\n",
    "            'INVOICE_DATE': 'invoice_date',\n",
    "            'DELIVERY_NOTE_NO': 'delivery_note_no',\n",
    "            'TOTAL_AMT_DC': 'total_amt_dc',\n",
    "            'TOTAL_VAT_DC': 'total_vat_dc',\n",
    "            'CURRENCY': 'currency',\n",
    "            'PRE_FINANCE_DATE': 'pre_finance_date',\n",
    "            'GOODS_RECEIPT_NO': 'goods_receipt_no',\n",
    "            'INVOICE_ENTRY_DATE': 'invoice_entry_date',\n",
    "            'INVOICE_STATUS': 'invoice_status',\n",
    "            'INVOICE_STATUS_INTERNAL': 'invoice_status_internal',\n",
    "            'NET_DUE_DATE': 'net_due_date',\n",
    "            'DEBIT_NOTE_NO': 'debit_note_no',\n",
    "            'REMITTANCE_ADVICE_NO': 'remittance_advice_no',\n",
    "            'DOCUMENT_NO': 'document_no',\n",
    "            'STORE_NO': 'store_no',\n",
    "            'ARKTX': 'arktx',\n",
    "            'CLEARING_DATE': 'clearing_date',\n",
    "            'GOODS_RECEIPT_DATE': 'goods_receipt_date',\n",
    "            'MATCHING_DATE': 'matching_date',\n",
    "            'MATCH_STATUS': 'match_status',\n",
    "            'SYNC_DATE': 'sync_date',\n",
    "            'SYNC_STATUS': 'sync_status',\n",
    "            'Description': 'description',\n",
    "            'Bus_year': 'bus_year',\n",
    "            'AUGBL': 'augbl',\n",
    "            'BLDAT': 'bldat',\n",
    "            'AUGDT': 'augdt'\n",
    "        }\n",
    "        date_columns = ['INVOICE_DATE', 'PRE_FINANCE_DATE', 'INVOICE_ENTRY_DATE', 'NET_DUE_DATE', 'CLEARING_DATE',\n",
    "                        'GOODS_RECEIPT_DATE', 'MATCHING_DATE', 'SYNC_DATE', 'BLDAT', 'AUGDT']\n",
    "        for column in date_columns:\n",
    "            loadfile_df_copy[column] = pd.to_datetime(loadfile_df_copy[column], format='%d.%m.%Y').dt.strftime('%m-%d-%Y')\n",
    "        loadfile_df_copy.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from tbl_processed\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "        loadfile_df_copy.to_sql('tbl_processed', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Processed Table of DB Instance...\")\n",
    "\n",
    "\n",
    "# db_instance_ops = DB_Instance_Operations()\n",
    "# sdp_df = db_instance_ops.readSDPTable()\n",
    "# sdp_df = db_instance_ops.updateSDP(sdp_df, sftp_df)\n",
    "# db_instance_ops.writeSDPTable(sdp_df)\n",
    "# sdp_supplier_list_for_mmsic = db_instance_ops.getSupplierNumberForMMSIC()\n",
    "# sdp_supplier_list_for_sisic = db_instance_ops.getSupplierNumberForSISIC()\n",
    "# sdp_supplier_list_for_fi = db_instance_ops.getSupplierNumberForFI()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f2b76b-b1d9-423a-b25e-00906f5808e0",
   "metadata": {},
   "source": [
    "# BigQuery_Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11125a77-fbf8-4f7e-bc60-ef0e93657466",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "class BigQuery_Operations:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client()\n",
    "\n",
    "    def extract_MMSIC(self, sdp_supplier_list_for_mmsic):\n",
    "        add_string = \"\"\n",
    "        for i in range(len(sdp_supplier_list_for_mmsic)):\n",
    "            add_string += \"'\"\n",
    "            add_string += str(sdp_supplier_list_for_mmsic[i])\n",
    "            add_string += \"'\"\n",
    "            add_string += \", \"\n",
    "        add_string = add_string[:-2]\n",
    "        query = f\"\"\"\n",
    "                WITH LatestRecords AS (\n",
    "    SELECT \n",
    "        LIFNR, \n",
    "        RENR, \n",
    "        MAX(dana_ingestion_timestamp) AS latest_timestamp\n",
    "    FROM \n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.mmsic_to_dana_gr_invoice_header`\n",
    "    GROUP BY \n",
    "        LIFNR, RENR\n",
    ")\n",
    "SELECT \n",
    "    T1.LIFNR, \n",
    "    T1.BELNR, \n",
    "    T1.RENR, \n",
    "    T1.REDAT, \n",
    "    T1.LFSNR, \n",
    "    T1.GEBRF, \n",
    "    T1.GSMWB, \n",
    "    T1.GSMWF,\n",
    "    T1.WAERS,\n",
    "    T1.WENUM,\n",
    "    T1.RGDAT,\n",
    "    T1.ABGST,\n",
    "    T1.AUFNR,\n",
    "    T1.VORGN,\n",
    "    T1.GJAHR,\n",
    "    T2.WEDAT,\n",
    "    T1.DEBNOTNO\n",
    "FROM \n",
    "    `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.mmsic_to_dana_gr_invoice_header` AS T1\n",
    "LEFT JOIN (\n",
    "    SELECT DISTINCT  \n",
    "        VORGN, \n",
    "        WEDAT, \n",
    "        GJAHR \n",
    "    FROM\n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.mmsic_to_dana_gr_table_header`\n",
    ") AS T2\n",
    "ON \n",
    "    T1.VORGN = T2.VORGN\n",
    "JOIN \n",
    "    LatestRecords LR\n",
    "ON \n",
    "    T1.LIFNR = LR.LIFNR \n",
    "    AND T1.RENR = LR.RENR \n",
    "    AND T1.dana_ingestion_timestamp = LR.latest_timestamp\n",
    "WHERE\n",
    "    T1.LIFNR IN ({add_string})\n",
    "ORDER BY \n",
    "    T1.dana_ingestion_timestamp;\n",
    "                \"\"\"\n",
    "        extracted_mmsic_df = self.client.query(query).to_dataframe()\n",
    "        return extracted_mmsic_df\n",
    "\n",
    "    def extract_SISIC(self, sdp_supplier_list_for_sisic):\n",
    "        add_string = \"\"\n",
    "        for i in range(len(sdp_supplier_list_for_sisic)):\n",
    "            add_string += \"'\"\n",
    "            add_string += str(sdp_supplier_list_for_sisic[i])\n",
    "            add_string += \"'\"\n",
    "            add_string += \", \"\n",
    "        add_string = add_string[:-2]\n",
    "        query = f\"\"\"\n",
    "                WITH LatestRecords AS (\n",
    "    SELECT \n",
    "        LIFNR, \n",
    "        RENR, \n",
    "        MAX(dana_ingestion_timestamp) AS latest_timestamp\n",
    "    FROM \n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.sis_to_dana_gr_invoice_header`\n",
    "    GROUP BY \n",
    "        LIFNR, RENR\n",
    ")\n",
    "SELECT \n",
    "    T1.LIFNR, \n",
    "    T1.BELNR, \n",
    "    T1.RENR, \n",
    "    T1.REDAT, \n",
    "    T1.LFSNR, \n",
    "    T1.GEBRF, \n",
    "    T1.GSMWB, \n",
    "    T1.GSMWF,\n",
    "    T1.WAERS,\n",
    "    T1.WENUM,\n",
    "    T1.RGDAT,\n",
    "    T1.ABGST,\n",
    "    T1.AUFNR,\n",
    "    T1.VORGN,\n",
    "    T1.GJAHR,\n",
    "    T2.WEDAT,\n",
    "    T1.DEBNOTNO\n",
    "FROM \n",
    "    `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.sis_to_dana_gr_invoice_header` AS T1\n",
    "LEFT JOIN (\n",
    "    SELECT DISTINCT  \n",
    "        VORGN, \n",
    "        WEDAT, \n",
    "        GJAHR \n",
    "    FROM\n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.sis_to_dana_gr_table_header`\n",
    ") AS T2\n",
    "ON \n",
    "    T1.VORGN = T2.VORGN\n",
    "JOIN \n",
    "    LatestRecords LR\n",
    "ON \n",
    "    T1.LIFNR = LR.LIFNR \n",
    "    AND T1.RENR = LR.RENR \n",
    "    AND T1.dana_ingestion_timestamp = LR.latest_timestamp\n",
    "WHERE\n",
    "    T1.LIFNR IN ({add_string})\n",
    "ORDER BY \n",
    "    T1.dana_ingestion_timestamp;\n",
    "                \"\"\"\n",
    "        extracted_sisic_df = self.client.query(query).to_dataframe()\n",
    "        return extracted_sisic_df\n",
    "\n",
    "    def extract_FI(self, sdp_supplier_list_for_fi):\n",
    "        add_string = \"\"\n",
    "        for i in range(len(sdp_supplier_list_for_fi)):\n",
    "            add_string += str(sdp_supplier_list_for_fi[i])\n",
    "            add_string += \", \"\n",
    "        add_string = add_string[:-2]\n",
    "        query = f\"\"\"\n",
    "                    DECLARE country STRING DEFAULT 'tur';\n",
    "        DECLARE current_fiscal_year INT64 DEFAULT 2025;\n",
    "        DECLARE store_flag STRING DEFAULT 'prctr';-- or 'gsber';\n",
    "        DECLARE end_month_id INT64 DEFAULT EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) * 100 + EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH));    \n",
    "        DECLARE start_year INT64 DEFAULT 2007;\n",
    "        DECLARE end_year INT64 DEFAULT 2025;\n",
    "        CREATE OR REPLACE TABLE metro-bi-wb-mag-figov-s00.data_integrity_proj.sap_tur_360data_BELNR_testdoc\n",
    "        AS(\n",
    "        WITH fidoc AS (\n",
    "        SELECT * ,\n",
    "        MAX  (dana_ingestion_timestamp) over (PARTITION BY MANDT, BELNR, GJAHR, BUKRS, bseg.BUZEI) as max_timestamp,\n",
    "        ROW_NUMBER () over (PARTITION BY MANDT, BELNR, GJAHR, BUKRS, bseg.BUZEI order by dana_ingestion_timestamp DESC) as rn\n",
    "        FROM metro-bi-dl-tur-prod.ingest_fgtf_sap.fidoc fi,\n",
    "        UNNEST (zbseg) AS bseg\n",
    "        WHERE 1=1\n",
    "        AND gjahr BETWEEN start_year AND end_year\n",
    "        ),\n",
    "        fidoc_unique AS (\n",
    "        SELECT *\n",
    "        , CASE  WHEN store_flag = 'gsber' THEN fi.gsber\n",
    "            WHEN store_flag = 'prctr' THEN fi.prctr\n",
    "        END AS business_area\n",
    "        FROM fidoc fi\n",
    "        WHERE fi.dana_ingestion_timestamp  = max_timestamp\n",
    "        AND rn = 1\n",
    "        )\n",
    "        SELECT\n",
    "        fi.MANDT\n",
    "        , zbkpf.blart AS Document_type\n",
    "        , doc_type.ltext as document_type_desc\n",
    "        , GJAHR\n",
    "        , BUKRS\n",
    "        , GSBER\n",
    "        , PRCTR\n",
    "        , cast(prctr as int64)-cast(bukrs as int64)*10000 as store_or_dc\n",
    "        , KOSTL\n",
    "        , zbkpf.monat as month_in_fin_year\n",
    "        , BELNR\n",
    "        , zbkpf.XBLNR\n",
    "        --, AUFNR\n",
    "        , AUGBL\n",
    "        , AUGDT\n",
    "        , ZFBDT\n",
    "        , ZBD1T\n",
    "        , ZBD2T\n",
    "        , NETDT\n",
    "        , BUZEI\n",
    "        , altkt\n",
    "        , hkont\n",
    "        , MOD(SAFE_CAST(fi.z_dana_lfa1.lifnr AS int64), 100000) as suppl_no\n",
    "        , zbkpf.BLDAT\n",
    "        , zbkpf.BUDAT\n",
    "        , zbkpf.CPUDT\n",
    "        , date(fi.PARTITIONTIME) partition_date\n",
    "        , date(fi.dana_ingestion_timestamp) dana_ingestion_date\n",
    "        , shkzg\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.dmbtr\n",
    "                    ELSE fi.dmbtr\n",
    "                    END                 as Amount_in_local_currency\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.wrbtr\n",
    "                    ELSE fi.wrbtr\n",
    "                    END                 as Amount_in_document_currency\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.mwsts\n",
    "                    ELSE fi.mwsts\n",
    "                    END                 as Tax_in_local_currency\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.wmwst\n",
    "                    ELSE fi.wmwst\n",
    "                    END                 as Tax_in_document_currency\n",
    "        , zbkpf.WAERS\n",
    "        , ZBKPF.GRPID AS Batch_Input_session_name\n",
    "        , sgtxt\n",
    "        -- *\n",
    "        FROM fidoc_unique fi\n",
    "        LEFT JOIN\n",
    "        ( select * from metro-bi-dl-tur-prod.ingest_fgtf_sap.t003t AS doc_type\n",
    "            WHERE 1=1\n",
    "            AND doc_type.spras = 'EN'\n",
    "            qualify dana_ingestion_timestamp = max(dana_ingestion_timestamp) over (partition by BLART, MANDT, SPRAS, SYSID)\n",
    "            order by doc_type.blart\n",
    "        ) AS doc_type\n",
    "            ON zbkpf.blart = doc_type.blart\n",
    "        where 1=1    \n",
    "        and MOD(SAFE_CAST(fi.z_dana_lfa1.lifnr AS int64), 100000) IN ({add_string})\n",
    "        )\n",
    "                \"\"\"\n",
    "        extracted_fi_full_df = self.client.query(query).to_dataframe()\n",
    "        query2 = f\"select * from `metro-bi-wb-mag-figov-s00.data_integrity_proj.sap_tur_360data_BELNR_testdoc`\"\n",
    "        extracted_fi_df = self.client.query(query2).to_dataframe()\n",
    "        # extracted_fi_df = extracted_fi_df.drop('BUZEI', axis=1)\n",
    "        return extracted_fi_df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     bq_ops = BigQuery_Operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33d8d883-1ce5-4812-afdf-9e274894e9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sslrootcert downloaded and stored temporarily at /var/tmp/tmpes__5gbb\n",
      "sslcert downloaded and stored temporarily at /var/tmp/tmp6txboy0g\n",
      "sslkey downloaded and stored temporarily at /var/tmp/tmphvcipvt5\n"
     ]
    }
   ],
   "source": [
    "storage_bucket_operations = Storage_Bucket_Operations()\n",
    "db_instance_operations = DB_Instance_Operations()\n",
    "bigquery_operations = BigQuery_Operations()\n",
    "latest_sftp_file = Latest_SFTP_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49f33456-78c8-4615-a3cd-4501933e19f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The latest file in 'Downloaded Files' is: Downloaded Files/miag.35.288125.20250205.63.csv\n",
      "SFTP file :  Downloaded Files/miag.35.288125.20250205.63.csv\n",
      "SFTP shape :  (164079, 18)\n"
     ]
    }
   ],
   "source": [
    "sftp_file = latest_sftp_file.get_latest_file()\n",
    "print(\"SFTP file : \", sftp_file)\n",
    "sftp_df = storage_bucket_operations.readFromBucket(sftp_file)\n",
    "print(\"SFTP shape : \", sftp_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2de55d58-e591-412c-888a-6164dc676820",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sftp_df['SFTP_bus_year'] = sftp_df['Document date'].str.split('.').str[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e520ac6-b71c-4a71-ad0c-178b22df79b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftp_df['SFTP_bus_year'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02b61754-5196-4654-b705-e6c1bbad14d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sftp_mv_df = sftp_df[sftp_df['Document type']=='MV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "839ea301-1874-4df0-a613-6df27adadcda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sftp_non_mv_df = sftp_df[sftp_df['Document type']!='MV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14e0b56c-ca1e-4ab8-be22-2b09915ea75d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 19)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftp_mv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7108d2db-dd38-4f45-9fd3-2d0488d3cd08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(163358, 19)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftp_non_mv_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cf8263e-4846-4bf6-928c-226ca1e43dac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_group(group):\n",
    "    if len(group) == 2:\n",
    "        descriptions = group['Description'].tolist()\n",
    "        if 'open' in descriptions and 'pre-financed' in descriptions:\n",
    "            # Keep only the 'open' record\n",
    "            group = group[group['Description'] != 'pre-financed']\n",
    "    return group\n",
    "\n",
    "# Apply filtering group-wise\n",
    "sftp_result_df = (\n",
    "    sftp_mv_df.groupby(\n",
    "        ['Supplier number (Sales Line)', 'Document number', 'Invoice number', 'SFTP_bus_year'],\n",
    "        group_keys=False\n",
    "    ).apply(filter_group)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cb2c396-e22e-4da9-a682-e6145127d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_1st_merged_df = pd.merge(\n",
    "    sftp_result_df,\n",
    "    sftp_non_mv_df,\n",
    "    on=['Supplier number (Sales Line)', 'Document number', 'Invoice number', 'SFTP_bus_year'],\n",
    "    how='left',\n",
    "    suffixes=('_x', '_y')  # Explicitly naming suffixes for clarity\n",
    ")\n",
    "\n",
    "# Ensure 'Document type' columns are handled properly\n",
    "mv_1st_merged_df['Combined_Document_types'] = mv_1st_merged_df[['Document type_x', 'Document type_y']].apply(\n",
    "    lambda row: [val for val in row if pd.notna(val)], axis=1\n",
    ")\n",
    "mv_1st_merged_df['RAN'] = mv_1st_merged_df[['Remittance advice number_x', 'Remittance advice number_y']].apply(\n",
    "    lambda row: [val for val in row if pd.notna(val)], axis=1\n",
    ")\n",
    "# Group by the relevant columns in sftp_mv_df\n",
    "grouped = mv_1st_merged_df.groupby(\n",
    "    ['Supplier number (Sales Line)', 'Document number', 'Invoice number', 'SFTP_bus_year']\n",
    ")\n",
    "\n",
    "# Aggregate the results\n",
    "old_mv_metadata_df = grouped.agg(\n",
    "    Document_Count=('Combined_Document_types', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "    Document_types=('Combined_Document_types', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique document types\n",
    "    RAN_count=('RAN', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "    RAN_list=('RAN', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item))))  # Flattened unique list ignoring NaN\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16ce6a1b-f17d-4d3f-87ca-306b1ba5f179",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Supplier number (Sales Line)</th>\n",
       "      <th>Document number</th>\n",
       "      <th>Invoice number</th>\n",
       "      <th>SFTP_bus_year</th>\n",
       "      <th>Document_Count</th>\n",
       "      <th>Document_types</th>\n",
       "      <th>RAN_count</th>\n",
       "      <th>RAN_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000019596</td>\n",
       "      <td>0810009095</td>\n",
       "      <td>J012024000000011</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>[WE, MV]</td>\n",
       "      <td>1</td>\n",
       "      <td>[03517411659]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000019596</td>\n",
       "      <td>0810009096</td>\n",
       "      <td>J012024000000010</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>[WE, MV]</td>\n",
       "      <td>1</td>\n",
       "      <td>[03517411659]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000019596</td>\n",
       "      <td>0810102410</td>\n",
       "      <td>L102024000000029</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>[MV]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000019596</td>\n",
       "      <td>0810104143</td>\n",
       "      <td>L102024000000028</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>[MV]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000019596</td>\n",
       "      <td>0810112628</td>\n",
       "      <td>L172024000000011</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>[MV]</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>1000061122</td>\n",
       "      <td>0810422866</td>\n",
       "      <td>ERS2024000047872</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>[WE, MV]</td>\n",
       "      <td>1</td>\n",
       "      <td>[03517187345]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>1000061122</td>\n",
       "      <td>0810422944</td>\n",
       "      <td>ERS2024000047874</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>[WE, MV]</td>\n",
       "      <td>1</td>\n",
       "      <td>[03517187345]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>1000061122</td>\n",
       "      <td>0810423000</td>\n",
       "      <td>ERS2024000047999</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>[WE, MV]</td>\n",
       "      <td>1</td>\n",
       "      <td>[03517187345]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>1000061122</td>\n",
       "      <td>0810423001</td>\n",
       "      <td>ERS2024000047998</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>[WE, MV]</td>\n",
       "      <td>1</td>\n",
       "      <td>[03517187345]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>1000061122</td>\n",
       "      <td>0810423004</td>\n",
       "      <td>ERS2024000048040</td>\n",
       "      <td>2024</td>\n",
       "      <td>2</td>\n",
       "      <td>[WE, MV]</td>\n",
       "      <td>1</td>\n",
       "      <td>[03517187345]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>721 rows  8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Supplier number (Sales Line) Document number    Invoice number  \\\n",
       "0                     1000019596      0810009095  J012024000000011   \n",
       "1                     1000019596      0810009096  J012024000000010   \n",
       "2                     1000019596      0810102410  L102024000000029   \n",
       "3                     1000019596      0810104143  L102024000000028   \n",
       "4                     1000019596      0810112628  L172024000000011   \n",
       "..                           ...             ...               ...   \n",
       "716                   1000061122      0810422866  ERS2024000047872   \n",
       "717                   1000061122      0810422944  ERS2024000047874   \n",
       "718                   1000061122      0810423000  ERS2024000047999   \n",
       "719                   1000061122      0810423001  ERS2024000047998   \n",
       "720                   1000061122      0810423004  ERS2024000048040   \n",
       "\n",
       "    SFTP_bus_year  Document_Count Document_types  RAN_count       RAN_list  \n",
       "0            2024               2       [WE, MV]          1  [03517411659]  \n",
       "1            2024               2       [WE, MV]          1  [03517411659]  \n",
       "2            2024               1           [MV]          0             []  \n",
       "3            2024               1           [MV]          0             []  \n",
       "4            2024               1           [MV]          0             []  \n",
       "..            ...             ...            ...        ...            ...  \n",
       "716          2024               2       [WE, MV]          1  [03517187345]  \n",
       "717          2024               2       [WE, MV]          1  [03517187345]  \n",
       "718          2024               2       [WE, MV]          1  [03517187345]  \n",
       "719          2024               2       [WE, MV]          1  [03517187345]  \n",
       "720          2024               2       [WE, MV]          1  [03517187345]  \n",
       "\n",
       "[721 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_mv_metadata_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3e6dc2a-98c6-4ca1-b662-8439ce83996d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "571e03b0-9149-45e8-adc6-f5a9e857ebdd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_mv_metadata_df[old_mv_metadata_df['Document_Count']==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e41ea4f5-1a5a-45f8-8b4e-1fa74466c353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_mv_metadata_df[old_mv_metadata_df['Document_Count']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83600ab8-2ddc-4346-b3ab-cde08ab6c16a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_mv_metadata_df[(old_mv_metadata_df['Document_Count']==1) & (old_mv_metadata_df['RAN_count']==1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7355925a-3541-4c82-856f-4dda32050ae5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_mv_metadata_df[(old_mv_metadata_df['Document_Count']==1) & (old_mv_metadata_df['RAN_count']==0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d46697bb-8cc2-4f07-80cc-dceedf3608be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "696cd780-8a03-4a0c-a3aa-a6fbd2f410fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_paid_df = old_mv_metadata_df[(old_mv_metadata_df['Document_Count']==1) &  (old_mv_metadata_df['Document_types'].apply(lambda x: x == ['MV'])) & (old_mv_metadata_df['RAN_count']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c2e1fa4-a0ac-4f35-a326-8589dee398af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Supplier number (Sales Line)', 'Document number', 'Invoice number',\n",
       "       'SFTP_bus_year', 'Document_Count', 'Document_types', 'RAN_count',\n",
       "       'RAN_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_paid_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cac460-a73d-4366-8d63-1e0c74d60899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73d76db7-993d-49ae-abf9-18db2be39faa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sftp_df = pd.concat([sftp_non_mv_df, sftp_result_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04000423-33bd-4d69-9db0-32ff1ea5dc25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New SFTP length after removing pre-financed records :  164079\n"
     ]
    }
   ],
   "source": [
    "print(\"New SFTP length after removing pre-financed records : \", len(sftp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed7614fd-773e-44ca-8869-beac30f42e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Contract area', 'Supplier number (Sales Line)',\n",
       "       'Supplier number (MIAG)', 'Supplier name', 'VAT number',\n",
       "       'Document number', 'Invoice number', 'Document type', 'Document date',\n",
       "       'Remittance advice number', 'Value date', 'Currency', 'Gross amount',\n",
       "       'Description', 'Contract indicator', 'Store', 'Company code', 'ARKTX',\n",
       "       'SFTP_bus_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftp_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb2ba6a9-2029-4a6c-acc2-fe7a9564f142",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written back to SDP Table of DB Instance...\n"
     ]
    }
   ],
   "source": [
    "sdp_df = db_instance_operations.readSDPTable()\n",
    "sdp_df = db_instance_operations.updateSDP(sdp_df, sftp_df)\n",
    "db_instance_operations.writeSDPTable(sdp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c428423-8e66-412d-aad3-c3c14bbd79fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdp_supplier_list_for_mmsic = db_instance_operations.getSupplierNumberForMMSIC()\n",
    "sdp_supplier_list_for_sisic = db_instance_operations.getSupplierNumberForSISIC()\n",
    "sdp_supplier_list_for_fi = db_instance_operations.getSupplierNumberForFI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b856ffa2-b781-4ce3-bd55-1698ae399f00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "88\n",
      "88\n"
     ]
    }
   ],
   "source": [
    "print(len(sdp_supplier_list_for_mmsic))\n",
    "print(len(sdp_supplier_list_for_sisic))\n",
    "print(len(sdp_supplier_list_for_fi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "703532f2-fc31-4ef3-814e-d5567e961782",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi = bigquery_operations.extract_FI(sdp_supplier_list_for_fi)\n",
    "sisic_df = bigquery_operations.extract_SISIC(sdp_supplier_list_for_sisic)\n",
    "mmsic_df = bigquery_operations.extract_MMSIC(sdp_supplier_list_for_mmsic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4355c3b9-8af3-48df-87a0-0246f836286a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3009645, 35)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9cb15efa-49cc-4275-8f6f-0c227306b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi = df_fi.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "sisic_df = sisic_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "mmsic_df = mmsic_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e073aea-65c5-40c5-99cd-f9bc05e8ba55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi_copy = df_fi.copy()\n",
    "df_ic = pd.concat([mmsic_df, sisic_df], ignore_index=True)\n",
    "df_ic_copy = df_ic.copy()\n",
    "df_fi = df_fi[df_fi['Document_type'] != 'PM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ed51e243-3dd2-4ee8-8b05-7a900b9c45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_to_type_ic = dict(zip(df_ic['BELNR'], df_ic['RENR']))\n",
    "df_fi['XBLNR'] = df_fi['XBLNR'].fillna(df_fi['BELNR'].map(doc_to_type_ic))\n",
    "doc_to_type_fi = dict(zip(df_fi['XBLNR'], df_fi['BELNR']))\n",
    "df_ic['BELNR'] = df_ic['BELNR'].str.strip().replace('', np.nan)\n",
    "df_ic['BELNR'] = df_ic['BELNR'].fillna(df_ic['RENR'].map(doc_to_type_fi))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e946e8-999b-4bfb-a8b1-e91858bfd8ac",
   "metadata": {},
   "source": [
    "# Cleaning FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c06182c-5a37-4951-b281-9abe895e4760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique records in FI before cleaning :  913760\n"
     ]
    }
   ],
   "source": [
    "fi_unique_count = df_fi[['suppl_no', 'BELNR', 'XBLNR', 'BLDAT']].drop_duplicates().shape[0]\n",
    "print(\"Count of unique records in FI before cleaning : \", fi_unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "018dc903-5964-406c-aa79-3f3162f6130b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MANDT                                  object\n",
       "Document_type                          object\n",
       "document_type_desc                     object\n",
       "GJAHR                                   int64\n",
       "BUKRS                                  object\n",
       "GSBER                                  object\n",
       "PRCTR                                  object\n",
       "store_or_dc                            object\n",
       "KOSTL                                  object\n",
       "month_in_fin_year                       int64\n",
       "BELNR                                  object\n",
       "XBLNR                                  object\n",
       "AUGBL                                  object\n",
       "AUGDT                                  object\n",
       "ZFBDT                                  object\n",
       "ZBD1T                                 float64\n",
       "ZBD2T                                 float64\n",
       "NETDT                          datetime64[ns]\n",
       "BUZEI                                   int64\n",
       "altkt                                  object\n",
       "hkont                                  object\n",
       "suppl_no                                int64\n",
       "BLDAT                                  object\n",
       "BUDAT                                  object\n",
       "CPUDT                                  object\n",
       "partition_date                         object\n",
       "dana_ingestion_date                    object\n",
       "shkzg                                  object\n",
       "Amount_in_local_currency              float64\n",
       "Amount_in_document_currency           float64\n",
       "Tax_in_local_currency                 float64\n",
       "Tax_in_document_currency              float64\n",
       "WAERS                                  object\n",
       "Batch_Input_session_name               object\n",
       "sgtxt                                  object\n",
       "dtype: object"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fi.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5d1144bb-36ba-48f3-9857-4951cc0bceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi['suppl_no'] = df_fi['suppl_no'].astype(str)\n",
    "df_fi['BLDAT'] = df_fi['BLDAT'].astype(str)\n",
    "df_fi['BELNR'] = df_fi['BELNR'].astype(str)\n",
    "df_fi['XBLNR'] = df_fi['XBLNR'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "16da499b-7829-40a8-b1da-61a45c03966d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_nos_with_no_ZFBDT = []\n",
    "doc_nos_with_1_ZFBDT = []\n",
    "doc_nos_with_more_ZFBDT = []\n",
    "\n",
    "def select_final_row(group):\n",
    "    # Filter rows where ZFBDT is present\n",
    "    group_with_ZFBDT = group[group['ZFBDT'].notna()]\n",
    "    number_ZFBDT_present = len(group_with_ZFBDT)\n",
    "\n",
    "    # Case: number_ZFBDT_present > 1\n",
    "    if number_ZFBDT_present > 1:\n",
    "        doc_nos_with_more_ZFBDT.append(\n",
    "(group['suppl_no'].iloc[0], group['BELNR'].iloc[0], group['XBLNR'].iloc[0], group['BLDAT'].iloc[0]))\n",
    "        # Check if any rows have ZBD1T or ZBD2T present\n",
    "        rows_with_ZBD = group_with_ZFBDT[group_with_ZFBDT['ZBD1T'].notna() | group_with_ZFBDT['ZBD2T'].notna()]\n",
    "        if not rows_with_ZBD.empty:\n",
    "            # Select the first row from rows_with_ZBD\n",
    "            final_selected_row = rows_with_ZBD.iloc[0]\n",
    "        else:\n",
    "            # Select the row with the least ZFBDT\n",
    "            min_ZFBDT_rows = group_with_ZFBDT[group_with_ZFBDT['ZFBDT'] == group_with_ZFBDT['ZFBDT'].min()]\n",
    "            # If multiple rows have the least ZFBDT, choose the one with the least BUZEI\n",
    "            final_selected_row = min_ZFBDT_rows.loc[min_ZFBDT_rows['BUZEI'].idxmin()]\n",
    "    elif number_ZFBDT_present == 1:\n",
    "        doc_nos_with_1_ZFBDT.append(\n",
    "(group['suppl_no'].iloc[0], group['BELNR'].iloc[0], group['XBLNR'].iloc[0], group['BLDAT'].iloc[0])\n",
    ")\n",
    "        # Case: number_ZFBDT_present == 1\n",
    "        final_selected_row = group_with_ZFBDT.iloc[0]\n",
    "    else:\n",
    "        # Case: number_ZFBDT_present == 0\n",
    "        # Select the row with the least BUZEI\n",
    "        doc_nos_with_no_ZFBDT.append(\n",
    "(group['suppl_no'].iloc[0], group['BELNR'].iloc[0], group['XBLNR'].iloc[0], group['BLDAT'].iloc[0])\n",
    ")\n",
    "        final_selected_row = group.loc[group['BUZEI'].idxmin()]\n",
    "\n",
    "    return final_selected_row\n",
    "\n",
    "df_fi = (\n",
    "            df_fi\n",
    "            .groupby(['suppl_no', 'BELNR', 'XBLNR', 'BLDAT'], group_keys=False)\n",
    "            .apply(select_final_row)\n",
    "            .reset_index(drop=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19ea1f3f-4905-4ba1-a335-803a8ac76c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi['BELNR'] = df_fi['BELNR'].replace('nan', np.nan)\n",
    "df_fi['XBLNR'] = df_fi['XBLNR'].replace('nan', np.nan)\n",
    "df_fi['BLDAT'] = df_fi['BLDAT'].replace('nan', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0919961a-4a8f-4276-bc2e-bf26a6df79c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi['suppl_no'] = df_fi['suppl_no'].astype('int64')\n",
    "df_fi['BLDAT'] = df_fi['BLDAT'].astype('object')\n",
    "df_fi['BELNR'] = df_fi['BELNR'].astype('object')\n",
    "df_fi['XBLNR'] = df_fi['XBLNR'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "481791da-89b5-4004-a6c1-5d13bc0f1027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FI shape after cleaning :  (913760, 35)\n"
     ]
    }
   ],
   "source": [
    "print(\"FI shape after cleaning : \", df_fi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4a4980-683d-4b5f-a597-c97d91ef12ab",
   "metadata": {},
   "source": [
    "# Writing FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3ba5d040-4813-4f4f-ae19-85efaf3104a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type casted cleaned FI for storage\n"
     ]
    }
   ],
   "source": [
    "column_types = {\n",
    "            \"MANDT\": \"object\",\n",
    "            \"Document_type\": \"object\",\n",
    "            \"document_type_desc\": \"object\",\n",
    "            \"GJAHR\": \"Int64\",  # Nullable integer\n",
    "            \"BUKRS\": \"object\",\n",
    "            \"GSBER\": \"object\",\n",
    "            \"PRCTR\": \"object\",\n",
    "            \"store_or_dc\": \"Int64\",  # Nullable integer\n",
    "            \"KOSTL\": \"object\",\n",
    "            \"month_in_fin_year\": \"Int64\",  # Nullable integer\n",
    "            \"BELNR\": \"object\",\n",
    "            \"XBLNR\": \"object\",\n",
    "            \"AUGBL\": \"object\",\n",
    "            \"AUGDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"ZFBDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"ZBD1T\": \"float64\",\n",
    "            \"ZBD2T\": \"float64\",\n",
    "            \"NETDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"BUZEI\": \"Int64\",  # Nullable integer\n",
    "            \"altkt\": \"object\",\n",
    "            \"hkont\": \"object\",\n",
    "            \"suppl_no\": \"Int64\",  # Nullable integer\n",
    "            \"BLDAT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"BUDAT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"CPUDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"partition_date\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"dana_ingestion_date\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"shkzg\": \"object\",\n",
    "            \"Amount_in_local_currency\": \"float64\",\n",
    "            \"Amount_in_document_currency\": \"float64\",\n",
    "            \"Tax_in_local_currency\": \"float64\",\n",
    "            \"Tax_in_document_currency\": \"float64\",\n",
    "            \"WAERS\": \"object\",\n",
    "            \"Batch_Input_session_name\": \"object\",\n",
    "            \"sgtxt\": \"object\",\n",
    "        }\n",
    "\n",
    "        # Convert columns to the appropriate types\n",
    "for col, dtype in column_types.items():\n",
    "    if dtype == \"datetime64[ns]\":\n",
    "        # Convert to datetime while handling errors\n",
    "        df_fi[col] = pd.to_datetime(df_fi[col], errors=\"coerce\")\n",
    "    elif dtype == \"Int64\":\n",
    "        # Convert to Pandas nullable integer\n",
    "        df_fi[col] = df_fi[col].astype(\"Int64\")\n",
    "    else:\n",
    "        # Convert to the specified type\n",
    "        df_fi[col] = df_fi[col].astype(dtype)\n",
    "\n",
    "print(\"Type casted cleaned FI for storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ca8baf7-02bd-4e13-a17f-f9e77c2d993b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to Intermediate FI Table of DB Instance...\n"
     ]
    }
   ],
   "source": [
    "db_instance_operations.writeFITable(df_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7f8f3-9a92-4665-a0c1-9b98e17e3959",
   "metadata": {},
   "source": [
    "# Cleaning IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "69802f8c-8ecc-449d-a416-250c6ad26bb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique records in IC before cleaning :  463262\n"
     ]
    }
   ],
   "source": [
    "ic_unique_count = df_ic[['LIFNR', 'BELNR', 'RENR', 'REDAT']].drop_duplicates().shape[0]\n",
    "print(\"Count of unique records in IC before cleaning : \", ic_unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bd2133ad-83aa-4c3f-846e-c155a1ca35d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LIFNR       object\n",
       "BELNR       object\n",
       "RENR        object\n",
       "REDAT       object\n",
       "LFSNR       object\n",
       "GEBRF       object\n",
       "GSMWB       object\n",
       "GSMWF       object\n",
       "WAERS       object\n",
       "WENUM       object\n",
       "RGDAT       object\n",
       "ABGST       object\n",
       "AUFNR       object\n",
       "VORGN       object\n",
       "GJAHR       object\n",
       "WEDAT       object\n",
       "DEBNOTNO    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ic.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "59acb78f-8405-41f0-ba75-78bc41bf0873",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic[['LIFNR', 'BELNR', 'RENR', 'REDAT']] = df_ic[['LIFNR', 'BELNR', 'RENR', 'REDAT']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e930b3dd-c95d-455d-8ce7-d2315280e1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_populated_row(group):\n",
    "    # Count non-null values for each row\n",
    "    non_null_counts = group.notnull().sum(axis=1)\n",
    "    # Get the index of the row with the maximum count\n",
    "    max_index = non_null_counts.idxmax()\n",
    "    return group.loc[max_index]\n",
    "\n",
    "df_ic = (\n",
    "            df_ic\n",
    "            .groupby(['LIFNR', 'BELNR', 'RENR', 'REDAT'])\n",
    "            .apply(get_max_populated_row)\n",
    "            .reset_index(drop=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27eeaceb-35a5-41ea-b541-a331891f0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic['LIFNR'] = df_ic['LIFNR'].replace('nan', np.nan)\n",
    "df_ic['BELNR'] = df_ic['BELNR'].replace('nan', np.nan)\n",
    "df_ic['RENR'] = df_ic['RENR'].replace('nan', np.nan)\n",
    "# df_ic['GEBRF'] = df_ic['GEBRF'].replace('nan', np.nan)\n",
    "df_ic['REDAT'] = df_ic['REDAT'].replace('nan', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "629ec92f-8f70-42d3-a18e-0bdc0a7ce26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic['LIFNR'] = df_ic['LIFNR'].astype('object')\n",
    "df_ic['BELNR'] = df_ic['BELNR'].astype('object')\n",
    "df_ic['RENR'] = df_ic['RENR'].astype('object')\n",
    "# df_ic['GEBRF'] = df_ic['GEBRF'].astype('object')\n",
    "df_ic['REDAT'] = df_ic['REDAT'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8122d959-ae91-4dcb-a0d8-686114a78ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC shape after cleaning :  (463262, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"IC shape after cleaning : \", df_ic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f870aa-deba-4d52-a32d-97541dd53f02",
   "metadata": {},
   "source": [
    "# Writing IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "32530b4c-b0da-43ba-87b9-fdf23b31e6cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to Intermediate IC Table of DB Instance...\n"
     ]
    }
   ],
   "source": [
    "db_instance_operations.writeICTable(df_ic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "92d8a227-f3a6-4ac2-8d41-a52484602ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi.columns = ['MANDT', 'Document_type', 'document_type_desc', 'GJAHR', 'BUKRS',\n",
    "                         'GSBER', 'PRCTR', 'store_or_dc', 'KOSTL', 'month_in_fin_year', 'BELNR',\n",
    "                         'XBLNR', 'AUGBL', 'AUGDT', 'ZFBDT', 'ZBD1T', 'ZBD2T', 'NETDT', 'BUZEI',\n",
    "                         'altkt', 'hkont', 'suppl_no', 'BLDAT', 'BUDAT', 'CPUDT',\n",
    "                         'partition_date', 'dana_ingestion_date', 'shkzg',\n",
    "                         'Amount_in_local_currency', 'Amount_in_document_currency',\n",
    "                         'Tax_in_local_currency', 'Tax_in_document_currency', 'WAERS',\n",
    "                         'Batch_Input_session_name', 'sgtxt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "407f71bd-462e-4e77-8cdc-4c7bda659672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic.columns = ['LIFNR', 'BELNR', 'RENR', 'REDAT', 'LFSNR', 'GEBRF', 'GSMWB', 'GSMWF',\n",
    "                         'WAERS', 'WENUM', 'RGDAT', 'ABGST', 'AUFNR', 'VORGN', 'GJAHR', 'WEDAT',\n",
    "                         'DEBNOTNO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "05649cbc-4bbd-46d3-8e47-2ecb3f25c3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic['LIFNR'] = df_ic['LIFNR'].apply(\n",
    "    lambda x: '10000' + str(x) if not pd.isna(x) and len(str(x)) < 10 else str(x) if not pd.isna(\n",
    "        x) else '0' * 10\n",
    ")\n",
    "\n",
    "df_fi['suppl_no'] = df_fi['suppl_no'].apply(\n",
    "    lambda x: '10000' + str(x) if not pd.isna(x) and len(str(x)) < 10 else str(x) if not pd.isna(\n",
    "        x) else '0' * 10\n",
    ")\n",
    "\n",
    "df_ic['BELNR'] = df_ic['BELNR'].apply(\n",
    "    lambda x: '0' * (10 - len(str(x))) + str(x) if not pd.isna(x) else '0' * 10)\n",
    "\n",
    "df_fi['BELNR'] = df_fi['BELNR'].apply(\n",
    "    lambda x: '0' * (10 - len(str(x))) + str(x) if not pd.isna(x) else '0' * 10)\n",
    "\n",
    "sftp_df['Document number'] = sftp_df['Document number'].apply(\n",
    "    lambda x: '0' * (10 - len(str(x))) + str(x) if not pd.isna(x) else '0' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3f42cbd-1166-4e65-8702-1f26309d5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_number(num):\n",
    "    if pd.isna(num):  # Check for NaN values\n",
    "        return np.nan  # Return NaN if the input is NaN\n",
    "    num_str = str(num)  # Convert to string\n",
    "\n",
    "    # If the number already starts with '10000', return it as is\n",
    "    if num_str.startswith('10000'):\n",
    "        return num_str\n",
    "\n",
    "    stripped_num = num_str.lstrip('0')  # Remove leading zeros\n",
    "    final_num = stripped_num.zfill(6)  # Ensure it has at least 6 digits\n",
    "    return '1000' + final_num  # Prepend '10000'\n",
    "\n",
    "df_ic['LIFNR'] = df_ic['LIFNR'].apply(transform_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf135329-054e-40d9-abdc-ea130391625b",
   "metadata": {},
   "source": [
    "# 2nd Cleaning IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6db0f006-3687-4822-9b6c-9bf057d746ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of unique records in IC after cleaning :  458449\n"
     ]
    }
   ],
   "source": [
    "new_ic_unique_count = df_ic[['LIFNR', 'BELNR', 'RENR', 'REDAT']].drop_duplicates().shape[0]\n",
    "print(\"Count of unique records in IC after cleaning : \", new_ic_unique_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "501ac41c-ce40-4376-95ed-14d3f75b5273",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic[['LIFNR', 'BELNR', 'RENR', 'REDAT']] = df_ic[['LIFNR', 'BELNR', 'RENR', 'REDAT']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88babb9a-6c36-4d87-b66b-1731d0951d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_populated_row(group):\n",
    "    # Count non-null values for each row\n",
    "    non_null_counts = group.notnull().sum(axis=1)\n",
    "    # Get the index of the row with the maximum count\n",
    "    max_index = non_null_counts.idxmax()\n",
    "    return group.loc[max_index]\n",
    "\n",
    "df_ic = (\n",
    "            df_ic\n",
    "            .groupby(['LIFNR', 'BELNR', 'RENR', 'REDAT'])\n",
    "            .apply(get_max_populated_row)\n",
    "            .reset_index(drop=True)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "aa615c5b-edcb-4333-82c5-cbaba0495719",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic['LIFNR'] = df_ic['LIFNR'].replace('nan', np.nan)\n",
    "df_ic['BELNR'] = df_ic['BELNR'].replace('nan', np.nan)\n",
    "df_ic['RENR'] = df_ic['RENR'].replace('nan', np.nan)\n",
    "# df_ic['GEBRF'] = df_ic['GEBRF'].replace('nan', np.nan)\n",
    "df_ic['REDAT'] = df_ic['REDAT'].replace('nan', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "85e611fe-37fd-4a56-994b-e9522ea9d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic['LIFNR'] = df_ic['LIFNR'].astype('object')\n",
    "df_ic['BELNR'] = df_ic['BELNR'].astype('object')\n",
    "df_ic['RENR'] = df_ic['RENR'].astype('object')\n",
    "# df_ic['GEBRF'] = df_ic['GEBRF'].astype('object')\n",
    "df_ic['REDAT'] = df_ic['REDAT'].astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5ab85623-7827-445a-a9f0-406d43c1956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IC shape after cleaning :  (458449, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"IC shape after cleaning : \", df_ic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d409143b-0adf-4200-a55a-00487857cccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert_ic = ['REDAT', 'RGDAT', 'WEDAT']\n",
    "for col in columns_to_convert_ic:\n",
    "    df_ic[col] = pd.to_datetime(df_ic[col], format='%Y%m%d').dt.strftime('%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "902e8fae-29f6-49b2-8a26-f88a62764d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert_fi = ['AUGDT', 'ZFBDT', 'NETDT', 'BLDAT', 'BUDAT', 'CPUDT', 'partition_date','dana_ingestion_date']\n",
    "for col in columns_to_convert_fi:\n",
    "    df_fi[col] = pd.to_datetime(df_fi[col], format='%Y-%m-%d').dt.strftime('%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "28bd93b0-e4a1-4465-88d8-0c72bc8a6a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fi['Amount_in_local_currency'] = df_fi['Amount_in_local_currency'] * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "69a79ba4-996e-49f2-9e23-09f938ce06b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         2024\n",
       "1         2024\n",
       "2         2024\n",
       "3         2024\n",
       "4         2024\n",
       "          ... \n",
       "164074    2024\n",
       "164075    2024\n",
       "164076    2024\n",
       "164077    2024\n",
       "164078    2024\n",
       "Name: SFTP_bus_year, Length: 164079, dtype: object"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sftp_df['SFTP_bus_year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "40fe205f-acec-45be-89d0-eefd0367f959",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic['fin_year_IC'] = pd.to_datetime(df_ic['REDAT'], format='%d.%m.%Y').dt.year.astype('Int64')\n",
    "\n",
    "df_fi['fin_year_FI'] = pd.to_datetime(df_fi['BLDAT'], format='%d.%m.%Y').dt.year.astype('Int64')\n",
    "\n",
    "sftp_df['SFTP_bus_year'] = sftp_df['SFTP_bus_year'].astype('Int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c88df30-c0cf-4bac-8a5e-187160e2adbe",
   "metadata": {},
   "source": [
    "# Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49c8ec74-e680-44ad-aa41-3d4917d22e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st merge shape :  921542\n"
     ]
    }
   ],
   "source": [
    "merge_fi_ic = df_fi.merge(df_ic, left_on=[\"suppl_no\", \"BELNR\", \"XBLNR\", \"fin_year_FI\"],\n",
    "                          right_on=[\"LIFNR\", \"BELNR\", \"RENR\", \"fin_year_IC\"], how=\"outer\")\n",
    "\n",
    "print(\"1st merge shape : \", merge_fi_ic.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c6042dd7-68e1-4f40-8b39-01af4ce7d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_fi_ic['GJAHR_x'] = (\n",
    "    pd.to_numeric(merge_fi_ic['GJAHR_x'], errors='coerce')  # Convert to numeric, set invalid to NaN\n",
    "    .astype('Int64')                                  # Convert to nullable integers\n",
    "    .astype('string')    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e703194c-0eb3-48aa-846d-945bbab6cd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_fi_ic['combined_suppl_no'] = merge_fi_ic['suppl_no'].fillna(\n",
    "#     merge_fi_ic['LIFNR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5a18da91-af49-4de5-9039-233adfc791f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_fi_ic['combined_doc_no'] = merge_fi_ic['BELNR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a86fcb82-e6e5-4a2d-9e79-db23d33dc090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_fi_ic['combined_inv_no'] = merge_fi_ic['XBLNR'].fillna(\n",
    "#     merge_fi_ic['RENR'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1b902fb4-37a1-4f30-b164-7ba06ac153b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd merge shape :  935534\n"
     ]
    }
   ],
   "source": [
    "merged_df = merge_fi_ic.merge(sftp_df, left_on=[\"suppl_no\", \"BELNR\", \"XBLNR\", \"fin_year_FI\"],\n",
    "                                      right_on=[\"Supplier number (Sales Line)\", \"Document number\", \"Invoice number\",\n",
    "                                                \"SFTP_bus_year\"],\n",
    "                                      how=\"outer\")\n",
    "print(\"2nd merge shape : \", merged_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ae794eff-0bf5-493e-8e5b-a10d561f52ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_columns = merged_df['ARKTX'].str.split('#', expand=True)\n",
    "\n",
    "merged_df['Business Year SFTP'] = split_columns[3].apply(\n",
    "    lambda x: str(x) if pd.notnull(x) and x.isdigit() else pd.NA\n",
    ").astype('string')\n",
    "\n",
    "merged_df['Line Item No. SFTP'] = split_columns[4].apply(\n",
    "    lambda x: str(x) if pd.notnull(x) and x.isdigit() else pd.NA\n",
    ").astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7bbba1a4-38f2-4a3f-be06-6ba22592f440",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Doc no. combined'] = merged_df['BELNR'].fillna(merged_df['Document number'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b854ec-01cc-43b9-b1dc-9a3e7a75b756",
   "metadata": {},
   "source": [
    "# Partial paid invoices merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e07f3864-af8a-49f2-b9c1-88acfea3a672",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_16203/1884940454.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  partial_paid_df['SFTP_bus_year'] = partial_paid_df['SFTP_bus_year'].astype('Int64')\n"
     ]
    }
   ],
   "source": [
    "partial_paid_df['SFTP_bus_year'] = partial_paid_df['SFTP_bus_year'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "16e3793c-9781-4b41-ad57-4d66e59a5af7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pp merge shape :  2\n"
     ]
    }
   ],
   "source": [
    "pp_merged_df=merge_fi_ic.merge(partial_paid_df, left_on=[\"suppl_no\", \"BELNR\", \"XBLNR\", \"fin_year_FI\"]\n",
    "                                 , right_on=[\"Supplier number (Sales Line)\", \"Document number\", \"Invoice number\", \"SFTP_bus_year\"]\n",
    "                                 , how=\"right\")\n",
    "print(\"pp merge shape : \", pp_merged_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac9345-46b3-406b-b3e6-dc474523be09",
   "metadata": {},
   "source": [
    "# ARKTX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "61d06b24-0b41-4285-8b13-21d2c6b03d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"year\"] = merged_df.BLDAT.apply(\n",
    "    lambda x: x.split('.')[2] if isinstance(x, str) and len(x.split('.')) > 2 else None)\n",
    "merged_df[\"month\"] = merged_df.BLDAT.apply(\n",
    "    lambda x: x.split('.')[1] if isinstance(x, str) and len(x.split('.')) > 1 else None)\n",
    "merged_df[\"day\"] = merged_df.BLDAT.apply(\n",
    "    lambda x: x.split('.')[0] if isinstance(x, str) and len(x.split('.')) > 0 else None)\n",
    "merged_df['BUZEI'] = merged_df['BUZEI'].apply(lambda x: int(x) if pd.notna(x) else x).astype('Int64')\n",
    "\n",
    "\n",
    "mask_empty_ARKTX = merged_df[\"ARKTX\"].isna()\n",
    "\n",
    "merged_df.loc[mask_empty_ARKTX, \"ARKTX\"] = merged_df[mask_empty_ARKTX].apply(\n",
    "    lambda row: (\n",
    "        f\"{row['Document_type']}#{row['BELNR']}#{row['year']}{row['month']}{row['day']}#{row['year']}#00{row['BUZEI']}\"\n",
    "        if all(\n",
    "            pd.notna([row['Document_type'], row['BELNR'], row['year'], row['month'], row['day'], row['BUZEI']]))\n",
    "        else np.nan\n",
    "    ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad59c30a-9033-4e0d-b21f-694721582aec",
   "metadata": {},
   "source": [
    "# NET_DUE_DATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a21f613b-a469-428f-989f-946de402ad79",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['ZFBDT'] = pd.to_datetime(merged_df['ZFBDT'], format='%d.%m.%Y')\n",
    "merged_df['ZBD1T'] = pd.to_numeric(merged_df['ZBD1T'], errors='coerce')\n",
    "merged_df['ZBD2T'] = pd.to_numeric(merged_df['ZBD2T'], errors='coerce')\n",
    "\n",
    "merged_df['NET_DUE_DATE'] = merged_df.apply(\n",
    "    lambda row: row['ZFBDT'] + pd.Timedelta(\n",
    "        days=row['ZBD1T'] if pd.notna(row['ZBD1T']) else (row['ZBD2T'] if pd.notna(row['ZBD2T']) else 0)),\n",
    "    axis=1\n",
    ")\n",
    "merged_df['NET_DUE_DATE'] = merged_df['NET_DUE_DATE'].dt.strftime('%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116509ae-d1f5-4c32-8f3a-c0fc9179611d",
   "metadata": {},
   "source": [
    "# IC Transaction File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bec3c7b9-dd17-42a3-babf-2e936f109152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_from_gcs():\n",
    "    \n",
    "    bucket_name = \"miag-m360-test-bucket\"\n",
    "    file_path = \"ic_transaction_status_R.csv\"\n",
    "    encoding = 'windows-1252'\n",
    "    dtype = {\n",
    "    'TRANSACTION STATUS (ABGST)': 'str',\n",
    "    'LBL - \\nTRANSACTION STATUS (ABGST)': 'str',\n",
    "    'VIPA \\nTRANSACTION STATUS (ABGST)': 'str',\n",
    "    \"360 invoice status - MVP- Display on External 360\": 'str',\n",
    "    }\n",
    "\n",
    "    # Initialize a GCS client\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Get the bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Fetch the blob (file) from the bucket\n",
    "    blob = bucket.blob(file_path)\n",
    "\n",
    "    # Download the blob content as bytes\n",
    "    data = blob.download_as_bytes()\n",
    "\n",
    "    # Read the CSV into a Pandas DataFrame\n",
    "    ic_transaction_df = pd.read_csv(io.BytesIO(data), encoding=encoding, dtype=dtype)\n",
    "\n",
    "    return ic_transaction_df\n",
    "\n",
    "ic_tran_status_df = read_csv_from_gcs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e18a354b-ad03-4c1a-8c09-0cec7aa0a31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_tran_status_df = ic_tran_status_df[\n",
    "    [\"TRANSACTION STATUS (ABGST)\", \"LBL - \\nTRANSACTION STATUS (ABGST)\", \"VIPA \\nTRANSACTION STATUS (ABGST)\",\n",
    "     \"360 invoice status - MVP- Display on External 360\"]]\n",
    "\n",
    "ic_tran_status_df[\"ABGST\"] = ic_tran_status_df[\"TRANSACTION STATUS (ABGST)\"].fillna(\n",
    "    ic_tran_status_df['LBL - \\nTRANSACTION STATUS (ABGST)']).fillna(\n",
    "    ic_tran_status_df[\"VIPA \\nTRANSACTION STATUS (ABGST)\"])\n",
    "\n",
    "ic_tran_status_df = ic_tran_status_df[[\"ABGST\", \"360 invoice status - MVP- Display on External 360\"]]\n",
    "ic_tran_status_df['ABGST'] = ic_tran_status_df['ABGST'].str.zfill(4)\n",
    "ic_tran_status_df['ABGST'] = ic_tran_status_df['ABGST'].astype('string')\n",
    "abgst_status_dict = ic_tran_status_df.set_index('ABGST')['360 invoice status - MVP- Display on External 360'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e9642c5-5c20-4d10-9927-22b43b2a45d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(abgst_status_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1da56-a9f9-46ba-a7c2-ffcacb404165",
   "metadata": {},
   "source": [
    "# INVOICE_STATUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "52f1065c-fc8d-4e20-bf96-539389c98382",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['ABGST'] = merged_df['ABGST'].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "82f140e5-fc3e-45d1-ab58-a2b0d1c0bd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['Remittance advice number'].notna(), 'INVOICE_STATUS'] = \"cleared-MIAG\"\n",
    "# Step 2: Populate 'Inv Stat' with \"cleared-FI\" for rows where 'Inv Stat' is null and 'AUGBL' is present\n",
    "merged_df.loc[merged_df['INVOICE_STATUS'].isna() & merged_df['AUGBL'].notna(), 'INVOICE_STATUS'] = \"cleared-FI\"\n",
    "# Step 3: Populate 'Inv Stat' with \"Invoice Approval completed\" for rows where 'Inv Stat' is null and 'BLDAT' is present\n",
    "merged_df.loc[merged_df['INVOICE_STATUS'].isna() & merged_df[\n",
    "    'BLDAT'].notna(), 'INVOICE_STATUS'] = \"Invoice approval completed\"\n",
    "# Step 4: Populate 'Inv Stat' based on mapping from abgst_status_dict for rows where 'Inv Stat' is null and 'ABGST' is present\n",
    "merged_df.loc[merged_df['INVOICE_STATUS'].isna() & merged_df['ABGST'].notna(), 'INVOICE_STATUS'] = merged_df[\n",
    "    'ABGST'].map(abgst_status_dict)\n",
    "# Step 5: Populate remaining 'Inv Stat' as \"In progress\" where 'Inv Stat' is still null\n",
    "merged_df['INVOICE_STATUS'].fillna(\"Direct Entry to FI / New Status\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8a4db5-2e60-4685-a6a7-bc3de62ae35d",
   "metadata": {},
   "source": [
    "# GR Invoice Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e7204fb8-6a21-4a5a-b984-c5b25efb597b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['Line Item No. SFTP'] = (\n",
    "    merged_df['Line Item No. SFTP']\n",
    "    .apply(lambda x: str(x).lstrip('0') if pd.notna(x) else x)  # Remove leading zeros if not NA\n",
    "    .astype('Int64')  # Convert to Int64 type\n",
    ")\n",
    "\n",
    "merged_df['GJAHR_x'] = (\n",
    "    pd.to_numeric(merged_df['GJAHR_x'], errors='coerce')  # Convert to numeric, set invalid to NaN\n",
    "    .astype('Int64')  # Convert to nullable integers\n",
    "    .astype('string')  # Convert integers to strings, keep <NA>\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2fc0cd5f-686f-4a81-b009-4a7f5e819b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatypes before GR Invoice Implementation :  Document_type                       object\n",
      "Document type                       object\n",
      "GJAHR_x                     string[python]\n",
      "Business Year SFTP          string[python]\n",
      "BUZEI                                Int64\n",
      "Line Item No. SFTP                   Int64\n",
      "Amount_in_local_currency           float64\n",
      "Gross amount                       float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(\"Datatypes before GR Invoice Implementation : \", merged_df[['Document_type', 'Document type', 'GJAHR_x', 'Business Year SFTP', 'BUZEI', 'Line Item No. SFTP', 'Amount_in_local_currency', 'Gross amount']].dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "08685639-21c4-455f-b83f-543319035b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GR_Invoice:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_all_zeros = pd.DataFrame()\n",
    "        self.df_non_zero = pd.DataFrame()\n",
    "\n",
    "        self.df_group_len_1 = pd.DataFrame()\n",
    "        self.df_group_len_2 = pd.DataFrame()\n",
    "        self.df_group_len_3 = pd.DataFrame()\n",
    "\n",
    "        self.both_cleared_or_one_progress_groups = pd.DataFrame()\n",
    "        self.gr_invoice_records_groups = pd.DataFrame()\n",
    "        self.not_cleared_miag_records_groups = pd.DataFrame()\n",
    "\n",
    "        self.cleared_df = pd.DataFrame()\n",
    "        self.other_df = pd.DataFrame()\n",
    "        self.failed_doc_nos = []\n",
    "\n",
    "        self.len_3_cleared_df = pd.DataFrame()\n",
    "        self.len_3_other_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def gr_invoice(self, merged_df):\n",
    "        print(\"Merged_df shape : \", merged_df.shape)\n",
    "        self.df_all_zeros = merged_df[merged_df['Doc no. combined'] == '0000000000']\n",
    "        self.df_non_zero = merged_df[merged_df['Doc no. combined'] != '0000000000']\n",
    "        grouped = self.df_non_zero.groupby('Doc no. combined')\n",
    "        self.df_group_len_2 = grouped.filter(lambda x: len(x) == 2)\n",
    "        self.df_group_len_1 = grouped.filter(lambda x: len(x) == 1)\n",
    "        self.df_group_len_3 = grouped.filter(lambda x: len(x) > 2)\n",
    "        print(\"\\n\")\n",
    "        print(\"All zeros : \", len(self.df_all_zeros))\n",
    "        print(\"Length 1 : \", len(self.df_group_len_1))\n",
    "        print(\"Length 2 : \", len(self.df_group_len_2))\n",
    "        print(\"Length 3 : \", len(self.df_group_len_3))\n",
    "        print(\"\\n\")\n",
    "        print(\"Total : \", len(self.df_all_zeros)+len(self.df_group_len_1)+len(self.df_group_len_2)+len(self.df_group_len_3))\n",
    "\n",
    "        gr_len2_start = time.time()\n",
    "        gr_len2_concat = self.gr_length_2()\n",
    "        gr_len2_end = time.time()\n",
    "        print(\"GR Inv Len2 Time : \",gr_len2_end-gr_len2_start)\n",
    "        gr_len3_concat = self.gr_length_3()\n",
    "        gr_len3_end = time.time()\n",
    "        print(\"GR Inv Len3 Time : \", gr_len3_end - gr_len2_end)\n",
    "\n",
    "        merged_df = pd.concat([gr_len2_concat, gr_len3_concat, self.df_group_len_1, self.df_all_zeros], ignore_index=True)\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        print(merged_df.shape)\n",
    "        return merged_df\n",
    "\n",
    "\n",
    "    def gr_length_2(self):\n",
    "        grouped = self.df_group_len_2.groupby('Doc no. combined')\n",
    "\n",
    "        # Initialize lists for storing groups\n",
    "        both_cleared_or_one_progress_groups = []\n",
    "        gr_invoice_records_groups = []\n",
    "        not_cleared_miag_records_groups = []\n",
    "\n",
    "        count = 0\n",
    "        # Iterate over groups with conditions\n",
    "        for doc_no, group in grouped:\n",
    "            count += 1\n",
    "            # print(count)\n",
    "            statuses = group['INVOICE_STATUS'].tolist()\n",
    "\n",
    "            # Check conditions for categorizing groups\n",
    "            if statuses == ['cleared-MIAG', 'cleared-MIAG'] or \\\n",
    "                    ('cleared-MIAG' in statuses and 'In progress' in statuses):\n",
    "                both_cleared_or_one_progress_groups.append(group)\n",
    "            elif 'cleared-MIAG' in statuses and \\\n",
    "                    any(status in ['cleared-FI', 'Invoice approval completed'] for status in statuses):\n",
    "                gr_invoice_records_groups.append(group)\n",
    "            elif all(status != 'cleared-MIAG' for status in statuses):\n",
    "                not_cleared_miag_records_groups.append(group)\n",
    "\n",
    "        # Concatenate the groups into DataFrames\n",
    "        self.both_cleared_or_one_progress = pd.concat(both_cleared_or_one_progress_groups, ignore_index=True)\n",
    "        self.gr_invoice_records = pd.concat(gr_invoice_records_groups, ignore_index=True)\n",
    "        self.not_cleared_miag_records = pd.concat(not_cleared_miag_records_groups, ignore_index=True)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Length 2 : \", len(self.df_group_len_2))\n",
    "        print(\"Both cleared or one progress : \", len(self.both_cleared_or_one_progress))\n",
    "        print(\"GR Invoice Records : \", len(self.gr_invoice_records))\n",
    "        print(\"not_cleared_miag_records \", len(self.not_cleared_miag_records))\n",
    "        print(\"\\n\")\n",
    "        print(\"Total : \", len(self.both_cleared_or_one_progress)+ len(self.gr_invoice_records)+ len(self.not_cleared_miag_records))\n",
    "\n",
    "        if (len(self.gr_invoice_records) == 0):\n",
    "            self.gr_invoice_records = pd.DataFrame(columns=self.df_group_len_2.columns)\n",
    "\n",
    "        grouped = self.gr_invoice_records.groupby('Doc no. combined')\n",
    "\n",
    "        # Initialize lists to store the rows based on INVOICE_STATUS\n",
    "        cleared_records = []\n",
    "        other_records = []\n",
    "\n",
    "        # Iterate through each group\n",
    "        for name, group in grouped:\n",
    "            # Separate rows based on 'INVOICE_STATUS' value\n",
    "            cleared_record = group[group['INVOICE_STATUS'] == 'cleared-MIAG']\n",
    "            other_record = group[group['INVOICE_STATUS'] != 'cleared-MIAG']\n",
    "\n",
    "            # Append to the lists if the record exists\n",
    "            if not cleared_record.empty:\n",
    "                cleared_records.append(\n",
    "                    cleared_record.iloc[0])  # Assuming there's only one 'cleared-MIAG' record per group\n",
    "            if not other_record.empty:\n",
    "                other_records.append(\n",
    "                    other_record.iloc[0])  # Assuming there's only one non-'cleared-MIAG' record per group\n",
    "\n",
    "        # Convert lists to DataFrames if needed\n",
    "        self.cleared_df = pd.DataFrame(cleared_records)\n",
    "        self.other_df = pd.DataFrame(other_records)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"GR Invoice Records : \", len(self.gr_invoice_records))\n",
    "        print(\"Cleared df : \", len(self.cleared_df))\n",
    "        print(\"Non Cleared df : \", len(self.other_df))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if len(self.cleared_df) == 0:\n",
    "            self.cleared_df = pd.DataFrame(columns=self.gr_invoice_records.columns)\n",
    "\n",
    "        if len(self.other_df) == 0:\n",
    "            self.other_df = pd.DataFrame(columns=self.gr_invoice_records.columns)\n",
    "\n",
    "        # Loop through each row in cleared_df and perform validations\n",
    "        for index, row1 in self.cleared_df.iterrows():\n",
    "            # Get the corresponding row in other_df based on 'Doc no. combined'\n",
    "            row2 = self.other_df[self.other_df['Doc no. combined'] == row1['Doc no. combined']]\n",
    "\n",
    "            # Proceed only if there is exactly one matching row in other_df\n",
    "            if len(row2) == 1:\n",
    "                row2 = row2.iloc[0]\n",
    "\n",
    "                # Perform validation checks\n",
    "                if (\n",
    "                        pd.notnull(row1['Line Item No. SFTP']) and pd.notnull(row2['BUZEI']) and row1[\n",
    "                    'Line Item No. SFTP'] == row2['BUZEI'] and\n",
    "                        pd.notnull(row1['Document type']) and pd.notnull(row2['Document_type']) and row1[\n",
    "                    'Document type'] == row2['Document_type'] and\n",
    "                        pd.notnull(row1['Business Year SFTP']) and pd.notnull(row2['GJAHR_x']) and row1[\n",
    "                    'Business Year SFTP'] == row2['GJAHR_x'] and\n",
    "                        pd.notnull(row1['Gross amount']) and pd.notnull(row2['Amount_in_local_currency']) and row1[\n",
    "                    'Gross amount'] == row2['Amount_in_local_currency']\n",
    "                ):\n",
    "                    self.cleared_df.at[index, 'NET_DUE_DATE'] = row2['NET_DUE_DATE']\n",
    "\n",
    "                    # Check if 'Store' column in row1 is not empty\n",
    "                    if pd.notnull(row1['Store']) and row1['Store'] != '':\n",
    "                        # Directly remove row2 from other_df\n",
    "                        self.other_df = self.other_df.drop(row2.name)\n",
    "                    else:\n",
    "                        # Copy 'store_or_dc' value from row2 to 'Store' column in row1\n",
    "                        self.cleared_df.at[index, 'Store'] = row2['store_or_dc']\n",
    "                        # Remove row2 from other_df\n",
    "                        self.other_df = self.other_df.drop(row2.name)\n",
    "                else:\n",
    "                    # If validation fails, add the 'Doc no. combined' to the failed list\n",
    "                    self.failed_doc_nos.append(row1['Doc no. combined'])\n",
    "            # else:\n",
    "            #     # If no match or multiple matches, add to failed list\n",
    "            #     failed_doc_nos.append(row1['Doc no. combined'])\n",
    "\n",
    "        # Output the updated cleared_df, other_df, and the failed list\n",
    "        self.cleared_df.reset_index(drop=True, inplace=True)\n",
    "        self.other_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        print(\"Cleared df : \", len(self.cleared_df))\n",
    "        print(\"Non Cleared df : \", len(self.other_df))\n",
    "        print(\"Failed docs : \", len(self.failed_doc_nos))\n",
    "        print(\"Examples of Failed docs : \", self.failed_doc_nos[0:20])\n",
    "\n",
    "        gr_len2_concat = pd.concat([self.cleared_df, self.other_df, self.not_cleared_miag_records, self.both_cleared_or_one_progress], ignore_index=True)\n",
    "        gr_len2_concat.reset_index(drop=True, inplace=True)\n",
    "        return gr_len2_concat\n",
    "\n",
    "    def gr_length_3(self):\n",
    "        self.len_3_cleared_df = self.df_group_len_3[self.df_group_len_3['INVOICE_STATUS'] == 'cleared-MIAG'].copy()\n",
    "        self.len_3_other_df = self.df_group_len_3[self.df_group_len_3['INVOICE_STATUS'] != 'cleared-MIAG'].copy()\n",
    "\n",
    "        rows_to_delete = []\n",
    "        for index_other, row2 in self.len_3_other_df.iterrows():\n",
    "            # Find matching row in len_3_cleared_df\n",
    "            match = self.len_3_cleared_df[\n",
    "                (self.len_3_cleared_df['Line Item No. SFTP'] == row2['BUZEI']) &\n",
    "                (self.len_3_cleared_df['Document type'] == row2['Document_type']) &\n",
    "                (self.len_3_cleared_df['Business Year SFTP'] == row2['GJAHR_x']) &\n",
    "                (self.len_3_cleared_df['Gross amount'] == row2['Amount_in_local_currency']) &\n",
    "                (self.len_3_cleared_df['Doc no. combined'] == row2['Doc no. combined'])\n",
    "                ]\n",
    "\n",
    "            if not match.empty:\n",
    "                # Take the first matched row (assuming only one match is expected)\n",
    "                row1 = match.iloc[0]\n",
    "\n",
    "                self.len_3_cleared_df.loc[match.index[0], 'NET_DUE_DATE'] = row2['NET_DUE_DATE']\n",
    "\n",
    "                # Check the Store column in row1\n",
    "                if pd.isna(row1['Store']):\n",
    "                    # Check the store_or_dc column in row2\n",
    "                    if not pd.isna(row2.get('store_or_dc')):\n",
    "                        self.len_3_cleared_df.loc[match.index[0], 'Store'] = row2['store_or_dc']\n",
    "\n",
    "                # Mark the row2 for deletion\n",
    "                rows_to_delete.append(index_other)\n",
    "\n",
    "        # Delete rows from len_3_other_df that were processed\n",
    "        self.len_3_other_df.drop(index=rows_to_delete, inplace=True)\n",
    "\n",
    "        print(\"Length 3 : \", len(self.df_group_len_3))\n",
    "        print(\"len_3_cleared_df : \", len(self.len_3_cleared_df))\n",
    "        print(\"len_3_other_df : \", len(self.len_3_other_df))\n",
    "\n",
    "        gr_len3_concat = pd.concat([self.len_3_cleared_df, self.len_3_other_df], ignore_index=True)\n",
    "        gr_len3_concat.reset_index(drop=True, inplace=True)\n",
    "        return gr_len3_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a319a97c-811c-4d29-aad6-9c4ccd7cb367",
   "metadata": {},
   "outputs": [],
   "source": [
    "gr_invoice = GR_Invoice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "78427fd2-c452-4f39-a048-d38bccd19f85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(935534, 80)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "97e79c88-fb42-4e5e-81ad-f246c9cdd2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged_df shape :  (935534, 80)\n",
      "\n",
      "\n",
      "All zeros :  2817\n",
      "Length 1 :  435606\n",
      "Length 2 :  323980\n",
      "Length 3 :  173131\n",
      "\n",
      "\n",
      "Total :  935534\n",
      "\n",
      "\n",
      "Length 2 :  323980\n",
      "Both cleared or one progress :  5342\n",
      "GR Invoice Records :  125574\n",
      "not_cleared_miag_records  193064\n",
      "\n",
      "\n",
      "Total :  323980\n",
      "\n",
      "\n",
      "GR Invoice Records :  125574\n",
      "Cleared df :  62787\n",
      "Non Cleared df :  62787\n",
      "\n",
      "\n",
      "Cleared df :  62787\n",
      "Non Cleared df :  56651\n",
      "Failed docs :  56651\n",
      "Examples of Failed docs :  ['0810000000', '0810000010', '0810000011', '0810000012', '0810000013', '0810000023', '0810000029', '0810000031', '0810000050', '0810000051', '0810000055', '0810000071', '0810000077', '0810000092', '0810000100', '0810000101', '0810000102', '0810000105', '0810000123', '0810000133']\n",
      "GR Inv Len2 Time :  1454.1691045761108\n",
      "Length 3 :  173131\n",
      "len_3_cleared_df :  41094\n",
      "len_3_other_df :  128215\n",
      "GR Inv Len3 Time :  1737.9273700714111\n",
      "(925576, 80)\n"
     ]
    }
   ],
   "source": [
    "merged_df = gr_invoice.gr_invoice(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd07eea8-5386-4bf1-a7a7-b882ceeae795",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pairs = merged_df.drop_duplicates(subset=['Supplier number (Sales Line)', 'Supplier number (MIAG)'])\n",
    "unique_pairs = unique_pairs.drop_duplicates(subset=['Supplier number (Sales Line)'], keep='first')\n",
    "doc_to_type = dict(zip(unique_pairs['Supplier number (Sales Line)'], unique_pairs['Supplier number (MIAG)']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "8a67ea34-bd08-43d1-8909-03e598e4b3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MANDT', 'Document_type', 'document_type_desc', 'GJAHR_x', 'BUKRS',\n",
       "       'GSBER', 'PRCTR', 'store_or_dc', 'KOSTL', 'month_in_fin_year', 'BELNR',\n",
       "       'XBLNR', 'AUGBL', 'AUGDT', 'ZFBDT', 'ZBD1T', 'ZBD2T', 'NETDT', 'BUZEI',\n",
       "       'altkt', 'hkont', 'suppl_no', 'BLDAT', 'BUDAT', 'CPUDT',\n",
       "       'partition_date', 'dana_ingestion_date', 'shkzg',\n",
       "       'Amount_in_local_currency', 'Amount_in_document_currency',\n",
       "       'Tax_in_local_currency', 'Tax_in_document_currency', 'WAERS_x',\n",
       "       'Batch_Input_session_name', 'sgtxt', 'fin_year_FI', 'LIFNR', 'RENR',\n",
       "       'REDAT', 'LFSNR', 'GEBRF', 'GSMWB', 'GSMWF', 'WAERS_y', 'WENUM',\n",
       "       'RGDAT', 'ABGST', 'AUFNR', 'VORGN', 'GJAHR_y', 'WEDAT', 'DEBNOTNO',\n",
       "       'fin_year_IC', 'Contract area', 'Supplier number (Sales Line)',\n",
       "       'Supplier number (MIAG)', 'Supplier name', 'VAT number',\n",
       "       'Document number', 'Invoice number', 'Document type', 'Document date',\n",
       "       'Remittance advice number', 'Value date', 'Currency', 'Gross amount',\n",
       "       'Description', 'Contract indicator', 'Store', 'Company code', 'ARKTX',\n",
       "       'SFTP_bus_year', 'Business Year SFTP', 'Line Item No. SFTP',\n",
       "       'Doc no. combined', 'year', 'month', 'day', 'NET_DUE_DATE',\n",
       "       'INVOICE_STATUS'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e9fd73fe-514a-44fa-b304-f33d3e791904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(925576, 80)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9a5153dd-5812-433c-bbfa-c12728a9ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_copy_df = merged_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4faea30c-e764-4665-b649-cd365f4a7f5a",
   "metadata": {},
   "source": [
    "# LOADFILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "310f005a-38ae-4d48-bb25-40fdb3ac25c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_df = pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f5cb07a8-c6bc-4748-b588-2356a25fd080",
   "metadata": {},
   "outputs": [],
   "source": [
    "company_code = 3142\n",
    "loadfile_df['COMPANY_CODE'] = company_code\n",
    "\n",
    "\n",
    "\n",
    "loadfile_df['SUPPLIER_NO'] = merged_df['Supplier number (Sales Line)'].fillna(\n",
    "    merged_df['suppl_no']\n",
    ").fillna(\n",
    "    merged_df['LIFNR']\n",
    ")\n",
    "\n",
    "\n",
    "loadfile_df['MIAG_SUPPLIER_NO'] = merged_df['Supplier number (MIAG)']\n",
    "loadfile_df['MIAG_SUPPLIER_NO'] = loadfile_df['MIAG_SUPPLIER_NO'].fillna(\n",
    "    loadfile_df['SUPPLIER_NO'].map(doc_to_type))\n",
    "\n",
    "\n",
    "\n",
    "loadfile_df['ORDER_NO'] = merged_df['AUFNR']\n",
    "\n",
    "\n",
    "\n",
    "loadfile_df['DOC_TYPE'] = merged_df['Document type'].where(merged_df['Document type'].notna(),\n",
    "                                                           merged_df['Document_type'])\n",
    "\n",
    "\n",
    "\n",
    "loadfile_df['INVOICE_NO'] = merged_df['Invoice number'].where(merged_df['Invoice number'].notna(),\n",
    "                                                              merged_df['XBLNR'])\n",
    "loadfile_df['INVOICE_NO'] = merged_df['Invoice number'].fillna(\n",
    "    merged_df['XBLNR']\n",
    ").fillna(\n",
    "    merged_df['RENR']\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "loadfile_df['INVOICE_DATE'] = merged_df['Document date'].fillna(merged_df['REDAT'])\n",
    "\n",
    "\n",
    "\n",
    "loadfile_df['DELIVERY_NOTE_NO'] = merged_df['LFSNR']\n",
    "\n",
    "\n",
    "merged_df['GEBRF'] = pd.to_numeric(merged_df['GEBRF'], errors='coerce')\n",
    "merged_df['GSMWF'] = pd.to_numeric(merged_df['GSMWF'], errors='coerce')\n",
    "\n",
    "loadfile_df['TOTAL_AMT_DC'] = merged_df['Gross amount'].fillna(\n",
    "    merged_df['Amount_in_local_currency']\n",
    ").fillna(\n",
    "    merged_df['GEBRF']\n",
    ").astype('float64')\n",
    "\n",
    "\n",
    "loadfile_df['TOTAL_VAT_DC'] = merged_df['GSMWF'].astype('float64')\n",
    "\n",
    "company_code = 3142\n",
    "loadfile_df['COMPANY_CODE'] = company_code\n",
    "\n",
    "country_currency_dict = {3142: 'TRY'}\n",
    "loadfile_df['CURRENCY'] = loadfile_df['COMPANY_CODE'].map(country_currency_dict)\n",
    "\n",
    "condition = (loadfile_df['DOC_TYPE'] == 'MV')\n",
    "\n",
    "loadfile_df['PRE_FINANCE_DATE'] = np.where(condition, merged_df['Value date'], '')\n",
    "\n",
    "loadfile_df['GOODS_RECEIPT_NO'] = merged_df['WENUM']\n",
    "\n",
    "loadfile_df['GOODS_RECEIPT_DATE'] = merged_df['WEDAT']\n",
    "\n",
    "loadfile_df['INVOICE_ENTRY_DATE'] = merged_df['RGDAT'].where(merged_df['RGDAT'].notna(), merged_df['BLDAT'])\n",
    "\n",
    "loadfile_df['INVOICE_STATUS'] = merged_df['INVOICE_STATUS']\n",
    "\n",
    "loadfile_df['INVOICE_STATUS_INTERNAL'] = merged_df['ABGST']\n",
    "\n",
    "loadfile_df['NET_DUE_DATE'] = merged_df['NET_DUE_DATE']\n",
    "\n",
    "loadfile_df['DEBIT_NOTE_NO'] = merged_df['DEBNOTNO']\n",
    "\n",
    "loadfile_df['REMITTANCE_ADVICE_NO'] = np.where(\n",
    "    merged_df['INVOICE_STATUS'] == 'cleared-MIAG',\n",
    "    merged_df['Remittance advice number'],\n",
    "    '')\n",
    "\n",
    "loadfile_df['CLEARING_DATE'] = merged_df['Value date'].where(merged_df['Value date'].notna(),\n",
    "                                                             merged_df['AUGDT'])\n",
    "\n",
    "loadfile_df['DOCUMENT_NO'] = merged_df['BELNR'].where(merged_df['BELNR'].notna(), merged_df['Document number'])\n",
    "\n",
    "loadfile_df['STORE_NO'] = merged_df['Store'].fillna(merged_df['store_or_dc'])\n",
    "\n",
    "loadfile_df['ARKTX'] = merged_df['ARKTX']\n",
    "\n",
    "loadfile_df['Description'] = merged_df['Description']\n",
    "\n",
    "loadfile_df['AUGBL'] = merged_df['AUGBL']\n",
    "\n",
    "loadfile_df['BLDAT'] = merged_df['BLDAT']\n",
    "\n",
    "loadfile_df['Bus_year'] = merged_df['SFTP_bus_year'].fillna(merged_df['fin_year_FI']).fillna(\n",
    "    merged_df['fin_year_IC']\n",
    ")\n",
    "\n",
    "loadfile_df['AUGDT'] = merged_df['AUGDT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "a690b5fb-c445-42d2-9ed3-810cb2439300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "current_date = datetime.date.today()\n",
    "formatted_current_date = current_date.strftime(\"%d.%m.%Y\")\n",
    "loadfile_df['MATCHING_DATE'] = formatted_current_date\n",
    "loadfile_df['MATCH_STATUS'] = 'No Matching Requested'\n",
    "loadfile_df['SYNC_DATE'] = formatted_current_date\n",
    "loadfile_df['SYNC_STATUS'] = '1'\n",
    "loadfile_df = loadfile_df.fillna('')\n",
    "loadfile_df['INVOICE_NO'].replace('nan', '', inplace=True)\n",
    "loadfile_df['DOCUMENT_NO'] = loadfile_df['DOCUMENT_NO'].replace('0000000000', '')\n",
    "loadfile_df['DEBIT_NOTE_NO'] = loadfile_df['DEBIT_NOTE_NO'].apply(\n",
    "    lambda x: str(x).strip() if str(x).strip() else '')\n",
    "loadfile_df = loadfile_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c16d784e-34f4-41e3-9821-e74e39c42d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         25.09.2024\n",
       "1         25.09.2024\n",
       "2         25.09.2024\n",
       "3         25.09.2024\n",
       "4         25.09.2024\n",
       "             ...    \n",
       "925571              \n",
       "925572              \n",
       "925573              \n",
       "925574              \n",
       "925575              \n",
       "Name: BLDAT, Length: 925576, dtype: object"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_df['BLDAT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "02efea8f-7439-4f44-af2a-1d52317d430b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(925576, 33)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "21e27b16-46f3-452b-953e-649d7f32834e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "only_suppl_data_record_count = ((loadfile_df['INVOICE_STATUS'] == 'Direct Entry to FI / New Status') & \n",
    "         (loadfile_df['DOCUMENT_NO'] == '') & \n",
    "         (loadfile_df['INVOICE_NO'] == '')).sum()\n",
    "print(only_suppl_data_record_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7a3fe191-afe8-478d-b405-48da14341224",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_df = loadfile_df[~((loadfile_df['INVOICE_STATUS'] == 'Direct Entry to FI / New Status') & \n",
    "          (loadfile_df['DOCUMENT_NO'] == '') & \n",
    "          (loadfile_df['INVOICE_NO'] == ''))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "dd07e00d-117d-4dc1-800d-c25fdcfc4dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(925568, 33)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "5093ea90-cfab-4d9d-9b3c-0be5fbaee93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_copy_2 = loadfile_df.copy()\n",
    "loadfile_copy_3 = loadfile_df.copy()\n",
    "# loadfile_df = loadfile_copy_2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "f72120b4-fe49-4029-8aeb-9d9ec71f1c23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(925568, 33)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2a208342-f30a-4c8a-a32d-5f7edb06fd3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(925568, 33)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_copy_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7f063cf7-4902-4131-8339-bce170e4bfb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_mv_df_1 = loadfile_df[loadfile_df['DOC_TYPE']=='MV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "a05cc460-4682-4539-95c3-6cc4f5809e7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_non_mv_df_1 = loadfile_df[loadfile_df['DOC_TYPE']!='MV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "92c5de95-b0aa-48f9-a747-33cedcb17273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 33)"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_mv_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "f62cfa88-3e06-4f29-bcef-accc501e2d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(924847, 33)"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_non_mv_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "38fc8dfa-f757-4a15-b3c9-88204704c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loadfile_df['ARKTX_YEAR'] = loadfile_df['ARKTX'].str.split('#').str[3]\n",
    "# # loadfile_mv_df['ARKTX_YEAR'] = loadfile_mv_df['ARKTX'].str.split('#').str[3]\n",
    "\n",
    "# # Perform a merge to combine relevant rows from both DataFrames\n",
    "# merged_mv_df = loadfile_mv_df_1.merge(\n",
    "#     loadfile_non_mv_df_1,\n",
    "#     how='left',\n",
    "#     left_on=['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year'],\n",
    "#     right_on=['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year'],\n",
    "#     suffixes=('_mv', '_loadfile')\n",
    "# )\n",
    "\n",
    "# # Group by relevant columns from loadfile_mv_df\n",
    "# grouped = merged_mv_df.groupby(['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year'])\n",
    "\n",
    "# # Build the metadata DataFrame using aggregation\n",
    "# mv_metadata_df = grouped.agg(\n",
    "#     Document_Type_Count=('DOC_TYPE_loadfile', 'count'),\n",
    "#     Document_Types=('DOC_TYPE_loadfile', lambda x: list(x.unique())),\n",
    "#     Desc_Count=('Description_loadfile', 'count'),\n",
    "#     Desc_Types=('Description_loadfile', lambda x: list(x.unique())),\n",
    "# ).reset_index()\n",
    "\n",
    "# # # Drop the extra columns from the original DataFrames\n",
    "# # loadfile_df.drop(columns=['ARKTX_YEAR'], inplace=True)\n",
    "# # loadfile_mv_df.drop(columns=['ARKTX_YEAR'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf19b6a-77f9-4001-ba1f-13b62bf0a9b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "adac1891-7651-4e1b-81ec-35a58822c278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_mv_df = pd.merge(\n",
    "    loadfile_mv_df_1,\n",
    "    loadfile_non_mv_df_1,\n",
    "    on=['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year'],\n",
    "    how='left',\n",
    "    suffixes=('_x', '_y')  # Explicitly naming suffixes for clarity\n",
    ")\n",
    "\n",
    "# Ensure 'Document type' columns are handled properly\n",
    "merged_mv_df['Combined_Document_types'] = merged_mv_df[['DOC_TYPE_x', 'DOC_TYPE_y']].apply(\n",
    "    lambda row: [val for val in row if pd.notna(val)], axis=1\n",
    ")\n",
    "merged_mv_df['Combined_RAN'] = merged_mv_df[['REMITTANCE_ADVICE_NO_x', 'REMITTANCE_ADVICE_NO_y']].apply(\n",
    "    lambda row: [val for val in row if pd.notna(val)], axis=1\n",
    ")\n",
    "merged_mv_df['Combined_Description'] = merged_mv_df[['Description_x', 'Description_y']].apply(\n",
    "    lambda row: [val for val in row if pd.notna(val)], axis=1        \n",
    ")\n",
    "\n",
    "# Group by the relevant columns in sftp_mv_df\n",
    "grouped = merged_mv_df.groupby(\n",
    "    ['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year']\n",
    ")\n",
    "\n",
    "# Aggregate the results\n",
    "mv_metadata_df = grouped.agg(\n",
    "    Document_Count=('Combined_Document_types', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "    Document_types=('Combined_Document_types', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique document types\n",
    "    RAN_count=('Combined_RAN', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "    RAN_list=('Combined_RAN', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Flattened unique list ignoring NaN\n",
    "    Desc_count=('Combined_Description', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "    Desc_list=('Combined_Description', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item))))  # Flattened unique list ignoring NaN\n",
    ").reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cf9934ae-d3ee-43ca-8b45-0933181075b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# merged_mv_df = loadfile_mv_df_1.merge(\n",
    "#     loadfile_non_mv_df_1,\n",
    "#     how='inner',\n",
    "#     left_on=['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year'],\n",
    "#     right_on=['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year'],\n",
    "#     suffixes=('_x', '_y')\n",
    "# )\n",
    "\n",
    "# # Ensure 'Document type' columns are handled properly\n",
    "# merged_mv_df['Combined_Document_types'] = merged_mv_df[['DOC_TYPE_x', 'DOC_TYPE_y']].apply(\n",
    "#     lambda row: [val for val in row if pd.notna(val)], axis=1\n",
    "# )\n",
    "# merged_mv_df['Combined_RAN'] = merged_mv_df[['REMITTANCE_ADVICE_NO_x', 'REMITTANCE_ADVICE_NO_y']].apply(\n",
    "#     lambda row: [val for val in row if pd.notna(val)], axis=1\n",
    "# )\n",
    "# merged_mv_df['Combined_Description'] = merged_mv_df[['Description_x', 'Description_y']].apply(\n",
    "#     lambda row: [val for val in row if pd.notna(val)], axis=1\n",
    "# # Group by the relevant columns in sftp_mv_df\n",
    "# )\n",
    "# grouped = merged_mv_df.groupby(['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year'])\n",
    "\n",
    "\n",
    "# # Aggregate the results\n",
    "# mv_metadata_df = grouped.agg(\n",
    "#     Document_Count=('Combined_Document_types', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "#     Document_types=('Combined_Document_types', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique document types\n",
    "#     RAN_count=('Combined_RAN', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "#     RAN_list=('Combined_RAN', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Flattened unique list ignoring NaN\n",
    "#     Desc_count=('Combined_Description', lambda x: len(set(item for sublist in x for item in sublist if pd.notna(item)))),  # Unique count ignoring NaN\n",
    "#     Desc_list=('Combined_Description', lambda x: list(set(item for sublist in x for item in sublist if pd.notna(item))))  # Flattened unique list ignoring NaN\n",
    "# ).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "d8cf4a6e-f16d-41f9-a775-b17df103f0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 10)"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_metadata_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "766b4f81-c5c5-4914-a66b-e321a90bc43e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "43d3d8d6-a9c6-4f1d-bb84-becc6422f18e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721, 33)"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_mv_df_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "3b71984f-c62d-477e-a52a-d54766090b81",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "721"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mv_metadata_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "80c49137-8c7b-4477-b00a-5fad14e698a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mv_metadata_df[mv_metadata_df['Document_Count']==3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "3d14c786-4380-469e-8217-4a45c4deaabf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "659"
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mv_metadata_df[mv_metadata_df['Document_Count']==2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "cf9b646b-ddb5-4c64-a775-4eb9a9d6325b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mv_metadata_df[mv_metadata_df['Document_Count']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "8616363e-fc80-4acb-a6e0-13538f77f11b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mv_metadata_df[(mv_metadata_df['Document_Count']==1) & (mv_metadata_df['RAN_list'].apply(lambda x: len(x) == 1 and x[0] == ''))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "399aaf33-87f3-4fd7-8871-a697e596710c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mv_metadata_df[(mv_metadata_df['Document_Count']==1) & (~(mv_metadata_df['RAN_list'].apply(lambda x: len(x) == 1 and x[0] == '')))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a6551e60-8f68-46f2-9af3-830841d28cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUPPLIER_NO</th>\n",
       "      <th>DOCUMENT_NO</th>\n",
       "      <th>INVOICE_NO</th>\n",
       "      <th>Bus_year</th>\n",
       "      <th>Document_Count</th>\n",
       "      <th>Document_types</th>\n",
       "      <th>RAN_count</th>\n",
       "      <th>RAN_list</th>\n",
       "      <th>Desc_count</th>\n",
       "      <th>Desc_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [SUPPLIER_NO, DOCUMENT_NO, INVOICE_NO, Bus_year, Document_Count, Document_types, RAN_count, RAN_list, Desc_count, Desc_list]\n",
       "Index: []"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_metadata_df[mv_metadata_df['Document_Count']==3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "8c463fac-18d3-419e-a470-695ba8325b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e8f2c7b5-3911-495b-8085-3b6b100257a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year',\n",
       "       'Document_Count', 'Document_types', 'RAN_count', 'RAN_list',\n",
       "       'Desc_count', 'Desc_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mv_metadata_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "c4e3d1c7-0346-44f1-88ab-5d86d9002759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MV Metadata length :  721\n",
      "Error records :  0\n",
      "no. of both present (cleared & pre-financed) :  659\n",
      "no. of once present (pre-financed) :  62\n",
      "\t\t partial paid records :  2\n",
      "\t\t not partial paid records :  60\n",
      "Final records should be :  924909\n"
     ]
    }
   ],
   "source": [
    "print('MV Metadata length : ', len(mv_metadata_df))\n",
    "print(\"Error records : \", len(mv_metadata_df[mv_metadata_df['Document_Count']==3]))\n",
    "print('no. of both present (cleared & pre-financed) : ', len(mv_metadata_df[mv_metadata_df['Document_Count']==2]))\n",
    "print('no. of once present (pre-financed) : ', len(mv_metadata_df[mv_metadata_df['Document_Count']==1]))\n",
    "print(\"\\t\\t partial paid records : \", len(mv_metadata_df[(mv_metadata_df['Document_Count']==1) & (~(mv_metadata_df['RAN_list'].apply(lambda x: len(x) == 1 and x[0] == '')))]))\n",
    "print(\"\\t\\t not partial paid records : \", len(mv_metadata_df[(mv_metadata_df['Document_Count']==1) & (mv_metadata_df['RAN_list'].apply(lambda x: len(x) == 1 and x[0] == ''))]))\n",
    "print('Final records should be : ', len(loadfile_df)-len(mv_metadata_df[mv_metadata_df['Document_Count']==2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d812094b-8263-4ce3-96d7-0b729705cad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process the dataframes to avoid repeated operations\n",
    "# Convert columns to appropriate dtypes for faster comparison\n",
    "for df in [mv_metadata_df, loadfile_copy_2, pp_merged_df]:\n",
    "    for col in ['SUPPLIER_NO', 'DOCUMENT_NO', 'INVOICE_NO', 'Bus_year']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "# Create a lookup key column for faster filtering\n",
    "loadfile_copy_2['lookup_key'] = loadfile_copy_2.apply(\n",
    "    lambda x: f\"{x['SUPPLIER_NO']}_{x['DOCUMENT_NO']}_{x['INVOICE_NO']}_{x['Bus_year']}\", \n",
    "    axis=1\n",
    ")\n",
    "pp_merged_df['lookup_key'] = pp_merged_df.apply(\n",
    "    lambda x: f\"{x['suppl_no']}_{x['BELNR']}_{x['XBLNR']}_{x['fin_year_FI']}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "counts = {'count1': 0, 'count2': 0, 'count3': 0}\n",
    "\n",
    "def process_single_record(filtered_df, mv_row):\n",
    "    \"\"\"Process a single record and return updates\"\"\"\n",
    "    idx = filtered_df.index[0]\n",
    "    row = filtered_df.iloc[0]\n",
    "    \n",
    "    if row['DOC_TYPE'] == 'MV' and row['Description'] == 'pre-financed' and row['REMITTANCE_ADVICE_NO'] == '':\n",
    "        counts['count2'] += 1\n",
    "        return {'index': idx, 'updates': {'INVOICE_STATUS': 'pre-financed'}}\n",
    "    \n",
    "    if row['DOC_TYPE'] == 'MV' and row['Description'] == 'open' and row['REMITTANCE_ADVICE_NO'] == '':\n",
    "        # No updates needed, just return None\n",
    "        return None\n",
    "    \n",
    "    counts['count3'] += 1\n",
    "    \n",
    "    # Determine INVOICE_STATUS\n",
    "    if row['AUGBL']:\n",
    "        new_status = 'cleared-FI'\n",
    "    elif row['BLDAT']:\n",
    "        new_status = 'Invoice approval completed'\n",
    "    elif row['INVOICE_STATUS_INTERNAL']:\n",
    "        new_status = abgst_status_dict.get(row['INVOICE_STATUS_INTERNAL'], \n",
    "                                         'Direct Entry to FI / New Status')\n",
    "    else:\n",
    "        new_status = 'Direct Entry to FI / New Status'\n",
    "    \n",
    "    # Get corresponding merged data\n",
    "    lookup_key = f\"{mv_row['SUPPLIER_NO']}_{mv_row['DOCUMENT_NO']}_{mv_row['INVOICE_NO']}_{mv_row['Bus_year']}\"\n",
    "    merged_data = pp_merged_df[pp_merged_df['lookup_key'] == lookup_key]\n",
    "    \n",
    "    if not merged_data.empty:\n",
    "        merged_row = merged_data.iloc[0]\n",
    "        new_value = ''\n",
    "        if pd.notna(merged_row['BLDAT']):\n",
    "            day, month, year = merged_row['BLDAT'].split('.')\n",
    "            new_value = f\"{merged_row['Document_type']}#{merged_row['BELNR']}#{year}{month}{day}#{year}#00{merged_row['BUZEI']}\"\n",
    "        \n",
    "        updates = {\n",
    "            'INVOICE_STATUS': new_status,\n",
    "            'DOC_TYPE': merged_row['Document_type'],\n",
    "            'STORE_NO': merged_row['store_or_dc'],\n",
    "            'ARKTX': new_value\n",
    "        }\n",
    "        return {'index': idx, 'updates': updates}\n",
    "    \n",
    "    return {'index': idx, 'updates': {'INVOICE_STATUS': new_status}}\n",
    "\n",
    "# Create a list to store all updates\n",
    "updates = []\n",
    "\n",
    "# Process records in batches\n",
    "batch_size = 1000\n",
    "for batch_start in range(0, len(mv_metadata_df), batch_size):\n",
    "    batch_end = min(batch_start + batch_size, len(mv_metadata_df))\n",
    "    batch = mv_metadata_df.iloc[batch_start:batch_end]\n",
    "    \n",
    "    for _, mv_row in batch.iterrows():\n",
    "        lookup_key = f\"{mv_row['SUPPLIER_NO']}_{mv_row['DOCUMENT_NO']}_{mv_row['INVOICE_NO']}_{mv_row['Bus_year']}\"\n",
    "        filtered_df = loadfile_copy_2[loadfile_copy_2['lookup_key'] == lookup_key]\n",
    "            \n",
    "        if len(filtered_df) == 2:\n",
    "            counts['count1'] += 1\n",
    "            if filtered_df['INVOICE_STATUS'].isin(['cleared-MIAG', 'cleared-FI', 'Invoice approval completed']).all():\n",
    "                mv_rows = filtered_df[filtered_df['DOC_TYPE'] == 'MV']\n",
    "                non_mv_rows = filtered_df[filtered_df['DOC_TYPE'] != 'MV']\n",
    "                \n",
    "                if (len(mv_rows) == 1 and len(non_mv_rows) == 1 and \n",
    "                    mv_rows.iloc[0]['Description'] == 'pre-financed'):\n",
    "                    updates.append({\n",
    "                        'index': non_mv_rows.index[0],\n",
    "                        'updates': {'PRE_FINANCE_DATE': mv_rows.iloc[0]['PRE_FINANCE_DATE']}\n",
    "                    })\n",
    "                updates.append({'index': mv_rows.index[0], 'delete': True})\n",
    "                \n",
    "        elif len(filtered_df) == 1:\n",
    "            update = process_single_record(filtered_df, mv_row)\n",
    "            if update:\n",
    "                updates.append(update)\n",
    "\n",
    "# Apply all updates in bulk\n",
    "indices_to_drop = []\n",
    "update_dict = {}\n",
    "\n",
    "for update in updates:\n",
    "    if update.get('delete'):\n",
    "        indices_to_drop.append(update['index'])\n",
    "    else:\n",
    "        for column, value in update['updates'].items():\n",
    "            if column not in update_dict:\n",
    "                update_dict[column] = {}\n",
    "            update_dict[column][update['index']] = value\n",
    "\n",
    "# Apply updates using loc\n",
    "for column, values in update_dict.items():\n",
    "    loadfile_copy_2.loc[list(values.keys()), column] = list(values.values())\n",
    "\n",
    "# Drop rows marked for deletion\n",
    "if indices_to_drop:\n",
    "    loadfile_copy_2.drop(index=indices_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "6312cbd4-6480-4a8e-a57e-ee1e14be3acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count1': 659, 'count2': 60, 'count3': 2}\n"
     ]
    }
   ],
   "source": [
    "print(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "ed413485-9c8f-4d55-a0a9-66cfe5f5a746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index(['MANDT', 'Document_type', 'document_type_desc', 'GJAHR_x', 'BUKRS',\n",
    "#        'GSBER', 'PRCTR', 'store_or_dc', 'KOSTL', 'month_in_fin_year', 'BELNR',\n",
    "#        'XBLNR', 'AUGBL', 'AUGDT', 'ZFBDT', 'ZBD1T', 'ZBD2T', 'NETDT', 'BUZEI',\n",
    "#        'altkt', 'hkont', 'suppl_no', 'BLDAT', 'BUDAT', 'CPUDT',\n",
    "#        'partition_date', 'dana_ingestion_date', 'shkzg',\n",
    "#        'Amount_in_local_currency', 'Amount_in_document_currency',\n",
    "#        'Tax_in_local_currency', 'Tax_in_document_currency', 'WAERS_x',\n",
    "#        'Batch_Input_session_name', 'sgtxt', 'fin_year_FI', 'LIFNR', 'RENR',\n",
    "#        'REDAT', 'LFSNR', 'GEBRF', 'GSMWB', 'GSMWF', 'WAERS_y', 'WENUM',\n",
    "#        'RGDAT', 'ABGST', 'AUFNR', 'VORGN', 'GJAHR_y', 'WEDAT', 'DEBNOTNO',\n",
    "#        'fin_year_IC', 'combined_suppl_no', 'combined_doc_no',\n",
    "#        'combined_inv_no', 'Supplier number (Sales Line)', 'Document number',\n",
    "#        'Invoice number', 'Document_Count', 'Document_types', 'RAN_count',\n",
    "#        'RAN_list'],\n",
    "#       dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "5d654520-69ba-414b-9bea-1438ce3192c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Loadfile length :  924910\n"
     ]
    }
   ],
   "source": [
    "print('Final Loadfile length : ', len(loadfile_copy_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "1bead305-943e-4337-b33a-c7dd845b39c8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COMPANY_CODE', 'SUPPLIER_NO', 'MIAG_SUPPLIER_NO', 'ORDER_NO',\n",
       "       'DOC_TYPE', 'INVOICE_NO', 'INVOICE_DATE', 'DELIVERY_NOTE_NO',\n",
       "       'TOTAL_AMT_DC', 'TOTAL_VAT_DC', 'CURRENCY', 'PRE_FINANCE_DATE',\n",
       "       'GOODS_RECEIPT_NO', 'GOODS_RECEIPT_DATE', 'INVOICE_ENTRY_DATE',\n",
       "       'INVOICE_STATUS', 'INVOICE_STATUS_INTERNAL', 'NET_DUE_DATE',\n",
       "       'DEBIT_NOTE_NO', 'REMITTANCE_ADVICE_NO', 'CLEARING_DATE', 'DOCUMENT_NO',\n",
       "       'STORE_NO', 'ARKTX', 'Description', 'AUGBL', 'BLDAT', 'Bus_year',\n",
       "       'AUGDT', 'MATCHING_DATE', 'MATCH_STATUS', 'SYNC_DATE', 'SYNC_STATUS',\n",
       "       'lookup_key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_copy_2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "72a720eb-9f12-40f5-8dcb-07a3272b61ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "COMPANY_CODE         int64\n",
       "SUPPLIER_NO         object\n",
       "MIAG_SUPPLIER_NO    object\n",
       "ORDER_NO            object\n",
       "DOC_TYPE            object\n",
       "                     ...  \n",
       "MATCHING_DATE       object\n",
       "MATCH_STATUS        object\n",
       "SYNC_DATE           object\n",
       "SYNC_STATUS         object\n",
       "lookup_key          object\n",
       "Length: 34, dtype: object"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfile_copy_2.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961fe023-1368-4427-b9e8-778b83fe1025",
   "metadata": {},
   "source": [
    "## Removing old-cleared FI Invoices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e0ff7130-0578-424f-bb0e-2ed5fd95c4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_copy_2['AUGDT'] = pd.to_datetime(loadfile_copy_2['AUGDT'], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "# Filter rows where AUGBL is not null and AUGDT is less than 01.08.2021\n",
    "old_FI_df = loadfile_copy_2[(loadfile_copy_2['INVOICE_STATUS']=='cleared-FI') & (loadfile_copy_2['AUGBL']!='') & (loadfile_copy_2['AUGDT'] < '2023-01-01')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "f64ae824-f187-48bb-9ed6-b46b8fd0ae9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "494529"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(old_FI_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "be2f2c1f-88e4-487d-8f99-d30c8734d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loadfile_df = loadfile_copy_2[~((loadfile_copy_2['INVOICE_STATUS']=='cleared-FI') & (loadfile_copy_2['AUGBL']!='') & (loadfile_copy_2['AUGDT'] < '2023-01-01'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "eb52b4c9-6d87-4d7d-8900-af45055ea458",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COMPANY_CODE', 'SUPPLIER_NO', 'MIAG_SUPPLIER_NO', 'ORDER_NO',\n",
       "       'DOC_TYPE', 'INVOICE_NO', 'INVOICE_DATE', 'DELIVERY_NOTE_NO',\n",
       "       'TOTAL_AMT_DC', 'TOTAL_VAT_DC', 'CURRENCY', 'PRE_FINANCE_DATE',\n",
       "       'GOODS_RECEIPT_NO', 'GOODS_RECEIPT_DATE', 'INVOICE_ENTRY_DATE',\n",
       "       'INVOICE_STATUS', 'INVOICE_STATUS_INTERNAL', 'NET_DUE_DATE',\n",
       "       'DEBIT_NOTE_NO', 'REMITTANCE_ADVICE_NO', 'CLEARING_DATE', 'DOCUMENT_NO',\n",
       "       'STORE_NO', 'ARKTX', 'Description', 'AUGBL', 'BLDAT', 'Bus_year',\n",
       "       'AUGDT', 'MATCHING_DATE', 'MATCH_STATUS', 'SYNC_DATE', 'SYNC_STATUS',\n",
       "       'lookup_key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loadfile_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "558e1b77-94fb-4653-ac06-6bba5f67fe0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cleared-MIAG', 'Invoice approval completed', 'cleared-FI',\n",
       "       'pre-financed', 'In progress', 'Direct Entry to FI / New Status'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loadfile_df['INVOICE_STATUS'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "b8f702b4-2b23-4e0f-bd9a-63bfa6be5874",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430381, 34)"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loadfile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9ec0bd04-c506-48ab-859c-93965bb6d707",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_16203/3868718509.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_loadfile_df['AUGDT'] = pd.to_datetime(new_loadfile_df['AUGDT'])\n",
      "/var/tmp/ipykernel_16203/3868718509.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_loadfile_df['AUGDT'] = new_loadfile_df['AUGDT'].dt.strftime('%d.%m.%Y')\n"
     ]
    }
   ],
   "source": [
    "new_loadfile_df['AUGDT'] = pd.to_datetime(new_loadfile_df['AUGDT'])\n",
    "\n",
    "# Convert to desired format\n",
    "new_loadfile_df['AUGDT'] = new_loadfile_df['AUGDT'].dt.strftime('%d.%m.%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "5e287fd5-abd7-4699-8f00-3470ff736241",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['COMPANY_CODE', 'SUPPLIER_NO', 'MIAG_SUPPLIER_NO', 'ORDER_NO',\n",
       "       'DOC_TYPE', 'INVOICE_NO', 'INVOICE_DATE', 'DELIVERY_NOTE_NO',\n",
       "       'TOTAL_AMT_DC', 'TOTAL_VAT_DC', 'CURRENCY', 'PRE_FINANCE_DATE',\n",
       "       'GOODS_RECEIPT_NO', 'GOODS_RECEIPT_DATE', 'INVOICE_ENTRY_DATE',\n",
       "       'INVOICE_STATUS', 'INVOICE_STATUS_INTERNAL', 'NET_DUE_DATE',\n",
       "       'DEBIT_NOTE_NO', 'REMITTANCE_ADVICE_NO', 'CLEARING_DATE', 'DOCUMENT_NO',\n",
       "       'STORE_NO', 'ARKTX', 'Description', 'AUGBL', 'BLDAT', 'Bus_year',\n",
       "       'AUGDT', 'MATCHING_DATE', 'MATCH_STATUS', 'SYNC_DATE', 'SYNC_STATUS',\n",
       "       'lookup_key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loadfile_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "44e2def0-1cd4-425e-b3ab-f88a8a7a3af8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(430381, 34)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_loadfile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5fc2a862-579d-4665-96ac-de80c449250c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_loadfile_df = new_loadfile_df_copy2.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8d29fee9-9de0-4f51-8ca3-eb9273a3ff1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loadfile_df_copy2 = new_loadfile_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "377e70b4-14b6-4122-85bc-575803efc51f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[282], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241m+\u001b[39mb\u001b[38;5;241m+\u001b[39mz\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a+b+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42d4ae6-c70e-4a28-96f1-cb139d88cba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff_supp_duplicates_groups = new_loadfile_df.groupby([\"INVOICE_NO\", \"TOTAL_AMT_DC\", \"Bus_year\"]).filter(lambda x: (len(x) == 2) and (x[\"SUPPLIER_NO\"].nunique() > 1))\n",
    "\n",
    "print(\"1 : \", diff_supp_duplicates_groups.groupby(['INVOICE_NO', 'TOTAL_AMT_DC', 'Bus_year']).ngroups)\n",
    "\n",
    "invoice_numbers_diff_supp = diff_supp_duplicates_groups[\"INVOICE_NO\"].unique().tolist()\n",
    "\n",
    "# Step 2: Check if one has 'In Progress' and the other has a different status\n",
    "def should_delete(group):\n",
    "    has_in_progress = (group[\"INVOICE_STATUS\"] == \"In progress\").sum()  # Count \"In Progress\" rows\n",
    "    has_other_status = (group[\"INVOICE_STATUS\"] != \"In progress\").sum()  # Count other status rows\n",
    "    return has_in_progress == 1 and has_other_status == 1  # Only delete if exactly one is \"In Progress\"\n",
    "\n",
    "# Step 3: Identify groups where deletion should happen\n",
    "to_be_deleted_groups = diff_supp_duplicates_groups.groupby([\"INVOICE_NO\", \"TOTAL_AMT_DC\", \"Bus_year\"]).filter(lambda x: should_delete(x))\n",
    "\n",
    "print(\"2 : \", to_be_deleted_groups.groupby(['INVOICE_NO', 'TOTAL_AMT_DC', 'Bus_year']).ngroups)\n",
    "\n",
    "invoice_numbers_to_delete = to_be_deleted_groups[\"INVOICE_NO\"].unique().tolist()\n",
    "\n",
    "# Step 4: Keep only rows where INVOICE_STATUS is \"In Progress\"\n",
    "to_be_deleted_groups = to_be_deleted_groups[to_be_deleted_groups[\"INVOICE_STATUS\"] == \"In progress\"]\n",
    "\n",
    "# Step 5: Remove \"In Progress\" records from original DataFrame\n",
    "new_loadfile_df = new_loadfile_df[~new_loadfile_df.index.isin(to_be_deleted_groups.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5ffbcf-2072-4278-a00e-1f8426ebb537",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "invoice_numbers_diff_supp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683bf969-b0ba-47b1-93e4-99b3ee23a391",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "invoice_numbers_to_delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747b80a-860a-4362-a5fa-160cf2915cc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_loadfile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e583ab3-754d-409d-b40c-d570c1bb816a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_loadfile_df_copy2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "60195c68-3c50-4287-8f51-64b12687722b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COMPANY_CODE</th>\n",
       "      <th>SUPPLIER_NO</th>\n",
       "      <th>MIAG_SUPPLIER_NO</th>\n",
       "      <th>ORDER_NO</th>\n",
       "      <th>DOC_TYPE</th>\n",
       "      <th>INVOICE_NO</th>\n",
       "      <th>INVOICE_DATE</th>\n",
       "      <th>DELIVERY_NOTE_NO</th>\n",
       "      <th>TOTAL_AMT_DC</th>\n",
       "      <th>TOTAL_VAT_DC</th>\n",
       "      <th>CURRENCY</th>\n",
       "      <th>PRE_FINANCE_DATE</th>\n",
       "      <th>GOODS_RECEIPT_NO</th>\n",
       "      <th>GOODS_RECEIPT_DATE</th>\n",
       "      <th>INVOICE_ENTRY_DATE</th>\n",
       "      <th>INVOICE_STATUS</th>\n",
       "      <th>INVOICE_STATUS_INTERNAL</th>\n",
       "      <th>NET_DUE_DATE</th>\n",
       "      <th>DEBIT_NOTE_NO</th>\n",
       "      <th>REMITTANCE_ADVICE_NO</th>\n",
       "      <th>CLEARING_DATE</th>\n",
       "      <th>DOCUMENT_NO</th>\n",
       "      <th>STORE_NO</th>\n",
       "      <th>ARKTX</th>\n",
       "      <th>Description</th>\n",
       "      <th>AUGBL</th>\n",
       "      <th>BLDAT</th>\n",
       "      <th>Bus_year</th>\n",
       "      <th>AUGDT</th>\n",
       "      <th>MATCHING_DATE</th>\n",
       "      <th>MATCH_STATUS</th>\n",
       "      <th>SYNC_DATE</th>\n",
       "      <th>SYNC_STATUS</th>\n",
       "      <th>lookup_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>183970</th>\n",
       "      <td>3142</td>\n",
       "      <td>1000061122</td>\n",
       "      <td>3500028787</td>\n",
       "      <td>0000000502590005</td>\n",
       "      <td>WE</td>\n",
       "      <td>ERS2025000004880</td>\n",
       "      <td>24.01.2025</td>\n",
       "      <td>EYM2025000000685</td>\n",
       "      <td>308999.0</td>\n",
       "      <td>3059.4</td>\n",
       "      <td>TRY</td>\n",
       "      <td></td>\n",
       "      <td>000425206</td>\n",
       "      <td>25.01.2025</td>\n",
       "      <td>27.01.2025</td>\n",
       "      <td>Invoice approval completed</td>\n",
       "      <td>0256</td>\n",
       "      <td>01.02.2025</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0810152169</td>\n",
       "      <td>1045</td>\n",
       "      <td>WE#0810152169#20250124#2025#001</td>\n",
       "      <td>open</td>\n",
       "      <td></td>\n",
       "      <td>24.01.2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06.02.2025</td>\n",
       "      <td>No Matching Requested</td>\n",
       "      <td>06.02.2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1000061122_0810152169_ERS2025000004880_2025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183971</th>\n",
       "      <td>3142</td>\n",
       "      <td>1000061122</td>\n",
       "      <td>3500028787</td>\n",
       "      <td>0000000502590005</td>\n",
       "      <td>MV</td>\n",
       "      <td>ERS2025000004880</td>\n",
       "      <td>24.01.2025</td>\n",
       "      <td>EYM2025000000685</td>\n",
       "      <td>308999.0</td>\n",
       "      <td>3059.4</td>\n",
       "      <td>TRY</td>\n",
       "      <td>28.01.2025</td>\n",
       "      <td>000425206</td>\n",
       "      <td>25.01.2025</td>\n",
       "      <td>27.01.2025</td>\n",
       "      <td>Invoice approval completed</td>\n",
       "      <td>0256</td>\n",
       "      <td>01.02.2025</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>28.01.2025</td>\n",
       "      <td>0810152169</td>\n",
       "      <td>0000</td>\n",
       "      <td>MV#0810152169#20250124#2025#001</td>\n",
       "      <td>pre-financed</td>\n",
       "      <td></td>\n",
       "      <td>24.01.2025</td>\n",
       "      <td>2025</td>\n",
       "      <td>NaN</td>\n",
       "      <td>06.02.2025</td>\n",
       "      <td>No Matching Requested</td>\n",
       "      <td>06.02.2025</td>\n",
       "      <td>1</td>\n",
       "      <td>1000061122_0810152169_ERS2025000004880_2025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        COMPANY_CODE SUPPLIER_NO MIAG_SUPPLIER_NO          ORDER_NO DOC_TYPE  \\\n",
       "183970          3142  1000061122       3500028787  0000000502590005       WE   \n",
       "183971          3142  1000061122       3500028787  0000000502590005       MV   \n",
       "\n",
       "              INVOICE_NO INVOICE_DATE  DELIVERY_NOTE_NO TOTAL_AMT_DC  \\\n",
       "183970  ERS2025000004880   24.01.2025  EYM2025000000685     308999.0   \n",
       "183971  ERS2025000004880   24.01.2025  EYM2025000000685     308999.0   \n",
       "\n",
       "       TOTAL_VAT_DC CURRENCY PRE_FINANCE_DATE GOODS_RECEIPT_NO  \\\n",
       "183970       3059.4      TRY                         000425206   \n",
       "183971       3059.4      TRY       28.01.2025        000425206   \n",
       "\n",
       "       GOODS_RECEIPT_DATE INVOICE_ENTRY_DATE              INVOICE_STATUS  \\\n",
       "183970         25.01.2025         27.01.2025  Invoice approval completed   \n",
       "183971         25.01.2025         27.01.2025  Invoice approval completed   \n",
       "\n",
       "       INVOICE_STATUS_INTERNAL NET_DUE_DATE DEBIT_NOTE_NO  \\\n",
       "183970                    0256   01.02.2025                 \n",
       "183971                    0256   01.02.2025                 \n",
       "\n",
       "       REMITTANCE_ADVICE_NO CLEARING_DATE DOCUMENT_NO STORE_NO  \\\n",
       "183970                                     0810152169     1045   \n",
       "183971                         28.01.2025  0810152169     0000   \n",
       "\n",
       "                                  ARKTX   Description AUGBL       BLDAT  \\\n",
       "183970  WE#0810152169#20250124#2025#001          open        24.01.2025   \n",
       "183971  MV#0810152169#20250124#2025#001  pre-financed        24.01.2025   \n",
       "\n",
       "       Bus_year AUGDT MATCHING_DATE           MATCH_STATUS   SYNC_DATE  \\\n",
       "183970     2025   NaN    06.02.2025  No Matching Requested  06.02.2025   \n",
       "183971     2025   NaN    06.02.2025  No Matching Requested  06.02.2025   \n",
       "\n",
       "       SYNC_STATUS                                   lookup_key  \n",
       "183970           1  1000061122_0810152169_ERS2025000004880_2025  \n",
       "183971           1  1000061122_0810152169_ERS2025000004880_2025  "
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "new_loadfile_df_copy2[(new_loadfile_df_copy2['INVOICE_NO']=='ERS2025000004880')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0459783-0f90-4859-a24e-a2681e353c64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6660f1a2-d3c7-4f76-8147-1d89dfd19c03",
   "metadata": {},
   "source": [
    "# M360 Portal upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964e358-93c2-4060-b337-93ad3d996612",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_cleared_df = new_loadfile_df[(new_loadfile_df['INVOICE_STATUS']=='cleared-MIAG') | (new_loadfile_df['INVOICE_STATUS']=='cleared-FI')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f42de5c-6889-4ee4-af78-a0780f13b9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_not_cleared_df = new_loadfile_df[~((new_loadfile_df['INVOICE_STATUS'] == 'cleared-MIAG') | (new_loadfile_df['INVOICE_STATUS'] == 'cleared-FI'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016d8513-36e5-42a6-aca1-84136c82a594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_cleared_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4e5f3b-950c-4935-8aca-e7268617ea06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_not_cleared_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046db442-b7fb-4850-a892-2344a102c073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_cleared_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d1c8c-63e8-4d6c-9d08-69c598312eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_cleared_df.drop(columns=['lookup_key'], inplace=True)\n",
    "loadfile_not_cleared_df.drop(columns=['lookup_key'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab331fbc-c0ec-499a-9f44-cd8143aa3f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "# db_instance_operations.writeProcessedTable(loadfile_cleared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac2418-d0ec-4e67-8c4a-2edb196c70fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_instance_operations.writeStagedTable(loadfile_not_cleared_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf36f2-8beb-4e4a-b3fc-a68a31bf96ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db_instance_operations.writeMergedTable(loadfile_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d78eb-96e3-48fe-9e2e-0ea4e4fc441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loadfile_df.drop(columns=['Bus_year', 'AUGBL', 'BLDAT', 'Description', 'AUGDT', 'lookup_key'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b3df2b-49a2-4132-aff6-1d97cb83c0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loadfile_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa3d825-8a7a-4922-aae4-70c2a1fb5563",
   "metadata": {},
   "source": [
    "# Loadfile Upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b90043e-f7c7-4846-b1af-72502a252951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "class Loadfile_upload:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bucket_name = \"miag-m360-test-bucket\"\n",
    "\n",
    "    def upload_dataframe_to_gcs(self, df, destination_blob_name, separator):\n",
    "        storage_client = storage.Client()\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "        try:\n",
    "            df.to_csv(temp_file.name, index=False, sep=separator, encoding='utf-8')\n",
    "            bucket = storage_client.bucket(self.bucket_name)\n",
    "            blob = bucket.blob(destination_blob_name)\n",
    "            blob.upload_from_filename(temp_file.name)\n",
    "            print(f\"CSV uploaded to {destination_blob_name} in bucket {self.bucket_name}.\")\n",
    "        finally:\n",
    "            temp_file.close()\n",
    "            os.remove(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0ddc78-a79c-4bd4-beb4-1648d42ba026",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_upload = Loadfile_upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab64f9e4-b15b-4081-8a59-ffb5b0d0cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_loadfile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a640c44-59ce-483c-b407-32c5d3b4054c",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_datetime = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "loadfile_upload.upload_dataframe_to_gcs(new_loadfile_df, f\"analysis/load.360.35.{current_datetime}.001_test_internal_s6_new3.csv\", separator=',')\n",
    "loadfile_upload.upload_dataframe_to_gcs(new_loadfile_df, f\"share/load.360.35.{current_datetime}.001_new3.csv\", separator=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a69642-b870-49d7-ae5a-515e741c9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_instance_operations.__del__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e6e183-9f77-4b94-9654-44c3f1bcab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2779b16c-af1e-4fa3-b55a-5257879fa8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total time : ', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcb2ba1-3efe-4953-bb01-02d89ebabca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfile_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df54ffb-5b61-474e-b9d2-c126171d3a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4e170-0ae3-4e8d-a589-447afcdc978e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "end_time-start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99ae8ca-cdb7-491c-a0ee-0b8971119488",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[(merged_df['BELNR']=='0810000092') & (merged_df['XBLNR']=='BNI2021000000548')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b41c77-9a79-4b5f-b3c9-33b69d722961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4252bf6d-7602-4b44-92b5-c2ce6b2e9818",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_df[(loadfile_df['DOCUMENT_NO']=='0810000092') & (loadfile_df['INVOICE_NO']=='BNI2021000000548')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7d0732-3703-47d6-a1b0-259fa0d0f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mv_metadata_df[(mv_metadata_df['Document_Type_Count']==2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045dbf4a-c6e8-4e45-a8fa-07194500a367",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mv_metadata_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ad7f61-21a8-486d-ad9e-48f2408ce94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mv_metadata_df[(mv_metadata_df['Document_Count']==1) & (mv_metadata_df['RAN_list'].apply(lambda x: len(x) == 1 and x[0] == ''))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e7ee6-9cb0-418d-a384-72e3fcc988e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c546d8-7c21-48cc-8d46-fe2577db384e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_copy_3[(loadfile_copy_3['DOCUMENT_NO']=='0810059844') & (loadfile_copy_3['DOC_TYPE'] == 'MV') & (loadfile_copy_3['REMITTANCE_ADVICE_NO'] == '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224dad97-7184-46cd-9781-1a5afd2b0e6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loadfile_copy_3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3d4483-400b-40e1-8690-b12a941de60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "partial_paid_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c293561a-0827-427b-936d-54446abe7fe5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_merged_df_new=merge_fi_ic.merge(partial_paid_df, left_on=[\"suppl_no\", \"BELNR\", \"XBLNR\", \"fin_year_FI\"]\n",
    "                                 , right_on=[\"Supplier number (Sales Line)\", \"Document number\", \"Invoice number\", \"SFTP_bus_year\"]\n",
    "                                 , how=\"right\")\n",
    "print(\"pp merge shape : \", pp_merged_df_new.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016caf01-c352-4302-9f17-2c0663ddbf9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c16639-da91-48a5-9dc9-bbcbf27c602e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_merged_df_new[pp_merged_df_new['Document number']=='2900001147']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811962d9-5732-4cb9-a6ea-aa15ed24aa36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pp_merged_df_new['lookup_key'] = pp_merged_df_new.apply(\n",
    "    lambda x: f\"{x['suppl_no']}_{x['BELNR']}_{x['XBLNR']}_{x['fin_year_FI']}\", \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bd68e6-9ef1-4267-8470-589eb5aca0c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mv_metadata_df[(mv_metadata_df['Document_Count']==1) & (mv_metadata_df['RAN_list'].apply(lambda x: len(x) == 1 and x[0] == ''))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a12e2b-f9a7-4049-94dd-8f9f71cedfb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu118.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118:m126"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9090be-4e9b-4b73-a77a-3c2d2edba926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fcbc00-8ca8-441f-903a-c44173c1e2e2",
   "metadata": {},
   "source": [
    "# Latest_SFTP_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b71217-9918-46c0-b719-eca4319ad9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class Latest_SFTP_file:\n",
    "    def __init__(self):\n",
    "        self.bucket_name = \"miag-m360-test-bucket\"\n",
    "\n",
    "    def get_latest_file(self):\n",
    "        \"\"\"\n",
    "        Retrieves the name of the latest file uploaded to a specified GCS bucket.\n",
    "\n",
    "        :param bucket_name: Name of the GCS bucket\n",
    "        :return: The name of the latest file or None if the bucket is empty\n",
    "        \"\"\"\n",
    "        client = storage.Client()\n",
    "        bucket = client.get_bucket(self.bucket_name)\n",
    "        blobs = list(bucket.list_blobs())\n",
    "        \n",
    "        downloaded_files = [blob for blob in blobs if blob.name.startswith(\"Downloaded Files/\")]\n",
    "\n",
    "        if not downloaded_files:\n",
    "            print(\"No files in the 'Downloaded Files' folder.\")\n",
    "            return None\n",
    "\n",
    "        # Sort blobs by their updated timestamps (most recent first)\n",
    "        latest_blob = max(downloaded_files, key=lambda blob: blob.updated)\n",
    "\n",
    "        print(f\"The latest file in 'Downloaded Files' is: {latest_blob.name}\")\n",
    "        return latest_blob.name\n",
    "\n",
    "\n",
    "    def lowest_document_date(self, sftp_df):\n",
    "        lowest_date = pd.to_datetime(sftp_df['Document date'], format='%d.%m.%Y').min()\n",
    "        return lowest_date\n",
    "\n",
    "    def convert_to_yyyymmdd(self, date_str):\n",
    "        \"\"\"\n",
    "        Converts a date string from 'yyyy-mm-dd' to 'yyyymmdd' format.\n",
    "\n",
    "        :param date_str: Date string in 'yyyy-mm-dd' format\n",
    "        :return: Date string in 'yyyymmdd' format\n",
    "        \"\"\"\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")  # Parse the input date\n",
    "            return date_obj.strftime(\"%Y%m%d\")  # Format it to 'yyyymmdd'\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Invalid date format, expected 'yyyy-mm-dd'\")\n",
    "\n",
    "    def convert_to_yyyy_mm_dd(self, date_str):\n",
    "        \"\"\"\n",
    "        Converts a date string from 'yyyymmdd' to 'yyyy-mm-dd' format.\n",
    "\n",
    "        :param date_str: Date string in 'yyyymmdd' format\n",
    "        :return: Date string in 'yyyy-mm-dd' format\n",
    "        \"\"\"\n",
    "        try:\n",
    "            date_obj = datetime.strptime(date_str, \"%Y-%m-%d %H:%M:%S\")  # Parse the input date\n",
    "            return date_obj.strftime(\"%Y-%m-%d\")  # Format it to 'yyyy-mm-dd'\n",
    "        except ValueError:\n",
    "            raise ValueError(\"Invalid date format, expected 'yyyy-mm-dd'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26106ce3-41fc-4e0b-8dd7-0baed685cde5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "439e87c9-2f75-4f86-8869-4630a41aefe3",
   "metadata": {},
   "source": [
    "# Storage_Bucket_Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7cc5153-dd9c-4925-8b01-acc3d66b2fd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import io\n",
    "\n",
    "\n",
    "class Storage_Bucket_Operations:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bucket_name = \"miag-m360-test-bucket\"\n",
    "        self.download_files_path = \"Downloaded Files\"\n",
    "\n",
    "    def readFromBucket(self, sftp_file):\n",
    "        client = storage.Client(project='cf-hada-bsc-mcctk-mia-kg')\n",
    "        bucket = client.get_bucket(self.bucket_name)\n",
    "        blob = bucket.blob(f\"{sftp_file}\")\n",
    "        csv_data = blob.download_as_text()\n",
    "        sftp_df = pd.read_csv(io.StringIO(csv_data), index_col=False,\n",
    "                              dtype={\"Store\": str, \"Supplier number (MIAG)\": str, \"Remittance advice number\": str,\n",
    "                                     \"Supplier number (Sales Line)\": str, \"Document number\": str,\n",
    "                                     \"Invoice number\": str})\n",
    "        return sftp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eae3399-aba9-4828-835b-2e45eea86bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5e8895-f938-43fa-a7b1-2176b0b94da7",
   "metadata": {},
   "source": [
    "# DB_Instance_Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea446af6-6ec9-4470-8fee-b0adbf742a5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "import pandasql as ps\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "\n",
    "class DB_Instance_Operations:\n",
    "\n",
    "    def __init__(self):\n",
    "    # GCS bucket details\n",
    "        bucket_name = \"miag-m360-test-bucket\"\n",
    "        cert_files = {\n",
    "            \"sslrootcert\": \"hada-bsc-miag-m360-psql-pp-server-ca.pem\",\n",
    "            \"sslcert\": \"hada-bsc-miag-m360-psql-pp-client-cert.pem\",\n",
    "            \"sslkey\": \"hada-bsc-miag-m360-psql-pp-client-key.pem\"\n",
    "        }\n",
    "    \n",
    "        # Download certificate files into temporary files\n",
    "        self.cert_temp_paths = self.get_certificates_from_gcs(bucket_name, cert_files)\n",
    "    \n",
    "        # Create the database URL using temporary file paths\n",
    "        self.db_url = (\n",
    "            r\"postgresql+psycopg2://postgres:9rk$Y}gib9kZEucj@10.32.111.54:5432/MIAG-M360_UAT\"\n",
    "            f\"?sslmode=require\"\n",
    "            f\"&sslrootcert={self.cert_temp_paths['sslrootcert']}\"\n",
    "            f\"&sslcert={self.cert_temp_paths['sslcert']}\"\n",
    "            f\"&sslkey={self.cert_temp_paths['sslkey']}\"\n",
    "        )\n",
    "    \n",
    "        # Create the database engine\n",
    "        self.engine = create_engine(self.db_url)\n",
    "\n",
    "\n",
    "    def get_certificates_from_gcs(self, bucket_name, cert_files):\n",
    "        \"\"\"Fetch certificate files from GCS and store them in temporary files.\"\"\"\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        temp_paths = {}\n",
    "    \n",
    "        for key, gcs_path in cert_files.items():\n",
    "            # Create a temporary file\n",
    "            temp_file = tempfile.NamedTemporaryFile(delete=False)\n",
    "            blob = bucket.blob(gcs_path)\n",
    "            blob.download_to_filename(temp_file.name)\n",
    "            temp_file.close()\n",
    "            temp_paths[key] = temp_file.name\n",
    "            print(f\"{key} downloaded and stored temporarily at {temp_file.name}\")\n",
    "    \n",
    "        return temp_paths\n",
    "\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Clean up temporary files.\"\"\"\n",
    "        for path in self.cert_temp_paths.values():\n",
    "            if os.path.exists(path):\n",
    "                os.remove(path)\n",
    "                print(f\"Deleted temporary file: {path}\")\n",
    "\n",
    "\n",
    "    def readSDPTable(self):\n",
    "        # query = \"Delete from sdp_pool;\"\n",
    "        # with self.engine.connect() as connection:\n",
    "        #     connection.execute(text(query))\n",
    "        #     connection.commit()\n",
    "        # print(\"Rows deleted\")\n",
    "        query = \"select * from sdp_pool\"\n",
    "        sdp_df = pd.read_sql_query(query, self.engine)\n",
    "        return sdp_df\n",
    "\n",
    "    def updateSDP(self, sdp, sftp_df):\n",
    "        sq1 = \"SELECT DISTINCT `Supplier number (Sales Line)`, `Supplier number (MIAG)`, `Supplier name`, `Contract area` FROM sftp_df\"\n",
    "        miag2 = ps.sqldf(sq1, locals())\n",
    "        if sdp.shape[0] == 0:\n",
    "            sdp = miag2.copy()\n",
    "        else:\n",
    "            sdpq = \"Select distinct * from sdp\"\n",
    "            sdp_dist_df = ps.sqldf(sdpq)\n",
    "            new_supp_in_sftp_query = \"SELECT * from sftp_df where `Supplier number (Sales Line)` not in (SELECT `Supplier_Number_Sales` FROM sdp)\"\n",
    "            new_supp_df = ps.sqldf(new_supp_in_sftp_query)\n",
    "            push_to_sdp_query = \"Select `Supplier number (Sales Line)`, `Supplier number (MIAG)`, `Supplier name`, `Contract area` from new_supp_df union Select `Supplier_Number_Sales`, `Supplier_Number_MIAG`, `Supplier_Name`, `Contract_Area` from sdp\"\n",
    "            sdp = ps.sqldf(push_to_sdp_query)\n",
    "        return sdp\n",
    "\n",
    "    def writeSDPTable(self, sdp_df):\n",
    "        column_mapping = {\n",
    "            'Supplier number (Sales Line)': 'Supplier_Number_Sales',\n",
    "            'Supplier number (MIAG)': 'Supplier_Number_MIAG',\n",
    "            'Supplier name': 'Supplier_Name',\n",
    "            'Contract area': 'Contract_Area'\n",
    "        }\n",
    "        sdp_df.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.begin() as connection:\n",
    "            delete_query = text(\"Delete from sdp_pool\")\n",
    "            connection.execute(delete_query)\n",
    "            sdp_df.to_sql('sdp_pool', connection, if_exists='append', index=False)\n",
    "        print(\"Written back to SDP Table of DB Instance...\")\n",
    "\n",
    "    def getSupplierNumberForMMSIC(self):\n",
    "        new_supplier_list_for_mmsic = []\n",
    "        supplier_list_for_mmsic = self.readSDPTable()['Supplier_Number_Sales'].to_list()\n",
    "        for i in range(len(supplier_list_for_mmsic)):\n",
    "            new_supplier_list_for_mmsic.append(str(0) + supplier_list_for_mmsic[i][1:])\n",
    "        return new_supplier_list_for_mmsic\n",
    "\n",
    "    def getSupplierNumberForSISIC(self):\n",
    "        new_supplier_list_for_sisic = []\n",
    "        supplier_list_for_sisic = self.readSDPTable()['Supplier_Number_Sales'].to_list()\n",
    "        for i in range(len(supplier_list_for_sisic)):\n",
    "            new_supplier_list_for_sisic.append(str(1) + supplier_list_for_sisic[i][1:])\n",
    "        return new_supplier_list_for_sisic\n",
    "\n",
    "    def getSupplierNumberForFI(self):\n",
    "        new_supplier_list_for_fi = []\n",
    "        supplier_list_for_fi = self.readSDPTable()['Supplier_Number_Sales'].to_list()\n",
    "        for i in range(len(supplier_list_for_fi)):\n",
    "            new_supplier_list_for_fi.append(supplier_list_for_fi[i][5:])\n",
    "        return new_supplier_list_for_fi\n",
    "\n",
    "    def writeICTable(self, extracted_ic_df):\n",
    "        column_mapping = {\n",
    "            'LIFNR': 'lifnr',\n",
    "            'BELNR': 'belnr',\n",
    "            'RENR': 'renr',\n",
    "            'REDAT': 'redat',\n",
    "            'LFSNR': 'lfsnr',\n",
    "            'GEBRF': 'gebrf',\n",
    "            'GSMWB': 'gsmwb',\n",
    "            'GSMWF': 'gsmwf',\n",
    "            'WAERS': 'waers',\n",
    "            'WENUM': 'wenum',\n",
    "            'RGDAT': 'rgdat',\n",
    "            'ABGST': 'abgst',\n",
    "            'AUFNR': 'aufnr',\n",
    "            'VORGN': 'vorgn',\n",
    "            'GJAHR': 'gjahr',\n",
    "            'WEDAT': 'wedat',\n",
    "            'DEBNOTNO': 'debnotno',\n",
    "        }\n",
    "\n",
    "        extracted_ic_df.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from intermediate_ic\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "            extracted_ic_df.to_sql('intermediate_ic', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Intermediate IC Table of DB Instance...\")\n",
    "\n",
    "    def writeFITable(self, extracted_fi_df):\n",
    "        column_mapping = {\n",
    "            'MANDT': 'mandt',\n",
    "            'Document_type': 'document_type',\n",
    "            'document_type_desc': 'document_type_desc',\n",
    "            'GJAHR': 'gjahr',\n",
    "            'BUKRS': 'bukrs',\n",
    "            'GSBER': 'gsber',\n",
    "            'PRCTR': 'prctr',\n",
    "            'store_or_dc': 'store_or_dc',\n",
    "            'KOSTL': 'kostl',\n",
    "            'month_in_fin_year': 'month_in_fin_year',\n",
    "            'BELNR': 'belnr',\n",
    "            'XBLNR': 'xblnr',\n",
    "            'AUGBL': 'augbl',\n",
    "            'AUGDT': 'augdt',\n",
    "            'ZFBDT': 'zfbdt',\n",
    "            'ZBD1T': 'zbd1t',\n",
    "            'ZBD2T': 'zbd2t',\n",
    "            'NETDT': 'netdt',\n",
    "            'BUZEI': 'buzei',\n",
    "            'altkt': 'altkt',\n",
    "            'hkont': 'hkont',\n",
    "            'suppl_no': 'suppl_no',\n",
    "            'BLDAT': 'bldat',\n",
    "            'BUDAT': 'budat',\n",
    "            'CPUDT': 'cpudt',\n",
    "            'partition_date': 'partition_date',\n",
    "            'dana_ingestion_date': 'dana_ingestion_date',\n",
    "            'shkzg': 'shkzg',\n",
    "            'Amount_in_local_currency': 'amount_in_local_currency',\n",
    "            'Amount_in_document_currency': 'amount_in_document_currency',\n",
    "            'Tax_in_local_currency': 'tax_in_local_currency',\n",
    "            'Tax_in_document_currency': 'tax_in_document_currency',\n",
    "            'WAERS': 'waers',\n",
    "            'Batch_Input_session_name': 'batch_input_session_name',\n",
    "            'sgtxt': 'sgtxt',\n",
    "        }\n",
    "\n",
    "        extracted_fi_df.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from intermediate_fi\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "        extracted_fi_df.to_sql('intermediate_fi', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Intermediate FI Table of DB Instance...\")\n",
    "\n",
    "    def readICTable(self):\n",
    "        query = \"select * from intermediate_ic\"\n",
    "        df_ic = pd.read_sql_query(query, self.engine)\n",
    "        return df_ic\n",
    "\n",
    "    def readFITable(self):\n",
    "        query = \"select * from intermediate_fi\"\n",
    "        df_fi = pd.read_sql_query(query, self.engine)\n",
    "        return df_fi\n",
    "    \n",
    "    def writeMergedTable(self, loadfile_df_copy):\n",
    "        loadfile_df_copy['COMPANY_CODE'] = loadfile_df_copy['COMPANY_CODE'].astype(str)\n",
    "        loadfile_df_copy['SUPPLIER_NO'] = loadfile_df_copy['SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['MIAG_SUPPLIER_NO'] = loadfile_df_copy['MIAG_SUPPLIER_NO'].astype(str)\n",
    "        loadfile_df_copy['ORDER_NO'] = loadfile_df_copy['ORDER_NO'].astype(str)\n",
    "        loadfile_df_copy['DOC_TYPE'] = loadfile_df_copy['DOC_TYPE'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_NO'] = loadfile_df_copy['INVOICE_NO'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_DATE'] = loadfile_df_copy['INVOICE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DELIVERY_NOTE_NO'] = loadfile_df_copy['DELIVERY_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['TOTAL_AMT_DC'] = pd.to_numeric(loadfile_df_copy['TOTAL_AMT_DC'])\n",
    "        loadfile_df_copy['TOTAL_VAT_DC'] = loadfile_df_copy['TOTAL_VAT_DC'].astype(str)\n",
    "        loadfile_df_copy['CURRENCY'] = loadfile_df_copy['CURRENCY'].astype(str)\n",
    "        loadfile_df_copy['PRE_FINANCE_DATE'] = pd.to_datetime(loadfile_df_copy['PRE_FINANCE_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['GOODS_RECEIPT_NO'] = loadfile_df_copy['GOODS_RECEIPT_NO'].astype(str)\n",
    "        loadfile_df_copy['GOODS_RECEIPT_DATE'] = pd.to_datetime(loadfile_df_copy['GOODS_RECEIPT_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_ENTRY_DATE'] = pd.to_datetime(loadfile_df_copy['INVOICE_ENTRY_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['INVOICE_STATUS'] = loadfile_df_copy['INVOICE_STATUS'].astype(str)\n",
    "        loadfile_df_copy['INVOICE_STATUS_INTERNAL'] = loadfile_df_copy['INVOICE_STATUS_INTERNAL'].astype(str)\n",
    "        loadfile_df_copy['NET_DUE_DATE'] = loadfile_df_copy['NET_DUE_DATE'].astype(str)\n",
    "        loadfile_df_copy['DEBIT_NOTE_NO'] = loadfile_df_copy['DEBIT_NOTE_NO'].astype(str)\n",
    "        loadfile_df_copy['REMITTANCE_ADVICE_NO'] = loadfile_df_copy['REMITTANCE_ADVICE_NO'].astype(str)\n",
    "        loadfile_df_copy['CLEARING_DATE'] = pd.to_datetime(loadfile_df_copy['CLEARING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['DOCUMENT_NO'] = loadfile_df_copy['DOCUMENT_NO'].astype(str)\n",
    "        loadfile_df_copy['STORE_NO'] = loadfile_df_copy['STORE_NO'].astype(str)\n",
    "        loadfile_df_copy['MATCHING_DATE'] = pd.to_datetime(loadfile_df_copy['MATCHING_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['MATCH_STATUS'] = loadfile_df_copy['MATCH_STATUS'].astype(str)\n",
    "        loadfile_df_copy['SYNC_DATE'] = pd.to_datetime(loadfile_df_copy['SYNC_DATE'], format='%d.%m.%Y')\n",
    "        loadfile_df_copy['SYNC_STATUS'] = loadfile_df_copy['SYNC_STATUS'].astype(str)\n",
    "        loadfile_df_copy['ARKTX'] = loadfile_df_copy['ARKTX'].astype(str)\n",
    "        \n",
    "        \n",
    "        column_mapping = {\n",
    "            'COMPANY_CODE': 'company_code',\n",
    "            'SUPPLIER_NO': 'supplier_no',\n",
    "            'MIAG_SUPPLIER_NO': 'miag_supplier',\n",
    "            'ORDER_NO': 'order_no',\n",
    "            'DOC_TYPE': 'doc_type',\n",
    "            'INVOICE_NO': 'invoice_no',\n",
    "            'INVOICE_DATE': 'invoice_date',\n",
    "            'DELIVERY_NOTE_NO': 'delivery_note_no',\n",
    "            'TOTAL_AMT_DC': 'total_amt_dc',\n",
    "            'TOTAL_VAT_DC': 'total_vat_dc',\n",
    "            'CURRENCY': 'currency',\n",
    "            'PRE_FINANCE_DATE': 'pre_finance_date',\n",
    "            'GOODS_RECEIPT_NO': 'goods_receipt_no',\n",
    "            'INVOICE_ENTRY_DATE': 'invoice_entry_date',\n",
    "            'INVOICE_STATUS': 'invoice_status',\n",
    "            'INVOICE_STATUS_INTERNAL': 'invoice_status_internal',\n",
    "            'NET_DUE_DATE': 'net_due_date',\n",
    "            'DEBIT_NOTE_NO': 'debit_note_no',\n",
    "            'REMITTANCE_ADVICE_NO': 'remittance_advice_no',\n",
    "            'DOCUMENT_NO': 'document_no',\n",
    "            'STORE_NO': 'store_no',\n",
    "            'ARKTX': 'arktx',\n",
    "            'CLEARING_DATE': 'clearing_date',\n",
    "            'GOODS_RECEIPT_DATE': 'goods_receipt_date',\n",
    "            'MATCHING_DATE': 'matching_date',\n",
    "            'MATCH_STATUS': 'match_status',\n",
    "            'SYNC_DATE': 'sync_date',\n",
    "            'SYNC_STATUS': 'sync_status'\n",
    "        }\n",
    "        date_columns = ['INVOICE_DATE', 'PRE_FINANCE_DATE', 'INVOICE_ENTRY_DATE', 'NET_DUE_DATE', 'CLEARING_DATE',\n",
    "                        'GOODS_RECEIPT_DATE', 'MATCHING_DATE', 'SYNC_DATE']\n",
    "        for column in date_columns:\n",
    "            loadfile_df_copy[column] = pd.to_datetime(loadfile_df_copy[column], format='%d.%m.%Y').dt.strftime('%m-%d-%Y')\n",
    "        loadfile_df_copy.rename(columns=column_mapping, inplace=True)\n",
    "        with self.engine.connect() as connection:\n",
    "            delete_query = text(\"Delete from tbl_merged_data\")\n",
    "            connection.execute(delete_query)\n",
    "            connection.commit()\n",
    "        loadfile_df_copy.to_sql('tbl_merged_data', self.engine, if_exists='append', index=False)\n",
    "        print(\"Written to Final 360 Table of DB Instance...\")\n",
    "\n",
    "\n",
    "# db_instance_ops = DB_Instance_Operations()\n",
    "# sdp_df = db_instance_ops.readSDPTable()\n",
    "# sdp_df = db_instance_ops.updateSDP(sdp_df, sftp_df)\n",
    "# db_instance_ops.writeSDPTable(sdp_df)\n",
    "# sdp_supplier_list_for_mmsic = db_instance_ops.getSupplierNumberForMMSIC()\n",
    "# sdp_supplier_list_for_sisic = db_instance_ops.getSupplierNumberForSISIC()\n",
    "# sdp_supplier_list_for_fi = db_instance_ops.getSupplierNumberForFI()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d55085-4de9-4b01-a94a-78014226b71b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f31b37d-c79d-4f9c-bdb7-e516693c2bee",
   "metadata": {},
   "source": [
    "# BigQuery_Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd0ab3a-f3b3-464e-b4d9-b155e2c29d57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "\n",
    "class BigQuery_Operations:\n",
    "    def __init__(self):\n",
    "        self.client = bigquery.Client()\n",
    "\n",
    "    def extract_MMSIC(self, sdp_supplier_list_for_mmsic):\n",
    "        add_string = \"\"\n",
    "        for i in range(len(sdp_supplier_list_for_mmsic)):\n",
    "            add_string += \"'\"\n",
    "            add_string += str(sdp_supplier_list_for_mmsic[i])\n",
    "            add_string += \"'\"\n",
    "            add_string += \", \"\n",
    "        add_string = add_string[:-2]\n",
    "        query = f\"\"\"\n",
    "                WITH LatestRecords AS (\n",
    "    SELECT \n",
    "        LIFNR, \n",
    "        RENR, \n",
    "        MAX(dana_ingestion_timestamp) AS latest_timestamp\n",
    "    FROM \n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.mmsic_to_dana_gr_invoice_header`\n",
    "    GROUP BY \n",
    "        LIFNR, RENR\n",
    ")\n",
    "SELECT \n",
    "    T1.LIFNR, \n",
    "    T1.BELNR, \n",
    "    T1.RENR, \n",
    "    T1.REDAT, \n",
    "    T1.LFSNR, \n",
    "    T1.GEBRF, \n",
    "    T1.GSMWB, \n",
    "    T1.GSMWF,\n",
    "    T1.WAERS,\n",
    "    T1.WENUM,\n",
    "    T1.RGDAT,\n",
    "    T1.ABGST,\n",
    "    T1.AUFNR,\n",
    "    T1.VORGN,\n",
    "    T1.GJAHR,\n",
    "    T2.WEDAT,\n",
    "    T1.DEBNOTNO\n",
    "FROM \n",
    "    `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.mmsic_to_dana_gr_invoice_header` AS T1\n",
    "LEFT JOIN (\n",
    "    SELECT DISTINCT  \n",
    "        VORGN, \n",
    "        WEDAT, \n",
    "        GJAHR \n",
    "    FROM\n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.mmsic_to_dana_gr_table_header`\n",
    ") AS T2\n",
    "ON \n",
    "    T1.VORGN = T2.VORGN\n",
    "JOIN \n",
    "    LatestRecords LR\n",
    "ON \n",
    "    T1.LIFNR = LR.LIFNR \n",
    "    AND T1.RENR = LR.RENR \n",
    "    AND T1.dana_ingestion_timestamp = LR.latest_timestamp\n",
    "WHERE\n",
    "    T1.LIFNR IN ({add_string})\n",
    "    AND T1.REDAT >= '20230501' \n",
    "ORDER BY \n",
    "    T1.dana_ingestion_timestamp;\n",
    "                \"\"\"\n",
    "        extracted_mmsic_df = self.client.query(query).to_dataframe()\n",
    "        return extracted_mmsic_df\n",
    "\n",
    "    def extract_SISIC(self, sdp_supplier_list_for_sisic):\n",
    "        add_string = \"\"\n",
    "        for i in range(len(sdp_supplier_list_for_sisic)):\n",
    "            add_string += \"'\"\n",
    "            add_string += str(sdp_supplier_list_for_sisic[i])\n",
    "            add_string += \"'\"\n",
    "            add_string += \", \"\n",
    "        add_string = add_string[:-2]\n",
    "        query = f\"\"\"\n",
    "                WITH LatestRecords AS (\n",
    "    SELECT \n",
    "        LIFNR, \n",
    "        RENR, \n",
    "        MAX(dana_ingestion_timestamp) AS latest_timestamp\n",
    "    FROM \n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.sis_to_dana_gr_invoice_header`\n",
    "    GROUP BY \n",
    "        LIFNR, RENR\n",
    ")\n",
    "SELECT \n",
    "    T1.LIFNR, \n",
    "    T1.BELNR, \n",
    "    T1.RENR, \n",
    "    T1.REDAT, \n",
    "    T1.LFSNR, \n",
    "    T1.GEBRF, \n",
    "    T1.GSMWB, \n",
    "    T1.GSMWF,\n",
    "    T1.WAERS,\n",
    "    T1.WENUM,\n",
    "    T1.RGDAT,\n",
    "    T1.ABGST,\n",
    "    T1.AUFNR,\n",
    "    T1.VORGN,\n",
    "    T1.GJAHR,\n",
    "    T2.WEDAT,\n",
    "    T1.DEBNOTNO\n",
    "FROM \n",
    "    `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.sis_to_dana_gr_invoice_header` AS T1\n",
    "LEFT JOIN (\n",
    "    SELECT DISTINCT  \n",
    "        VORGN, \n",
    "        WEDAT, \n",
    "        GJAHR \n",
    "    FROM\n",
    "        `metro-bi-dl-tur-prod.ingest_fgtf_mmsic.sis_to_dana_gr_table_header`\n",
    ") AS T2\n",
    "ON \n",
    "    T1.VORGN = T2.VORGN\n",
    "JOIN \n",
    "    LatestRecords LR\n",
    "ON \n",
    "    T1.LIFNR = LR.LIFNR \n",
    "    AND T1.RENR = LR.RENR \n",
    "    AND T1.dana_ingestion_timestamp = LR.latest_timestamp\n",
    "WHERE\n",
    "    T1.LIFNR IN ({add_string})\n",
    "    AND T1.REDAT >= '20230501'\n",
    "ORDER BY \n",
    "    T1.dana_ingestion_timestamp;\n",
    "                \"\"\"\n",
    "        extracted_sisic_df = self.client.query(query).to_dataframe()\n",
    "        return extracted_sisic_df\n",
    "\n",
    "    def extract_FI(self, sdp_supplier_list_for_fi):\n",
    "        add_string = \"\"\n",
    "        for i in range(len(sdp_supplier_list_for_fi)):\n",
    "            add_string += str(sdp_supplier_list_for_fi[i])\n",
    "            add_string += \", \"\n",
    "        add_string = add_string[:-2]\n",
    "        query = f\"\"\"\n",
    "                    DECLARE country STRING DEFAULT 'tur';\n",
    "        DECLARE current_fiscal_year INT64 DEFAULT 2024;\n",
    "        DECLARE store_flag STRING DEFAULT 'prctr';-- or 'gsber';\n",
    "        DECLARE end_month_id INT64 DEFAULT EXTRACT(YEAR FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH)) * 100 + EXTRACT(MONTH FROM DATE_SUB(CURRENT_DATE(), INTERVAL 1 MONTH));    \n",
    "        DECLARE start_year INT64 DEFAULT 2024;\n",
    "        DECLARE end_year INT64 DEFAULT 2025;\n",
    "        CREATE OR REPLACE TABLE metro-bi-wb-mag-figov-s00.data_integrity_proj.sap_tur_360data_BELNR_testdoc\n",
    "        AS(\n",
    "        WITH fidoc AS (\n",
    "        SELECT * ,\n",
    "        MAX  (dana_ingestion_timestamp) over (PARTITION BY MANDT, BELNR, GJAHR, BUKRS, bseg.BUZEI) as max_timestamp,\n",
    "        ROW_NUMBER () over (PARTITION BY MANDT, BELNR, GJAHR, BUKRS, bseg.BUZEI order by dana_ingestion_timestamp DESC) as rn\n",
    "        FROM metro-bi-dl-tur-prod.ingest_fgtf_sap.fidoc fi,\n",
    "        UNNEST (zbseg) AS bseg\n",
    "        WHERE 1=1\n",
    "        AND gjahr BETWEEN start_year AND end_year\n",
    "        ),\n",
    "        fidoc_unique AS (\n",
    "        SELECT *\n",
    "        , CASE  WHEN store_flag = 'gsber' THEN fi.gsber\n",
    "            WHEN store_flag = 'prctr' THEN fi.prctr\n",
    "        END AS business_area\n",
    "        FROM fidoc fi\n",
    "        WHERE fi.dana_ingestion_timestamp  = max_timestamp\n",
    "        AND rn = 1\n",
    "        )\n",
    "        SELECT\n",
    "        fi.MANDT\n",
    "        , zbkpf.blart AS Document_type\n",
    "        , doc_type.ltext as document_type_desc\n",
    "        , GJAHR\n",
    "        , BUKRS\n",
    "        , GSBER\n",
    "        , PRCTR\n",
    "        , cast(prctr as int64)-cast(bukrs as int64)*10000 as store_or_dc\n",
    "        , KOSTL\n",
    "        , zbkpf.monat as month_in_fin_year\n",
    "        , BELNR\n",
    "        , zbkpf.XBLNR\n",
    "        --, AUFNR\n",
    "        , AUGBL\n",
    "        , AUGDT\n",
    "        , ZFBDT\n",
    "        , ZBD1T\n",
    "        , ZBD2T\n",
    "        , NETDT\n",
    "        , BUZEI\n",
    "        , altkt\n",
    "        , hkont\n",
    "        , MOD(SAFE_CAST(fi.z_dana_lfa1.lifnr AS int64), 100000) as suppl_no\n",
    "        , zbkpf.BLDAT\n",
    "        , zbkpf.BUDAT\n",
    "        , zbkpf.CPUDT\n",
    "        , date(fi.PARTITIONTIME) partition_date\n",
    "        , date(fi.dana_ingestion_timestamp) dana_ingestion_date\n",
    "        , shkzg\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.dmbtr\n",
    "                    ELSE fi.dmbtr\n",
    "                    END                 as Amount_in_local_currency\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.wrbtr\n",
    "                    ELSE fi.wrbtr\n",
    "                    END                 as Amount_in_document_currency\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.mwsts\n",
    "                    ELSE fi.mwsts\n",
    "                    END                 as Tax_in_local_currency\n",
    "        ,      CASE WHEN shkzg = 'H' THEN (-1) * fi.wmwst\n",
    "                    ELSE fi.wmwst\n",
    "                    END                 as Tax_in_document_currency\n",
    "        , zbkpf.WAERS\n",
    "        , ZBKPF.GRPID AS Batch_Input_session_name\n",
    "        , sgtxt\n",
    "        -- *\n",
    "        FROM fidoc_unique fi\n",
    "        LEFT JOIN\n",
    "        ( select * from metro-bi-dl-tur-prod.ingest_fgtf_sap.t003t AS doc_type\n",
    "            WHERE 1=1\n",
    "            AND doc_type.spras = 'EN'\n",
    "            qualify dana_ingestion_timestamp = max(dana_ingestion_timestamp) over (partition by BLART, MANDT, SPRAS, SYSID)\n",
    "            order by doc_type.blart\n",
    "        ) AS doc_type\n",
    "            ON zbkpf.blart = doc_type.blart\n",
    "        where 1=1    \n",
    "        and MOD(SAFE_CAST(fi.z_dana_lfa1.lifnr AS int64), 100000) IN ({add_string})\n",
    "         and zbkpf.BLDAT >= '2023-05-01'\n",
    "        )\n",
    "                \"\"\"\n",
    "        extracted_fi_full_df = self.client.query(query).to_dataframe()\n",
    "        query2 = f\"select * from `metro-bi-wb-mag-figov-s00.data_integrity_proj.sap_tur_360data_BELNR_testdoc`\"\n",
    "        extracted_fi_df = self.client.query(query2).to_dataframe()\n",
    "        # extracted_fi_df = extracted_fi_df.drop('BUZEI', axis=1)\n",
    "        return extracted_fi_df\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     bq_ops = BigQuery_Operations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e14d1e0-2de7-4d67-80f7-bcd8b4ee3b55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dff47a7-70ee-44db-b5f9-b4d5f82f9538",
   "metadata": {},
   "source": [
    "# Clean_FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73f5cf93-3b30-46fa-bc56-fa11f462f166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Clean_FI:\n",
    "    def __init__(self):\n",
    "        self.doc_nos_with_no_ZFBDT = []\n",
    "        self.doc_nos_with_1_ZFBDT = []\n",
    "        self.doc_nos_with_more_ZFBDT = []\n",
    "\n",
    "    def clean_fi(self, df_fi):\n",
    "        df_fi['suppl_no'] = df_fi['suppl_no'].astype(str)\n",
    "        df_fi['GJAHR'] = df_fi['GJAHR'].astype(str)\n",
    "        df_fi['BELNR'] = df_fi['BELNR'].astype(str)\n",
    "        df_fi['XBLNR'] = df_fi['XBLNR'].astype(str)\n",
    "        \n",
    "        df_fi = (\n",
    "            df_fi\n",
    "            .groupby(['suppl_no', 'BELNR', 'XBLNR', 'GJAHR'], group_keys=False)\n",
    "            .apply(self.select_final_row)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        df_fi['BELNR'] = df_fi['BELNR'].replace('nan', np.nan)\n",
    "        df_fi['XBLNR'] = df_fi['XBLNR'].replace('nan', np.nan)\n",
    "        \n",
    "        df_fi['suppl_no'] = df_fi['suppl_no'].astype('int64')\n",
    "        df_fi['GJAHR'] = df_fi['GJAHR'].astype('int64')\n",
    "        df_fi['BELNR'] = df_fi['BELNR'].astype('object')\n",
    "        df_fi['XBLNR'] = df_fi['XBLNR'].astype('object')\n",
    "    \n",
    "        print(\"FI shape after cleaning : \", df_fi.shape)\n",
    "        return df_fi\n",
    "\n",
    "    def select_final_row(self, group):\n",
    "        # Filter rows where ZFBDT is present\n",
    "        group_with_ZFBDT = group[group['ZFBDT'].notna()]\n",
    "        number_ZFBDT_present = len(group_with_ZFBDT)\n",
    "\n",
    "        # Case: number_ZFBDT_present > 1\n",
    "        if number_ZFBDT_present > 1:\n",
    "            self.doc_nos_with_more_ZFBDT.append(\n",
    "    (group['suppl_no'].iloc[0], group['BELNR'].iloc[0], group['XBLNR'].iloc[0], group['GJAHR'].iloc[0]))\n",
    "            # Check if any rows have ZBD1T or ZBD2T present\n",
    "            rows_with_ZBD = group_with_ZFBDT[group_with_ZFBDT['ZBD1T'].notna() | group_with_ZFBDT['ZBD2T'].notna()]\n",
    "            if not rows_with_ZBD.empty:\n",
    "                # Select the first row from rows_with_ZBD\n",
    "                final_selected_row = rows_with_ZBD.iloc[0]\n",
    "            else:\n",
    "                # Select the row with the least ZFBDT\n",
    "                min_ZFBDT_rows = group_with_ZFBDT[group_with_ZFBDT['ZFBDT'] == group_with_ZFBDT['ZFBDT'].min()]\n",
    "                # If multiple rows have the least ZFBDT, choose the one with the least BUZEI\n",
    "                final_selected_row = min_ZFBDT_rows.loc[min_ZFBDT_rows['BUZEI'].idxmin()]\n",
    "        elif number_ZFBDT_present == 1:\n",
    "            self.doc_nos_with_1_ZFBDT.append(\n",
    "    (group['suppl_no'].iloc[0], group['BELNR'].iloc[0], group['XBLNR'].iloc[0], group['GJAHR'].iloc[0])\n",
    ")\n",
    "            # Case: number_ZFBDT_present == 1\n",
    "            final_selected_row = group_with_ZFBDT.iloc[0]\n",
    "        else:\n",
    "            # Case: number_ZFBDT_present == 0\n",
    "            # Select the row with the least BUZEI\n",
    "            self.doc_nos_with_no_ZFBDT.append(\n",
    "    (group['suppl_no'].iloc[0], group['BELNR'].iloc[0], group['XBLNR'].iloc[0], group['GJAHR'].iloc[0])\n",
    ")\n",
    "            final_selected_row = group.loc[group['BUZEI'].idxmin()]\n",
    "\n",
    "        return final_selected_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9657fc3d-57ff-4b89-8d01-2fe1d80c7b02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5cd5e4f-76f4-409c-93c7-88778a6eafea",
   "metadata": {},
   "source": [
    "# Typecast_FI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24e34efc-88e1-439d-af90-72a46bda1167",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class Typecast_FI:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def typecast_fi(self, df_fi):\n",
    "        column_types = {\n",
    "            \"MANDT\": \"object\",\n",
    "            \"Document_type\": \"object\",\n",
    "            \"document_type_desc\": \"object\",\n",
    "            \"GJAHR\": \"Int64\",  # Nullable integer\n",
    "            \"BUKRS\": \"object\",\n",
    "            \"GSBER\": \"object\",\n",
    "            \"PRCTR\": \"object\",\n",
    "            \"store_or_dc\": \"Int64\",  # Nullable integer\n",
    "            \"KOSTL\": \"object\",\n",
    "            \"month_in_fin_year\": \"Int64\",  # Nullable integer\n",
    "            \"BELNR\": \"object\",\n",
    "            \"XBLNR\": \"object\",\n",
    "            \"AUGBL\": \"object\",\n",
    "            \"AUGDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"ZFBDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"ZBD1T\": \"float64\",\n",
    "            \"ZBD2T\": \"float64\",\n",
    "            \"NETDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"BUZEI\": \"Int64\",  # Nullable integer\n",
    "            \"altkt\": \"object\",\n",
    "            \"hkont\": \"object\",\n",
    "            \"suppl_no\": \"Int64\",  # Nullable integer\n",
    "            \"BLDAT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"BUDAT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"CPUDT\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"partition_date\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"dana_ingestion_date\": \"datetime64[ns]\",  # Assuming dbdate maps to datetime\n",
    "            \"shkzg\": \"object\",\n",
    "            \"Amount_in_local_currency\": \"float64\",\n",
    "            \"Amount_in_document_currency\": \"float64\",\n",
    "            \"Tax_in_local_currency\": \"float64\",\n",
    "            \"Tax_in_document_currency\": \"float64\",\n",
    "            \"WAERS\": \"object\",\n",
    "            \"Batch_Input_session_name\": \"object\",\n",
    "            \"sgtxt\": \"object\",\n",
    "        }\n",
    "\n",
    "        # Convert columns to the appropriate types\n",
    "        for col, dtype in column_types.items():\n",
    "            if dtype == \"datetime64[ns]\":\n",
    "                # Convert to datetime while handling errors\n",
    "                df_fi[col] = pd.to_datetime(df_fi[col], errors=\"coerce\")\n",
    "            elif dtype == \"Int64\":\n",
    "                # Convert to Pandas nullable integer\n",
    "                df_fi[col] = df_fi[col].astype(\"Int64\")\n",
    "            else:\n",
    "                # Convert to the specified type\n",
    "                df_fi[col] = df_fi[col].astype(dtype)\n",
    "\n",
    "        print(\"Type casted cleaned FI for storage\")\n",
    "        return df_fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a6899-f6c3-49c2-a9d8-e061df0e6ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfe87256-86cd-40b3-bafe-47a7ef5494c1",
   "metadata": {},
   "source": [
    "# Clean_IC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "557c0d3e-1b3e-4989-9f8e-5cc4906f1bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Clean_IC:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def clean_ic(self, df_ic):\n",
    "        \n",
    "        df_ic[['LIFNR', 'BELNR', 'RENR', 'GEBRF', 'GJAHR']] = df_ic[['LIFNR', 'BELNR', 'RENR', 'GEBRF', 'GJAHR']].astype(str)\n",
    "        \n",
    "        df_ic = (\n",
    "            df_ic\n",
    "            .groupby(['LIFNR', 'BELNR', 'RENR', 'GEBRF', 'GJAHR'])\n",
    "            .apply(self.get_max_populated_row)\n",
    "            .reset_index(drop=True)\n",
    "        )\n",
    "        \n",
    "        df_ic['LIFNR'] = df_ic['LIFNR'].replace('nan', np.nan)\n",
    "        df_ic['BELNR'] = df_ic['BELNR'].replace('nan', np.nan)\n",
    "        df_ic['RENR'] = df_ic['RENR'].replace('nan', np.nan)\n",
    "        df_ic['GEBRF'] = df_ic['GEBRF'].replace('nan', np.nan)\n",
    "        df_ic['GJAHR'] = df_ic['GJAHR'].replace('nan', np.nan)\n",
    "        \n",
    "        df_ic['LIFNR'] = df_ic['LIFNR'].astype('object')\n",
    "        df_ic['BELNR'] = df_ic['BELNR'].astype('object')\n",
    "        df_ic['RENR'] = df_ic['RENR'].astype('object')\n",
    "        df_ic['GEBRF'] = df_ic['GEBRF'].astype('object')\n",
    "        df_ic['GJAHR'] = df_ic['GJAHR'].astype('object')\n",
    "        \n",
    "        print(\"IC shape after cleaning : \", df_ic.shape)\n",
    "        return df_ic\n",
    "\n",
    "    def get_max_populated_row(self, group):\n",
    "        # Count non-null values for each row\n",
    "        non_null_counts = group.notnull().sum(axis=1)\n",
    "        # Get the index of the row with the maximum count\n",
    "        max_index = non_null_counts.idxmax()\n",
    "        return group.loc[max_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc55e61-77f3-4435-8920-6a5a93942901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "095a9d41-2263-48f5-8cf6-13e8b87f1507",
   "metadata": {},
   "source": [
    "# ReverseCast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42b0d5a8-c1a3-40fc-8ed9-71dcc8dea3d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ReverseCast:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def reverse_cast_fi(self, df_fi):\n",
    "        df_fi.columns = ['MANDT', 'Document_type', 'document_type_desc', 'GJAHR', 'BUKRS',\n",
    "                         'GSBER', 'PRCTR', 'store_or_dc', 'KOSTL', 'month_in_fin_year', 'BELNR',\n",
    "                         'XBLNR', 'AUGBL', 'AUGDT', 'ZFBDT', 'ZBD1T', 'ZBD2T', 'NETDT', 'BUZEI',\n",
    "                         'altkt', 'hkont', 'suppl_no', 'BLDAT', 'BUDAT', 'CPUDT',\n",
    "                         'partition_date', 'dana_ingestion_date', 'shkzg',\n",
    "                         'Amount_in_local_currency', 'Amount_in_document_currency',\n",
    "                         'Tax_in_local_currency', 'Tax_in_document_currency', 'WAERS',\n",
    "                         'Batch_Input_session_name', 'sgtxt']\n",
    "        return df_fi\n",
    "\n",
    "    def reverse_cast_ic(self, df_ic):\n",
    "        df_ic.columns = ['LIFNR', 'BELNR', 'RENR', 'REDAT', 'LFSNR', 'GEBRF', 'GSMWB', 'GSMWF',\n",
    "                         'WAERS', 'WENUM', 'RGDAT', 'ABGST', 'AUFNR', 'VORGN', 'GJAHR', 'WEDAT',\n",
    "                         'DEBNOTNO']\n",
    "        return df_ic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d15e4c6-8f95-43eb-879c-31d1d0f7ff69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1671a297-b0d3-47e7-925f-ff77e7ed8858",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "61b15dc0-0739-41c8-9ee5-584b02dd1ddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Merge:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def merge(self, df_fi, df_ic, sftp_df):\n",
    "        merge_fi_ic = df_fi.merge(df_ic, left_on=[\"suppl_no\", \"BELNR\", \"XBLNR\", \"fin_year_FI\"],\n",
    "                                  right_on=[\"LIFNR\", \"BELNR\", \"RENR\", \"fin_year_IC\"], how=\"outer\")\n",
    "        \n",
    "        print(\"1st merge shape : \", merge_fi_ic.shape[0])\n",
    "\n",
    "        merge_fi_ic['GJAHR_x'] = (\n",
    "    pd.to_numeric(merge_fi_ic['GJAHR_x'], errors='coerce')  # Convert to numeric, set invalid to NaN\n",
    "    .astype('Int64')                                  # Convert to nullable integers\n",
    "    .astype('string')    )\n",
    "\n",
    "        merged_df = merge_fi_ic.merge(sftp_df, left_on=[\"suppl_no\", \"BELNR\", \"XBLNR\", \"GJAHR_x\"],\n",
    "                                      right_on=[\"Supplier number (Sales Line)\", \"Document number\", \"Invoice number\",\n",
    "                                                \"Business Year new\"],\n",
    "                                      how=\"outer\")\n",
    "\n",
    "        return merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571e5c15-11da-428d-89b0-32bb573d30fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4eb2fa80-50af-4ccd-9c40-81734b36b670",
   "metadata": {},
   "source": [
    "# IC_Transaction_File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4574c49-53ef-4e88-ac08-5ec1f9872c22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "class IC_Transaction_File:\n",
    "    def __init__(self):\n",
    "        self.bucket_name = \"miag-m360-test-bucket\"\n",
    "        self.file_path = \"ic_transaction_status_R.csv\"\n",
    "        self.encoding = 'windows-1252'\n",
    "        self.dtype = {\n",
    "        'TRANSACTION STATUS (ABGST)': 'str',\n",
    "        'LBL - \\nTRANSACTION STATUS (ABGST)': 'str',\n",
    "        'VIPA \\nTRANSACTION STATUS (ABGST)': 'str',\n",
    "        \"360 invoice status - MVP- Display on External 360\": 'str',\n",
    "            \n",
    "    }\n",
    "\n",
    "    def read_csv_from_gcs(self):\n",
    "\n",
    "        # Initialize a GCS client\n",
    "        client = storage.Client()\n",
    "\n",
    "        # Get the bucket\n",
    "        bucket = client.get_bucket(self.bucket_name)\n",
    "\n",
    "        # Fetch the blob (file) from the bucket\n",
    "        blob = bucket.blob(self.file_path)\n",
    "\n",
    "        # Download the blob content as bytes\n",
    "        data = blob.download_as_bytes()\n",
    "\n",
    "        # Read the CSV into a Pandas DataFrame\n",
    "        ic_transaction_df = pd.read_csv(io.BytesIO(data), encoding=self.encoding, dtype=self.dtype)\n",
    "\n",
    "        return ic_transaction_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab18415-9cab-43b9-86ef-c85bb3753a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ce9dcf8-0bef-4347-9eff-f4ab8363c20b",
   "metadata": {},
   "source": [
    "# Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc614354-7521-46d7-b39c-b11a686abecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from IC_Transaction_File import *\n",
    "\n",
    "class Postprocess:\n",
    "    def __init__(self):\n",
    "        self.ic_transaction_file = IC_Transaction_File()\n",
    "\n",
    "    def postprocess(self, merged_df):\n",
    "        split_columns = merged_df['ARKTX'].str.split('#', expand=True)\n",
    "\n",
    "        merged_df['Business Year SFTP'] = split_columns[3].apply(\n",
    "            lambda x: str(x) if pd.notnull(x) and x.isdigit() else pd.NA\n",
    "        ).astype('string')\n",
    "        merged_df['Line Item No. SFTP'] = split_columns[4].apply(\n",
    "            lambda x: str(x) if pd.notnull(x) and x.isdigit() else pd.NA\n",
    "        ).astype('string')\n",
    "\n",
    "\n",
    "\n",
    "        merged_df['Doc no. combined'] = merged_df['BELNR'].fillna(merged_df['Document number'])\n",
    "\n",
    "\n",
    "\n",
    "        merged_df[\"year\"] = merged_df.BLDAT.apply(\n",
    "            lambda x: x.split('.')[2] if isinstance(x, str) and len(x.split('.')) > 2 else None)\n",
    "        merged_df[\"month\"] = merged_df.BLDAT.apply(\n",
    "            lambda x: x.split('.')[1] if isinstance(x, str) and len(x.split('.')) > 1 else None)\n",
    "        merged_df[\"day\"] = merged_df.BLDAT.apply(\n",
    "            lambda x: x.split('.')[0] if isinstance(x, str) and len(x.split('.')) > 0 else None)\n",
    "        merged_df['BUZEI'] = merged_df['BUZEI'].apply(lambda x: int(x) if pd.notna(x) else x).astype('Int64')\n",
    "        mask_empty_ARKTX = merged_df[\"ARKTX\"].isna()\n",
    "        merged_df.loc[mask_empty_ARKTX, \"ARKTX\"] = merged_df[mask_empty_ARKTX].apply(\n",
    "            lambda row: (\n",
    "                f\"{row['Document_type']}#{row['BELNR']}#{row['year']}{row['month']}{row['day']}#{row['year']}#00{row['BUZEI']}\"\n",
    "                if all(\n",
    "                    pd.notna([row['Document_type'], row['BELNR'], row['year'], row['month'], row['day'], row['BUZEI']]))\n",
    "                else np.nan\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        merged_df['ZFBDT'] = pd.to_datetime(merged_df['ZFBDT'], format='%d.%m.%Y')\n",
    "        merged_df['ZBD1T'] = pd.to_numeric(merged_df['ZBD1T'], errors='coerce')\n",
    "        merged_df['ZBD2T'] = pd.to_numeric(merged_df['ZBD2T'], errors='coerce')\n",
    "        merged_df['NET_DUE_DATE'] = merged_df.apply(\n",
    "            lambda row: row['ZFBDT'] + pd.Timedelta(\n",
    "                days=row['ZBD1T'] if pd.notna(row['ZBD1T']) else (row['ZBD2T'] if pd.notna(row['ZBD2T']) else 0)),\n",
    "            axis=1\n",
    "        )\n",
    "        merged_df['NET_DUE_DATE'] = merged_df['NET_DUE_DATE'].dt.strftime('%d.%m.%Y')\n",
    "\n",
    "\n",
    "\n",
    "        ic_tran_status_df = self.ic_transaction_file.read_csv_from_gcs()\n",
    "        ic_tran_status_df = ic_tran_status_df[\n",
    "            [\"TRANSACTION STATUS (ABGST)\", \"LBL - \\nTRANSACTION STATUS (ABGST)\", \"VIPA \\nTRANSACTION STATUS (ABGST)\",\n",
    "             \"360 invoice status - MVP- Display on External 360\"]]\n",
    "        ic_tran_status_df[\"ABGST\"] = ic_tran_status_df[\"TRANSACTION STATUS (ABGST)\"].fillna(\n",
    "            ic_tran_status_df['LBL - \\nTRANSACTION STATUS (ABGST)']).fillna(\n",
    "            ic_tran_status_df[\"VIPA \\nTRANSACTION STATUS (ABGST)\"])\n",
    "        ic_tran_status_df = ic_tran_status_df[[\"ABGST\", \"360 invoice status - MVP- Display on External 360\"]]\n",
    "        ic_tran_status_df['ABGST'] = ic_tran_status_df['ABGST'].str.zfill(4)\n",
    "        ic_tran_status_df['ABGST'] = ic_tran_status_df['ABGST'].astype('string')\n",
    "        abgst_status_dict = ic_tran_status_df.set_index('ABGST')['360 invoice status - MVP- Display on External 360'].to_dict()\n",
    "\n",
    "\n",
    "\n",
    "        merged_df['ABGST'] = merged_df['ABGST'].astype('string')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Step 1: Populate 'Inv Stat' with \"cleared-MIAG\" for rows where 'RAN' is present\n",
    "        merged_df.loc[merged_df['Remittance advice number'].notna(), 'INVOICE_STATUS'] = \"cleared-MIAG\"\n",
    "        # Step 2: Populate 'Inv Stat' with \"cleared-FI\" for rows where 'Inv Stat' is null and 'AUGBL' is present\n",
    "        merged_df.loc[merged_df['INVOICE_STATUS'].isna() & merged_df['AUGBL'].notna(), 'INVOICE_STATUS'] = \"cleared-FI\"\n",
    "        # Step 3: Populate 'Inv Stat' with \"Invoice Approval completed\" for rows where 'Inv Stat' is null and 'BLDAT' is present\n",
    "        merged_df.loc[merged_df['INVOICE_STATUS'].isna() & merged_df[\n",
    "            'BLDAT'].notna(), 'INVOICE_STATUS'] = \"Invoice approval completed\"\n",
    "        # Step 4: Populate 'Inv Stat' based on mapping from abgst_status_dict for rows where 'Inv Stat' is null and 'ABGST' is present\n",
    "        merged_df.loc[merged_df['INVOICE_STATUS'].isna() & merged_df['ABGST'].notna(), 'INVOICE_STATUS'] = merged_df[\n",
    "            'ABGST'].map(abgst_status_dict)\n",
    "        # Step 5: Populate remaining 'Inv Stat' as \"In progress\" where 'Inv Stat' is still null\n",
    "        merged_df['INVOICE_STATUS'].fillna(\"Direct Entry to FI / New Status\", inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        merged_df['Line Item No. SFTP'] = (\n",
    "            merged_df['Line Item No. SFTP']\n",
    "            .apply(lambda x: str(x).lstrip('0') if pd.notna(x) else x)  # Remove leading zeros if not NA\n",
    "            .astype('Int64')  # Convert to Int64 type\n",
    "        )\n",
    "        merged_df['GJAHR_x'] = (\n",
    "            pd.to_numeric(merged_df['GJAHR_x'], errors='coerce')  # Convert to numeric, set invalid to NaN\n",
    "            .astype('Int64')  # Convert to nullable integers\n",
    "            .astype('string')  # Convert integers to strings, keep <NA>\n",
    "        )\n",
    "\n",
    "        print(\"Datatypes before GR Invoice Implementation : \", merged_df[['Document_type', 'Document type', 'GJAHR_x', 'Business Year SFTP', 'BUZEI', 'Line Item No. SFTP', 'Amount_in_local_currency', 'Gross amount']].dtypes)\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f7d18b-db20-4ff4-92db-7e7b2421e3ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa97870-4389-4568-9b1c-1b3cc9b64762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6b00660d-a8cd-49d0-ac07-af4eda72d085",
   "metadata": {},
   "source": [
    "# GR_Invoice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f142ca00-a19b-4fe8-8aad-a2360c024c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class GR_Invoice:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.df_all_zeros = pd.DataFrame()\n",
    "        self.df_non_zero = pd.DataFrame()\n",
    "\n",
    "        self.df_group_len_1 = pd.DataFrame()\n",
    "        self.df_group_len_2 = pd.DataFrame()\n",
    "        self.df_group_len_3 = pd.DataFrame()\n",
    "\n",
    "        self.both_cleared_or_one_progress_groups = pd.DataFrame()\n",
    "        self.gr_invoice_records_groups = pd.DataFrame()\n",
    "        self.not_cleared_miag_records_groups = pd.DataFrame()\n",
    "\n",
    "        self.cleared_df = pd.DataFrame()\n",
    "        self.other_df = pd.DataFrame()\n",
    "        self.failed_doc_nos = []\n",
    "\n",
    "        self.len_3_cleared_df = pd.DataFrame()\n",
    "        self.len_3_other_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def gr_invoice(self, merged_df):\n",
    "        print(\"Merged_df shape : \", merged_df.shape)\n",
    "        self.df_all_zeros = merged_df[merged_df['Doc no. combined'] == '0000000000']\n",
    "        self.df_non_zero = merged_df[merged_df['Doc no. combined'] != '0000000000']\n",
    "        grouped = self.df_non_zero.groupby('Doc no. combined')\n",
    "        self.df_group_len_2 = grouped.filter(lambda x: len(x) == 2)\n",
    "        self.df_group_len_1 = grouped.filter(lambda x: len(x) == 1)\n",
    "        self.df_group_len_3 = grouped.filter(lambda x: len(x) > 2)\n",
    "        print(\"\\n\")\n",
    "        print(\"All zeros : \", len(self.df_all_zeros))\n",
    "        print(\"Length 1 : \", len(self.df_group_len_1))\n",
    "        print(\"Length 2 : \", len(self.df_group_len_2))\n",
    "        print(\"Length 3 : \", len(self.df_group_len_3))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        gr_len2_start = time.time()\n",
    "        gr_len2_concat = self.gr_length_2()\n",
    "        gr_len2_end = time.time()\n",
    "        print(\"GR Inv Len2 Time : \",gr_len2_end-gr_len2_start)\n",
    "        gr_len3_concat = self.gr_length_3()\n",
    "        gr_len3_end = time.time()\n",
    "        print(\"GR Inv Len3 Time : \", gr_len3_end - gr_len2_end)\n",
    "\n",
    "        merged_df = pd.concat([gr_len2_concat, gr_len3_concat, self.df_group_len_1, self.df_all_zeros], ignore_index=True)\n",
    "        merged_df.reset_index(drop=True, inplace=True)\n",
    "        print(merged_df.shape)\n",
    "        return merged_df\n",
    "\n",
    "\n",
    "    def gr_length_2(self):\n",
    "        grouped = self.df_group_len_2.groupby('Doc no. combined')\n",
    "\n",
    "        # Initialize lists for storing groups\n",
    "        both_cleared_or_one_progress_groups = []\n",
    "        gr_invoice_records_groups = []\n",
    "        not_cleared_miag_records_groups = []\n",
    "\n",
    "        count = 0\n",
    "        # Iterate over groups with conditions\n",
    "        for doc_no, group in grouped:\n",
    "            count += 1\n",
    "            # print(count)\n",
    "            statuses = group['INVOICE_STATUS'].tolist()\n",
    "\n",
    "            # Check conditions for categorizing groups\n",
    "            if statuses == ['cleared-MIAG', 'cleared-MIAG'] or \\\n",
    "                    ('cleared-MIAG' in statuses and 'In progress' in statuses):\n",
    "                both_cleared_or_one_progress_groups.append(group)\n",
    "            elif 'cleared-MIAG' in statuses and \\\n",
    "                    any(status in ['cleared-FI', 'Invoice approval completed'] for status in statuses):\n",
    "                gr_invoice_records_groups.append(group)\n",
    "            elif all(status != 'cleared-MIAG' for status in statuses):\n",
    "                not_cleared_miag_records_groups.append(group)\n",
    "\n",
    "        # Concatenate the groups into DataFrames\n",
    "        self.both_cleared_or_one_progress = pd.concat(both_cleared_or_one_progress_groups, ignore_index=True)\n",
    "        self.gr_invoice_records = pd.concat(gr_invoice_records_groups, ignore_index=True)\n",
    "        self.not_cleared_miag_records = pd.concat(not_cleared_miag_records_groups, ignore_index=True)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"Length 2 : \", len(self.df_group_len_2))\n",
    "        print(\"Both cleared or one progress : \", len(self.both_cleared_or_one_progress))\n",
    "        print(\"GR Invoice Records : \", len(self.gr_invoice_records))\n",
    "        print(\"not_cleared_miag_records \", len(self.not_cleared_miag_records))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if (len(self.gr_invoice_records) == 0):\n",
    "            self.gr_invoice_records = pd.DataFrame(columns=self.df_group_len_2.columns)\n",
    "\n",
    "        grouped = self.gr_invoice_records.groupby('Doc no. combined')\n",
    "\n",
    "        # Initialize lists to store the rows based on INVOICE_STATUS\n",
    "        cleared_records = []\n",
    "        other_records = []\n",
    "\n",
    "        # Iterate through each group\n",
    "        for name, group in grouped:\n",
    "            # Separate rows based on 'INVOICE_STATUS' value\n",
    "            cleared_record = group[group['INVOICE_STATUS'] == 'cleared-MIAG']\n",
    "            other_record = group[group['INVOICE_STATUS'] != 'cleared-MIAG']\n",
    "\n",
    "            # Append to the lists if the record exists\n",
    "            if not cleared_record.empty:\n",
    "                cleared_records.append(\n",
    "                    cleared_record.iloc[0])  # Assuming there's only one 'cleared-MIAG' record per group\n",
    "            if not other_record.empty:\n",
    "                other_records.append(\n",
    "                    other_record.iloc[0])  # Assuming there's only one non-'cleared-MIAG' record per group\n",
    "\n",
    "        # Convert lists to DataFrames if needed\n",
    "        self.cleared_df = pd.DataFrame(cleared_records)\n",
    "        self.other_df = pd.DataFrame(other_records)\n",
    "\n",
    "        print(\"\\n\")\n",
    "        print(\"GR Invoice Records : \", len(self.gr_invoice_records))\n",
    "        print(\"Cleared df : \", len(self.cleared_df))\n",
    "        print(\"Non Cleared df : \", len(self.other_df))\n",
    "        print(\"\\n\")\n",
    "\n",
    "        if len(self.cleared_df) == 0:\n",
    "            self.cleared_df = pd.DataFrame(columns=self.gr_invoice_records.columns)\n",
    "\n",
    "        if len(self.other_df) == 0:\n",
    "            self.other_df = pd.DataFrame(columns=self.gr_invoice_records.columns)\n",
    "\n",
    "        # Loop through each row in cleared_df and perform validations\n",
    "        for index, row1 in self.cleared_df.iterrows():\n",
    "            # Get the corresponding row in other_df based on 'Doc no. combined'\n",
    "            row2 = self.other_df[self.other_df['Doc no. combined'] == row1['Doc no. combined']]\n",
    "\n",
    "            # Proceed only if there is exactly one matching row in other_df\n",
    "            if len(row2) == 1:\n",
    "                row2 = row2.iloc[0]\n",
    "\n",
    "                # Perform validation checks\n",
    "                if (\n",
    "                        pd.notnull(row1['Line Item No. SFTP']) and pd.notnull(row2['BUZEI']) and row1[\n",
    "                    'Line Item No. SFTP'] == row2['BUZEI'] and\n",
    "                        pd.notnull(row1['Document type']) and pd.notnull(row2['Document_type']) and row1[\n",
    "                    'Document type'] == row2['Document_type'] and\n",
    "                        pd.notnull(row1['Business Year SFTP']) and pd.notnull(row2['GJAHR_x']) and row1[\n",
    "                    'Business Year SFTP'] == row2['GJAHR_x'] and\n",
    "                        pd.notnull(row1['Gross amount']) and pd.notnull(row2['Amount_in_local_currency']) and row1[\n",
    "                    'Gross amount'] == row2['Amount_in_local_currency']\n",
    "                ):\n",
    "                    self.cleared_df.at[index, 'NET_DUE_DATE'] = row2['NET_DUE_DATE']\n",
    "\n",
    "                    # Check if 'Store' column in row1 is not empty\n",
    "                    if pd.notnull(row1['Store']) and row1['Store'] != '':\n",
    "                        # Directly remove row2 from other_df\n",
    "                        self.other_df = self.other_df.drop(row2.name)\n",
    "                    else:\n",
    "                        # Copy 'store_or_dc' value from row2 to 'Store' column in row1\n",
    "                        self.cleared_df.at[index, 'Store'] = row2['store_or_dc']\n",
    "                        # Remove row2 from other_df\n",
    "                        self.other_df = self.other_df.drop(row2.name)\n",
    "                else:\n",
    "                    # If validation fails, add the 'Doc no. combined' to the failed list\n",
    "                    self.failed_doc_nos.append(row1['Doc no. combined'])\n",
    "            # else:\n",
    "            #     # If no match or multiple matches, add to failed list\n",
    "            #     failed_doc_nos.append(row1['Doc no. combined'])\n",
    "\n",
    "        # Output the updated cleared_df, other_df, and the failed list\n",
    "        self.cleared_df.reset_index(drop=True, inplace=True)\n",
    "        self.other_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        print(\"Cleared df : \", len(self.cleared_df))\n",
    "        print(\"Non Cleared df : \", len(self.other_df))\n",
    "        print(\"Failed docs : \", len(self.failed_doc_nos))\n",
    "        print(\"Examples of Failed docs : \", self.failed_doc_nos[0:20])\n",
    "\n",
    "        gr_len2_concat = pd.concat([self.cleared_df, self.other_df, self.not_cleared_miag_records, self.both_cleared_or_one_progress], ignore_index=True)\n",
    "        gr_len2_concat.reset_index(drop=True, inplace=True)\n",
    "        return gr_len2_concat\n",
    "\n",
    "    def gr_length_3(self):\n",
    "        self.len_3_cleared_df = self.df_group_len_3[self.df_group_len_3['INVOICE_STATUS'] == 'cleared-MIAG'].copy()\n",
    "        self.len_3_other_df = self.df_group_len_3[self.df_group_len_3['INVOICE_STATUS'] != 'cleared-MIAG'].copy()\n",
    "\n",
    "        rows_to_delete = []\n",
    "        for index_other, row2 in self.len_3_other_df.iterrows():\n",
    "            # Find matching row in len_3_cleared_df\n",
    "            match = self.len_3_cleared_df[\n",
    "                (self.len_3_cleared_df['Line Item No. SFTP'] == row2['BUZEI']) &\n",
    "                (self.len_3_cleared_df['Document type'] == row2['Document_type']) &\n",
    "                (self.len_3_cleared_df['Business Year SFTP'] == row2['GJAHR_x']) &\n",
    "                (self.len_3_cleared_df['Gross amount'] == row2['Amount_in_local_currency']) &\n",
    "                (self.len_3_cleared_df['Doc no. combined'] == row2['Doc no. combined'])\n",
    "                ]\n",
    "\n",
    "            if not match.empty:\n",
    "                # Take the first matched row (assuming only one match is expected)\n",
    "                row1 = match.iloc[0]\n",
    "\n",
    "                self.len_3_cleared_df.loc[match.index[0], 'NET_DUE_DATE'] = row2['NET_DUE_DATE']\n",
    "\n",
    "                # Check the Store column in row1\n",
    "                if pd.isna(row1['Store']):\n",
    "                    # Check the store_or_dc column in row2\n",
    "                    if not pd.isna(row2.get('store_or_dc')):\n",
    "                        self.len_3_cleared_df.loc[match.index[0], 'Store'] = row2['store_or_dc']\n",
    "\n",
    "                # Mark the row2 for deletion\n",
    "                rows_to_delete.append(index_other)\n",
    "\n",
    "        # Delete rows from len_3_other_df that were processed\n",
    "        self.len_3_other_df.drop(index=rows_to_delete, inplace=True)\n",
    "\n",
    "        print(\"Length 3 : \", len(self.df_group_len_3))\n",
    "        print(\"len_3_cleared_df : \", len(self.len_3_cleared_df))\n",
    "        print(\"len_3_other_df : \", len(self.len_3_other_df))\n",
    "\n",
    "        gr_len3_concat = pd.concat([self.len_3_cleared_df, self.len_3_other_df], ignore_index=True)\n",
    "        gr_len3_concat.reset_index(drop=True, inplace=True)\n",
    "        return gr_len3_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43325c08-db2f-4435-b06a-6e6318a2ce4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b57d9988-640d-4811-9efb-adfe2b11fb8d",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1cadb7a-a6da-42a3-bdd3-73854048f5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from Merge import *\n",
    "# from Postprocess import *\n",
    "# from GR_Invoice import *\n",
    "\n",
    "\n",
    "class Standardize:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.merge = Merge()\n",
    "        self.postprocess = Postprocess()\n",
    "        self.gr_invoice = GR_Invoice()\n",
    "        self.clean_ic = Clean_IC()\n",
    "\n",
    "    def standardize(self, df_fi, df_ic, sftp_df):\n",
    "        df_ic['LIFNR'] = df_ic['LIFNR'].apply(\n",
    "            lambda x: '10000' + str(x) if not pd.isna(x) and len(str(x)) < 10 else str(x) if not pd.isna(\n",
    "                x) else '0' * 10\n",
    "        )\n",
    "        df_fi['suppl_no'] = df_fi['suppl_no'].apply(\n",
    "            lambda x: '10000' + str(x) if not pd.isna(x) and len(str(x)) < 10 else str(x) if not pd.isna(\n",
    "                x) else '0' * 10\n",
    "        )\n",
    "        df_ic['BELNR'] = df_ic['BELNR'].apply(\n",
    "            lambda x: '0' * (10 - len(str(x))) + str(x) if not pd.isna(x) else '0' * 10)\n",
    "        df_fi['BELNR'] = df_fi['BELNR'].apply(\n",
    "            lambda x: '0' * (10 - len(str(x))) + str(x) if not pd.isna(x) else '0' * 10)\n",
    "        sftp_df['Document number'] = sftp_df['Document number'].apply(\n",
    "            lambda x: '0' * (10 - len(str(x))) + str(x) if not pd.isna(x) else '0' * 10)\n",
    "\n",
    "\n",
    "\n",
    "        df_ic['LIFNR'] = df_ic['LIFNR'].apply(self.transform_number)\n",
    "        \n",
    "        new_ic_unique_count = df_ic[['LIFNR', 'BELNR', 'RENR', 'GEBRF', 'GJAHR']].drop_duplicates().shape[0]\n",
    "        print(\"Count of unique records in IC after cleaning : \", new_ic_unique_count)\n",
    "        \n",
    "        \n",
    "        df_ic = self.clean_ic.clean_ic(df_ic)\n",
    "        \n",
    "\n",
    "        columns_to_convert_ic = ['REDAT', 'RGDAT', 'WEDAT']\n",
    "        for col in columns_to_convert_ic:\n",
    "            df_ic[col] = pd.to_datetime(df_ic[col], format='%Y%m%d').dt.strftime('%d.%m.%Y')\n",
    "\n",
    "\n",
    "\n",
    "        columns_to_convert_fi = ['AUGDT', 'ZFBDT', 'NETDT', 'BLDAT', 'BUDAT', 'CPUDT', 'partition_date','dana_ingestion_date']\n",
    "        for col in columns_to_convert_fi:\n",
    "            df_fi[col] = pd.to_datetime(df_fi[col], format='%Y-%m-%d').dt.strftime('%d.%m.%Y')\n",
    "\n",
    "\n",
    "\n",
    "        df_fi['Amount_in_local_currency'] = df_fi['Amount_in_local_currency'] * -1\n",
    "\n",
    "        df_ic['fin_year_IC'] = pd.to_datetime(df_ic['REDAT'], format='%d.%m.%Y').dt.year.astype('int64')\n",
    "\n",
    "        df_fi['fin_year_FI'] = pd.to_datetime(df_fi['BLDAT'], format='%d.%m.%Y').dt.year.astype('int64')\n",
    "\n",
    "        split_columns_sftp_df = sftp_df['ARKTX'].str.split('#', expand=True)\n",
    "        sftp_df['Business Year new'] = split_columns_sftp_df[3].apply(\n",
    "            lambda x: str(x) if pd.notnull(x) and x.isdigit() else pd.NA\n",
    "        ).astype('string')\n",
    "\n",
    "\n",
    "        merge_start = time.time()\n",
    "        merged_df = self.merge.merge(df_fi, df_ic, sftp_df)\n",
    "        merge_end = time.time()\n",
    "        print(\"Merge time : \", merge_end-merge_start)\n",
    "\n",
    "        postprocess_start = time.time()\n",
    "        merged_df = self.postprocess.postprocess(merged_df)\n",
    "        postprocess_end = time.time()\n",
    "        print(\"Postprocess time : \", postprocess_end-postprocess_start)\n",
    "\n",
    "\n",
    "        merged_df = self.gr_invoice.gr_invoice(merged_df)\n",
    "\n",
    "\n",
    "        return merged_df\n",
    "\n",
    "\n",
    "\n",
    "    def transform_number(self, num):\n",
    "        if pd.isna(num):  # Check for NaN values\n",
    "            return np.nan  # Return NaN if the input is NaN\n",
    "        num_str = str(num)  # Convert to string\n",
    "\n",
    "        # If the number already starts with '10000', return it as is\n",
    "        if num_str.startswith('10000'):\n",
    "            return num_str\n",
    "\n",
    "        stripped_num = num_str.lstrip('0')  # Remove leading zeros\n",
    "        final_num = stripped_num.zfill(6)  # Ensure it has at least 6 digits\n",
    "        return '1000' + final_num  # Prepend '10000'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1e3c9d-181a-496d-896a-b8fd427ea19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eab282fd-7140-4c80-bda6-d6bf2e426954",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13bccce0-b3f7-4863-90d6-2d3f30d6c48f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from Clean_FI import *\n",
    "# from Clean_IC import *\n",
    "# from Typecast_FI import *\n",
    "# from DB_Instance_Operations import *\n",
    "# from ReverseCast import *\n",
    "# from Standardize import *\n",
    "\n",
    "class Preprocess:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.clean_fi = Clean_FI()\n",
    "        self.clean_ic = Clean_IC()\n",
    "        self.typecast_fi = Typecast_FI()\n",
    "        self.db_instance_operations_2 = DB_Instance_Operations()\n",
    "        self.reverse_cast = ReverseCast()\n",
    "        self.standardize = Standardize()\n",
    "\n",
    "    def preprocess(self, df_fi, sisic_df, mmsic_df, sftp_df):\n",
    "        df_fi = df_fi.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        sisic_df = sisic_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        mmsic_df = mmsic_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "\n",
    "\n",
    "        df_fi_copy = df_fi.copy()\n",
    "        df_ic = pd.concat([mmsic_df, sisic_df], ignore_index=True)\n",
    "        df_ic_copy = df_ic.copy()\n",
    "        df_fi = df_fi[df_fi['Document_type'] != 'PM']\n",
    "\n",
    "\n",
    "        doc_to_type_ic = dict(zip(df_ic['BELNR'], df_ic['RENR']))\n",
    "        df_fi['XBLNR'] = df_fi['XBLNR'].fillna(df_fi['BELNR'].map(doc_to_type_ic))\n",
    "        doc_to_type_fi = dict(zip(df_fi['XBLNR'], df_fi['BELNR']))\n",
    "        df_ic['BELNR'] = df_ic['BELNR'].str.strip().replace('', np.nan)\n",
    "        df_ic['BELNR'] = df_ic['BELNR'].fillna(df_ic['RENR'].map(doc_to_type_fi))\n",
    "\n",
    "\n",
    "        fi_unique_count = df_fi[['suppl_no', 'BELNR', 'XBLNR', 'GJAHR']].drop_duplicates().shape[0]\n",
    "        print(\"Count of unique records in FI before cleaning : \", fi_unique_count)\n",
    "\n",
    "        clean_fi_start = time.time()\n",
    "        df_fi = self.clean_fi.clean_fi(df_fi)\n",
    "        clean_fi_end = time.time()\n",
    "        print(\"Cleaning FI time : \", clean_fi_end-clean_fi_start)\n",
    "        df_fi = self.typecast_fi.typecast_fi(df_fi)\n",
    "        typecast_fi_end = time.time()\n",
    "        print(\"Typecasting FI time : \", typecast_fi_end-clean_fi_end)\n",
    "        self.db_instance_operations_2.writeFITable(df_fi)\n",
    "        write_fi_end = time.time()\n",
    "        print(\"Writing FI time : \", write_fi_end-typecast_fi_end)\n",
    "\n",
    "        ic_unique_count = df_ic[['LIFNR', 'BELNR', 'RENR', 'GEBRF', 'GJAHR']].drop_duplicates().shape[0]\n",
    "        print(\"Count of unique records in IC before cleaning : \", ic_unique_count)\n",
    "\n",
    "        clean_ic_start = time.time()\n",
    "        df_ic = self.clean_ic.clean_ic(df_ic)\n",
    "        clean_ic_end = time.time()\n",
    "        print(\"Cleaning IC time : \", clean_ic_end-clean_ic_start)\n",
    "        self.db_instance_operations_2.writeICTable(df_ic)\n",
    "        write_ic_end = time.time()\n",
    "        print(\"Writing IC time : \", write_ic_end-clean_ic_end)\n",
    "\n",
    "        df_fi = self.reverse_cast.reverse_cast_fi(df_fi)\n",
    "        df_ic = self.reverse_cast.reverse_cast_ic(df_ic)\n",
    "\n",
    "\n",
    "        merged_df = self.standardize.standardize(df_fi, df_ic, sftp_df)\n",
    "\n",
    "        return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b30674-05b2-4b65-850f-1305123fdd3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "61b6d78a-9829-4385-9a18-075640e86956",
   "metadata": {},
   "source": [
    "# Loadfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "08860ca0-de67-475c-831c-e7e3101fea71",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "class Loadfile:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.loadfile_df = pd.DataFrame([])\n",
    "\n",
    "    def loadfile(self, merged_df):\n",
    "        \n",
    "        unique_pairs = merged_df.drop_duplicates(subset=['Supplier number (Sales Line)', 'Supplier number (MIAG)'])\n",
    "        unique_pairs = unique_pairs.drop_duplicates(subset=['Supplier number (Sales Line)'], keep='first')\n",
    "        doc_to_type = dict(zip(unique_pairs['Supplier number (Sales Line)'], unique_pairs['Supplier number (MIAG)']))\n",
    "\n",
    "\n",
    "\n",
    "        company_code = 3142\n",
    "        self.loadfile_df['COMPANY_CODE'] = company_code\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['SUPPLIER_NO'] = merged_df['Supplier number (Sales Line)'].fillna(\n",
    "            merged_df['suppl_no']\n",
    "        ).fillna(\n",
    "            merged_df['LIFNR']\n",
    "        )\n",
    "\n",
    "\n",
    "        self.loadfile_df['MIAG_SUPPLIER_NO'] = merged_df['Supplier number (MIAG)']\n",
    "        self.loadfile_df['MIAG_SUPPLIER_NO'] = self.loadfile_df['MIAG_SUPPLIER_NO'].fillna(\n",
    "            self.loadfile_df['SUPPLIER_NO'].map(doc_to_type))\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['ORDER_NO'] = merged_df['AUFNR']\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['DOC_TYPE'] = merged_df['Document type'].where(merged_df['Document type'].notna(),\n",
    "                                                                   merged_df['Document_type'])\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['INVOICE_NO'] = merged_df['Invoice number'].where(merged_df['Invoice number'].notna(),\n",
    "                                                                      merged_df['XBLNR'])\n",
    "        self.loadfile_df['INVOICE_NO'] = merged_df['Invoice number'].fillna(\n",
    "            merged_df['XBLNR']\n",
    "        ).fillna(\n",
    "            merged_df['RENR']\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['INVOICE_DATE'] = merged_df['Document date'].fillna(merged_df['REDAT'])\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['DELIVERY_NOTE_NO'] = merged_df['LFSNR']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['TOTAL_AMT_DC'] = merged_df['Gross amount'].fillna(\n",
    "            merged_df['Amount_in_local_currency']\n",
    "        ).fillna(\n",
    "            merged_df['GEBRF']\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "        self.loadfile_df['TOTAL_VAT_DC'] = merged_df['GSMWF']\n",
    "\n",
    "        company_code = 3142\n",
    "        self.loadfile_df['COMPANY_CODE'] = company_code\n",
    "\n",
    "        country_currency_dict = {3142: 'TRY'}\n",
    "        self.loadfile_df['CURRENCY'] = self.loadfile_df['COMPANY_CODE'].map(country_currency_dict)\n",
    "\n",
    "        condition = (self.loadfile_df['DOC_TYPE'] == 'MV')\n",
    "\n",
    "        self.loadfile_df['PRE_FINANCE_DATE'] = np.where(condition, merged_df['Value date'], '')\n",
    "\n",
    "        self.loadfile_df['GOODS_RECEIPT_NO'] = merged_df['WENUM']\n",
    "\n",
    "        self.loadfile_df['GOODS_RECEIPT_DATE'] = merged_df['WEDAT']\n",
    "\n",
    "        self.loadfile_df['INVOICE_ENTRY_DATE'] = merged_df['RGDAT'].where(merged_df['RGDAT'].notna(), merged_df['BLDAT'])\n",
    "\n",
    "        self.loadfile_df['INVOICE_STATUS'] = merged_df['INVOICE_STATUS']\n",
    "\n",
    "        self.loadfile_df['INVOICE_STATUS_INTERNAL'] = merged_df['ABGST']\n",
    "\n",
    "        self.loadfile_df['NET_DUE_DATE'] = merged_df['NET_DUE_DATE']\n",
    "\n",
    "        self.loadfile_df['DEBIT_NOTE_NO'] = merged_df['DEBNOTNO']\n",
    "\n",
    "        self.loadfile_df['REMITTANCE_ADVICE_NO'] = np.where(\n",
    "            merged_df['INVOICE_STATUS'] == 'cleared-MIAG',\n",
    "            merged_df['Remittance advice number'],\n",
    "            '')\n",
    "\n",
    "        self.loadfile_df['CLEARING_DATE'] = merged_df['Value date'].where(merged_df['Value date'].notna(),\n",
    "                                                                     merged_df['AUGDT'])\n",
    "\n",
    "        self.loadfile_df['DOCUMENT_NO'] = merged_df['BELNR'].where(merged_df['BELNR'].notna(), merged_df['Document number'])\n",
    "\n",
    "        self.loadfile_df['STORE_NO'] = merged_df['Store'].fillna(merged_df['store_or_dc'])\n",
    "\n",
    "        self.loadfile_df['ARKTX'] = merged_df['ARKTX']\n",
    "\n",
    "        current_date = datetime.date.today()\n",
    "        formatted_current_date = current_date.strftime(\"%d.%m.%Y\")\n",
    "        self.loadfile_df['MATCHING_DATE'] = formatted_current_date\n",
    "        self.loadfile_df['MATCH_STATUS'] = 'No Matching Requested'\n",
    "        self.loadfile_df['SYNC_DATE'] = formatted_current_date\n",
    "        self.loadfile_df['SYNC_STATUS'] = '1'\n",
    "        self.loadfile_df = self.loadfile_df.fillna('')\n",
    "        self.loadfile_df['INVOICE_NO'].replace('nan', '', inplace=True)\n",
    "        self.loadfile_df['DOCUMENT_NO'] = self.loadfile_df['DOCUMENT_NO'].replace('0000000000', '')\n",
    "        self.loadfile_df['DEBIT_NOTE_NO'] = self.loadfile_df['DEBIT_NOTE_NO'].apply(\n",
    "            lambda x: str(x).strip() if str(x).strip() else '')\n",
    "        self.loadfile_df = self.loadfile_df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "        return self.loadfile_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4c1e2a-3b0c-4ca5-b9ce-5daba26ed1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9fee130e-1ee6-426c-ab0b-dc3d000f5ece",
   "metadata": {},
   "source": [
    "# Loadfile_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0337a53a-57d9-4860-a59a-c4140582f3dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "import datetime\n",
    "import tempfile\n",
    "\n",
    "class Loadfile_upload:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.bucket_name = \"miag-m360-test-bucket\"\n",
    "\n",
    "    def upload_dataframe_to_gcs(self, df, destination_blob_name, separator):\n",
    "        storage_client = storage.Client()\n",
    "        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".csv\")\n",
    "        try:\n",
    "            df.to_csv(temp_file.name, index=False, sep=separator, encoding='utf-8')\n",
    "            bucket = storage_client.bucket(self.bucket_name)\n",
    "            blob = bucket.blob(destination_blob_name)\n",
    "            blob.upload_from_filename(temp_file.name)\n",
    "            print(f\"CSV uploaded to {destination_blob_name} in bucket {self.bucket_name}.\")\n",
    "        finally:\n",
    "            temp_file.close()\n",
    "            os.remove(temp_file.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9cf99-f79d-4677-8ba0-3b385fb4290f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c65d7a39-fd3b-4f8c-ba85-9d7981ba977b",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9f2fb080-3485-4d23-936c-cb39c704d813",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sslrootcert downloaded and stored temporarily at /var/tmp/tmphdn2lkvi\n",
      "sslcert downloaded and stored temporarily at /var/tmp/tmp81q7aodx\n",
      "sslkey downloaded and stored temporarily at /var/tmp/tmp5720vvhj\n",
      "sslrootcert downloaded and stored temporarily at /var/tmp/tmps5qjdav9\n",
      "sslcert downloaded and stored temporarily at /var/tmp/tmp2w3gkm4r\n",
      "sslkey downloaded and stored temporarily at /var/tmp/tmpf9z2gvne\n",
      "Process started...\n",
      "The latest file in 'Downloaded Files' is: Downloaded Files/miag.35.288560.20241114.959.csv\n",
      "SFTP file :  Downloaded Files/miag.35.288560.20241114.959.csv\n",
      "SFTP shape :  (110841, 18)\n",
      "SFTP fetch time :  1.5190658569335938\n",
      "Written back to SDP Table of DB Instance...\n",
      "SDP rxw time :  2.4581332206726074\n",
      "FI fetch time :  23.59712266921997\n",
      "SIS fetch time :  3.1702136993408203\n",
      "MMS fetch time :  11.371078491210938\n",
      "Count of unique records in FI before cleaning :  180274\n",
      "FI shape after cleaning :  (180274, 35)\n",
      "Cleaning FI time :  124.51346182823181\n",
      "Type casted cleaned FI for storage\n",
      "Typecasting FI time :  0.2225039005279541\n",
      "Written to Intermediate FI Table of DB Instance...\n",
      "Writing FI time :  24.30207848548889\n",
      "Count of unique records in IC before cleaning :  199574\n",
      "IC shape after cleaning :  (199574, 17)\n",
      "Cleaning IC time :  147.44045686721802\n",
      "Written to Intermediate IC Table of DB Instance...\n",
      "Writing IC time :  11.091203212738037\n",
      "Count of unique records in IC after cleaning :  195432\n",
      "IC shape after cleaning :  (195432, 17)\n",
      "1st merge shape :  229920\n",
      "Merge time :  1.167184829711914\n",
      "Datatypes before GR Invoice Implementation :  Document_type                       object\n",
      "Document type                       object\n",
      "GJAHR_x                     string[python]\n",
      "Business Year SFTP          string[python]\n",
      "BUZEI                                Int64\n",
      "Line Item No. SFTP                   Int64\n",
      "Amount_in_local_currency           float64\n",
      "Gross amount                       float64\n",
      "dtype: object\n",
      "Postprocess time :  9.600279331207275\n",
      "Merged_df shape :  (239523, 80)\n",
      "\n",
      "\n",
      "All zeros :  8262\n",
      "Length 1 :  175874\n",
      "Length 2 :  51498\n",
      "Length 3 :  3889\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Length 2 :  51498\n",
      "Both cleared or one progress :  4144\n",
      "GR Invoice Records :  34170\n",
      "not_cleared_miag_records  13184\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GR Invoice Records :  34170\n",
      "Cleared df :  17085\n",
      "Non Cleared df :  17085\n",
      "\n",
      "\n",
      "Cleared df :  17085\n",
      "Non Cleared df :  11092\n",
      "Failed docs :  11092\n",
      "Examples of Failed docs :  ['0810000029', '0810000047', '0810000048', '0810000049', '0810000157', '0810000163', '0810000195', '0810000246', '0810000346', '0810000349', '0810000350', '0810000351', '0810000418', '0810000441', '0810000451', '0810000452', '0810000480', '0810000698', '0810000721', '0810000754']\n",
      "GR Inv Len2 Time :  209.36795496940613\n",
      "Length 3 :  3889\n",
      "len_3_cleared_df :  2398\n",
      "len_3_other_df :  1377\n",
      "GR Inv Len3 Time :  2.4074084758758545\n",
      "(233416, 80)\n",
      "Loadfile Creation time :  1.7726330757141113\n",
      "CSV uploaded to analysis/load.360.35.20241219.001_test_internal.csv in bucket miag-m360-test-bucket.\n",
      "CSV uploaded to share/load.360.35.20241219.001.csv in bucket miag-m360-test-bucket.\n",
      "Written to Final 360 Table of DB Instance...\n",
      "Deleted temporary file: /var/tmp/tmphdn2lkvi\n",
      "Deleted temporary file: /var/tmp/tmp81q7aodx\n",
      "Deleted temporary file: /var/tmp/tmp5720vvhj\n",
      "Process completed...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# from Storage_Bucket_Operations import *\n",
    "# from DB_Instance_Operations import *\n",
    "# from BigQuery_Operations import *\n",
    "# from Preprocess import *\n",
    "# from Loadfile import *\n",
    "# from Latest_SFTP_file import *\n",
    "# import time\n",
    "# from Loadfile_upload import *\n",
    "\n",
    "\n",
    "class Process:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.storage_bucket_operations = Storage_Bucket_Operations()\n",
    "        self.db_instance_operations = DB_Instance_Operations()\n",
    "        self.bigquery_operations = BigQuery_Operations()\n",
    "        self.preprocess = Preprocess()\n",
    "        self.loadfile = Loadfile()\n",
    "        self.latest_sftp_file = Latest_SFTP_file()\n",
    "        self.loadfile_upload = Loadfile_upload()\n",
    "\n",
    "    def process(self):\n",
    "        print(\"Process started...\")\n",
    "        sftp_start_time = time.time()\n",
    "        sftp_file = self.latest_sftp_file.get_latest_file()\n",
    "        print(\"SFTP file : \", sftp_file)\n",
    "        sftp_df = self.storage_bucket_operations.readFromBucket(sftp_file)\n",
    "        print(\"SFTP shape : \", sftp_df.shape)\n",
    "        sftp_end_time = time.time()\n",
    "        print(\"SFTP fetch time : \", sftp_end_time-sftp_start_time)\n",
    "        # lowest_doc_date = self.latest_sftp_file.lowest_document_date(sftp_df)\n",
    "        # print(\"Lowest Document Date : \", lowest_doc_date)\n",
    "        # ic_start_date = self.latest_sftp_file.convert_to_yyyymmdd(lowest_doc_date)\n",
    "        # fi_start_date = self.latest_sftp_file.convert_to_yyyy_mm_dd(lowest_doc_date)\n",
    "        sdp_start_time = time.time()\n",
    "        sdp_df = self.db_instance_operations.readSDPTable()\n",
    "        sdp_df = self.db_instance_operations.updateSDP(sdp_df, sftp_df)\n",
    "        self.db_instance_operations.writeSDPTable(sdp_df)\n",
    "        sdp_end_time = time.time()\n",
    "        print(\"SDP rxw time : \", sdp_end_time-sdp_start_time)\n",
    "        sdp_supplier_list_for_mmsic = self.db_instance_operations.getSupplierNumberForMMSIC()\n",
    "        sdp_supplier_list_for_sisic = self.db_instance_operations.getSupplierNumberForSISIC()\n",
    "        sdp_supplier_list_for_fi = self.db_instance_operations.getSupplierNumberForFI()\n",
    "        fi_start_time = time.time()\n",
    "        df_fi = self.bigquery_operations.extract_FI(sdp_supplier_list_for_fi)\n",
    "        fi_end_time = time.time()\n",
    "        print(\"FI fetch time : \", fi_end_time-fi_start_time)\n",
    "        sisic_df = self.bigquery_operations.extract_SISIC(sdp_supplier_list_for_sisic)\n",
    "        sis_end_time = time.time()\n",
    "        print(\"SIS fetch time : \", sis_end_time-fi_end_time)\n",
    "        mmsic_df = self.bigquery_operations.extract_MMSIC(sdp_supplier_list_for_mmsic)\n",
    "        mmsic_end_time = time.time()\n",
    "        print(\"MMS fetch time : \",mmsic_end_time - sis_end_time)\n",
    "        merged_df = self.preprocess.preprocess(df_fi, sisic_df, mmsic_df, sftp_df)\n",
    "        loadfile_start = time.time()\n",
    "        loadfile_df = self.loadfile.loadfile(merged_df)\n",
    "        loadfile_end = time.time()\n",
    "        print(\"Loadfile Creation time : \", loadfile_end-loadfile_start)\n",
    "        current_datetime = datetime.datetime.now().strftime(\"%Y%m%d\")\n",
    "        self.loadfile_upload.upload_dataframe_to_gcs(loadfile_df, f\"analysis/load.360.35.{current_datetime}.001_test_internal.csv\", separator=',')\n",
    "        self.loadfile_upload.upload_dataframe_to_gcs(loadfile_df, f\"share/load.360.35.{current_datetime}.001.csv\", separator=';')\n",
    "        # print(loadfile_df.dtypes)\n",
    "        self.db_instance_operations.writeMergedTable(loadfile_df)\n",
    "        self.db_instance_operations.__del__()\n",
    "        print(\"Process completed...\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process = Process()\n",
    "    print(process.process())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a9a1aef5-0847-4ff5-a4c1-8d358c0c04d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "046fc288-27bb-4280-b1df-98fcc5957815",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "921.6354794502258\n"
     ]
    }
   ],
   "source": [
    "print(end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7277aea-d27c-4e51-8c41-a8fb3d6d982a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e996af48-8886-4b94-8213-b993bceadb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu118.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/base-cu118:m126"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
